
@article{andersonExtendingMertonianNorms2010,
  title = {Extending the {{Mertonian}} Norms: {{Scientists}}' Subscription to Norms of Research},
  shorttitle = {Extending the Mertonian Norms},
  author = {Anderson, Melissa S. and Ronning, Emily A. and DeVries, Raymond and Martinson, Brian C.},
  year = {2010},
  month = may,
  volume = {81},
  pages = {366--393},
  issn = {0022-1546},
  doi = {10.1353/jhe.0.0095},
  abstract = {This analysis, based on focus groups and a national survey, assesses scientists' subscription to the Mertonian norms of science and associated counternorms. It also supports extension of these norms to governance (as opposed to administration), as a norm of decision-making, and quality (as opposed to quantity), as a evaluative norm.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\JZTHS5RW\\Anderson et al. - 2010 - Extending the Mertonian Norms Scientists’ Subscri.pdf},
  journal = {The Journal of higher education},
  number = {3},
  pmcid = {PMC2995462},
  pmid = {21132074}
}

@article{andersonGraduateStudentExperience1994,
  title = {The Graduate Student Experience and Subscription to the Norms of Science},
  author = {Anderson, Melissa S. and Louis, Karen Seashore},
  year = {1994},
  month = may,
  volume = {35},
  pages = {273--299},
  issn = {1573-188X},
  doi = {10.1007/BF02496825},
  abstract = {This paper examines the normative orientations of doctoral students with respect to academic research. In particular, it analyzes the effects of graduate department structure, department climate, and students' mentoring experiences on students' subscription to the traditional norms of science and to alternative counternorms. Findings are based on data from a nationwide survey of students in chemistry, civil engineering, microbiology, and sociology. The analysis demonstrates substantial ambivalence among graduate students about the traditional norms of academic research. It also reveals significant differences in the normative orientations of U.S. and international students.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\UUE5L93T\\Anderson and Louis - 1994 - The graduate student experience and subscription t.pdf},
  journal = {Research in Higher Education},
  language = {en},
  number = {3}
}

@article{andersonNormativeDissonanceScience2007,
  title = {Normative Dissonance in Science: {{Results}} from a National Survey of {{U}}.{{S}}. Scientists},
  shorttitle = {Normative {{Dissonance}} in {{Science}}},
  author = {Anderson, Melissa S. and Martinson, Brian C. and De Vries, Raymond},
  year = {2007},
  month = dec,
  volume = {2},
  pages = {3--14},
  issn = {1556-2646},
  doi = {10.1525/jer.2007.2.4.3},
  abstract = {Norms of behavior in scientific research represent ideals to which most scientists subscribe. Our analysis of the extent of dissonance between these widely espoused ideals and scientists' perceptions of their own and others' behavior is based on survey responses from 3,247 mid- and early-career scientists who had research funding from the U.S. National Institutes of Health. We found substantial normative dissonance, particularly between espoused ideals and respondents' perceptions of other scientists' typical behavior. Also, respondents on average saw other scientists' behavior as more counternormative than normative. Scientists' views of their fields as cooperative or competitive were associated with their normative perspectives, with competitive fields showing more counternormative behavior. The high levels of normative dissonance documented here represent a persistent source of stress in science.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\YAR2V3YF\\Anderson et al. - 2007 - Normative dissonance in science Results from a na.pdf},
  journal = {Journal of Empirical Research on Human Research Ethics},
  language = {en},
  number = {4}
}

@article{baker500ScientistsLift2016,
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  author = {Baker, Monya},
  year = {2016},
  month = may,
  volume = {533},
  pages = {452},
  doi = {10.1038/533452a},
  abstract = {Survey sheds light on the `crisis' rocking research.},
  chapter = {News Feature},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\BIRUZYKM\\Baker - 2016 - 1,500 scientists lift the lid on reproducibility.pdf},
  journal = {Nature News},
  language = {en},
  number = {7604}
}

@misc{bottesiniParticipantsCareIf2019,
  title = {Do Participants Care If We P-Hack Their Data? {{A}} Registered Report},
  shorttitle = {Do Participants Care If We P-Hack Their Data?},
  author = {Bottesini, Julia and Vazire, S.},
  year = {2019},
  month = sep,
  abstract = {Winning Poster by Julia G. Bottesini at the Metascience 2019 Symposium (September 5th-8th, 2019 | Stanford University)},
  language = {en-US},
  type = {Poster}
}

@article{bowenCashCreditCompensation2017,
  title = {Cash or {{Credit}}? {{Compensation}} in {{Psychology Studies}}: {{Motivation Matters}}},
  shorttitle = {Cash or {{Credit}}?},
  author = {Bowen, Holly J. and Kensinger, Elizabeth A.},
  year = {2017},
  month = may,
  volume = {3},
  issn = {2474-7394},
  doi = {10.1525/collabra.77},
  abstract = {It is common practice for psychology researchers to recruit their sample of participants from the undergraduate student population. Participants are typically compensated with partial course credit or a monetary payment. The current study reveals that the motivation to participate in a study (cash versus course credit) can relate to performance on a behavioral task of rewarded memory. In Experiment 1, undergraduate participants were recruited and compensated for their time with either partial course credit or cash. Potential performance-based cash rewards were earned during a rewarded memory task, where correct recognition of half the stimuli was worth a high reward and the other half a low reward. Memory for high reward items was better than low reward items, but only for the cash group. The credit group did not modulate their performance based on the value of the stimuli. In Experiment 2, undergraduates were compensated with partial course credit for their time and given the opportunity to earn a bonus credit for performance on a memory test. The findings were in line with the results from the credit group of Experiment 1, suggesting that the modulation of performance in the cash group of Experiment 1 cannot be accounted for by congruency between motivation to participate and reward for task performance. Of methodological importance, the findings indicate that recruiting and compensating participants with cash versus course credit may influence the results on a rewarded memory task. This factor should be taken into consideration in studies of reward motivation.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\XJ9HZ4TH\\Bowen and Kensinger - 2017 - Cash or Credit Compensation in Psychology Studies.pdf},
  journal = {Collabra: Psychology},
  number = {12}
}

@article{brandtReplicationRecipeWhat2014,
  title = {The Replication Recipe: {{What}} Makes for a Convincing Replication?},
  shorttitle = {The {{Replication Recipe}}},
  author = {Brandt, Mark J. and IJzerman, Hans and Dijksterhuis, Ap and Farach, Frank J. and Geller, Jason and {Giner-Sorolla}, Roger and Grange, James A. and Perugini, Marco and Spies, Jeffrey R. and {van 't Veer}, Anna},
  year = {2014},
  month = jan,
  volume = {50},
  pages = {217--224},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2013.10.005},
  abstract = {Psychological scientists have recently started to reconsider the importance of close replications in building a cumulative knowledge base; however, there is no consensus about what constitutes a convincing close replication study. To facilitate convincing close replication attempts we have developed a Replication Recipe, outlining standard criteria for a convincing close replication. Our Replication Recipe can be used by researchers, teachers, and students to conduct meaningful replication studies and integrate replications into their scholarly habits.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\BRCJE93T\\Brandt et al. - 2014 - The Replication Recipe What makes for a convincin.pdf},
  journal = {Journal of Experimental Social Psychology},
  keywords = {Pre-registration,Replication,Research method,Solid Science,Statistical power},
  language = {en}
}

@article{chopikHowWhetherTeach2018,
  title = {How (and Whether) to Teach Undergraduates about the Replication Crisis in Psychological Science},
  author = {Chopik, William J. and Bremner, Ryan H. and Defever, Andrew M. and Keller, Victor N.},
  year = {2018},
  month = apr,
  volume = {45},
  pages = {158--163},
  issn = {0098-6283, 1532-8023},
  doi = {10.1177/0098628318762900},
  abstract = {Over the past 10 years, crises surrounding replication, fraud, and best practices in research methods have dominated discussions in the field of psychology. However, no research exists examining how to communicate these issues to undergraduates and what effect this has on their attitudes toward the field. We developed and validated a 1-hr lecture communicating issues surrounding the replication crisis and current recommendations to increase reproducibility. Pre- and post-lecture surveys suggest that the lecture serves as an excellent pedagogical tool. Following the lecture, students trusted psychological studies slightly less but saw greater similarities between psychology and natural science fields. We discuss challenges for instructors taking the initiative to communicate these issues to undergraduates in an evenhanded way.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\RFEYNL7L\\Chopik et al. - 2018 - How (and Whether) to Teach Undergraduates About th.pdf},
  journal = {Teaching of Psychology},
  language = {en},
  number = {2}
}

@article{dallmeier-tiessenHighlightsSOAPProject2011,
  title = {Highlights from the {{SOAP}} Project Survey. {{What}} Scientists Think about Open Access Publishing},
  author = {{Dallmeier-Tiessen}, Suenje and Darby, Robert and Goerner, Bettina and Hyppoelae, Jenni and {Igo-Kemenes}, Peter and Kahn, Deborah and Lambert, Simon and Lengenfelder, Anja and Leonard, Chris and Mele, Salvatore and Nowicka, Malgorzata and Polydoratou, Panayiota and Ross, David and {Ruiz-Perez}, Sergio and Schimmer, Ralf and Swaisland, Mark and {van der Stelt}, Wim},
  year = {2011},
  month = jan,
  abstract = {The SOAP (Study of Open Access Publishing) project has run a large-scale survey of the attitudes of researchers on, and the experiences with, open access publishing. Around forty thousands answers were collected across disciplines and around the world, showing an overwhelming support for the idea of open access, while highlighting funding and (perceived) quality as the main barriers to publishing in open access journals. This article serves as an introduction to the survey and presents this and other highlights from a preliminary analysis of the survey responses. To allow a maximal re-use of the information collected by this survey, the data are hereby released under a CC0 waiver, so to allow libraries, publishers, funding agencies and academics to further analyse risks and opportunities, drivers and barriers, in the transition to open access publishing.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\Z82KBGHN\\Dallmeier-Tiessen et al. - 2011 - Highlights from the SOAP project survey. What Scie.pdf},
  language = {en}
}

@misc{duyguuygun-tuncLastNightWas2021,
  title = {Last Night {{I}} Was Talking to a Friend Who Was Rather Unfamiliar with the Publication Scene and {{I}} Described How @{{Meta}}\_{{Psy}} Requires Open Data/Code and Has Open {{PR}}. {{Then}} She Asked: {{Of}} Course That Makes Total Sense, but Isn't What All Journals Already Do?😅},
  author = {{Duygu Uygun-Tunc}},
  year = {2021},
  month = mar,
  journal = {@uygun\_tunc},
  language = {en},
  type = {Tweet}
}

@article{graheHarnessingUndiscoveredResource2012,
  title = {Harnessing the Undiscovered Resource of Student Research Projects},
  author = {Grahe, Jon E. and Reifman, Alan and Hermann, Anthony D. and Walker, Marie and Oleson, Kathryn C. and {Nario-Redmond}, Michelle and Wiebe, Richard P.},
  year = {2012},
  month = nov,
  volume = {7},
  pages = {605--607},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691612459057},
  abstract = {This article suggests that undergraduate research can help advance the science of psychology. We introduce a hypothetical ``question-list paradigm'' as a mechanism to do this. Each year, thousands of undergraduate projects are completed as part of the educational experience. Although many of these studies may not contain sufficient contributions for publication, they provide a good test of the replicability of established findings across populations at different institutions and geographic locations. Thus, these projects could meet the needs of recent calls for increased replications of psychological studies while simultaneously benefiting the student researchers, their instructors, and the field in general.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\EM7NGBW3\\Grahe et al. - 2012 - Harnessing the Undiscovered Resource of Student Re.pdf},
  journal = {Perspectives on Psychological Science},
  language = {en},
  number = {6}
}

@article{hallerMisinterpretationsSignificanceProblem2002,
  title = {Misinterpretations of Significance: {{A}} Problem Students Share with Their Teachers?},
  author = {Haller, Heiko and Krauss, Stefan},
  year = {2002},
  volume = {7},
  pages = {20},
  abstract = {The use of significance tests in science has been debated from the invention of these tests until the present time. Apart from theoretical critiques on their appropriateness for evaluating scientific hypotheses, significance tests also receive criticism for inviting misinterpretations. We presented six common misinterpretations to psychologists who work in German universities and found out that they are still surprisingly widespread \textendash even among instructors who teach statistics to psychology students. Although these misinterpretations are well documented among students, until now there has been little research on pedagogical methods to remove them. Rather, they are considered ``hard facts'' that are impervious to correction. We discuss the roots of these misinterpretations and propose a pedagogical concept to teach significance tests, which involves explaining the meaning of statistical significance in an appropriate way.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\PPEZDWT6\\Haller and Krauss - 2002 - Misinterpretations of Significance A Problem Stud.pdf},
  language = {en},
  number = {1}
}

@article{harrisUseReproducibleResearch2018,
  title = {Use of Reproducible Research Practices in Public Health: {{A}} Survey of Public Health Analysts},
  shorttitle = {Use of Reproducible Research Practices in Public Health},
  author = {Harris, Jenine K. and Johnson, Kimberly J. and Carothers, Bobbi J. and Combs, Todd B. and Luke, Douglas A. and Wang, Xiaoyan},
  editor = {Gilligan, Conor},
  year = {2018},
  month = sep,
  volume = {13},
  pages = {e0202447},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0202447},
  abstract = {Objective Use of reproducible research practices improves the quality of science and the speed of scientific development. We sought to understand use of reproducible research practices in public health and associated barriers and facilitators. Methods In late 2017, we surveyed members of the American Public Health Association Applied Public Health Statistics section and others; 247 of 278 who screened eligible answered the survey, and 209 answered every applicable question. The survey included questions about file management, code annotation and documentation, reproducibility of analyses, and facilitators and barriers of using reproducible practices. Results Just 14.4\% of participants had shared code, data, or both. Many participants reported their data (33\%) and code (43.2\%) would be difficult for colleagues to find if they left their institution. Top reported barriers to using reproducible practices were data privacy (49.8\%) and lack of time (41.7\%). Participants suggested training (50.9\%) and requirements by journals (44.4\%) and funders (40.2\%) to increase use of reproducible research practices. Conclusions Increasing use of reproducible research practices is important for public health and requires action from researchers, training programs, funders, and journals.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\CM7SULP6\\Harris et al. - 2018 - Use of reproducible research practices in public h.pdf},
  journal = {PLOS ONE},
  language = {en},
  number = {9}
}

@article{jekelHowTeachOpen2020,
  title = {How to Teach Open Science Principles in the Undergraduate Curriculum\textemdash the {{Hagen}} Cumulative Science Project},
  author = {Jekel, Marc and Fiedler, Susann and Allstadt Torras, Ramona and Mischkowski, Dorothee and Dorrough, Angela Rachael and Gl{\"o}ckner, Andreas},
  year = {2020},
  month = mar,
  volume = {19},
  pages = {91--106},
  issn = {1475-7257, 1475-7257},
  doi = {10.1177/1475725719868149},
  abstract = {The Hagen Cumulative Science Project is a large-scale replication project based on students' thesis work. In the project, we aim to (a) teach students to conduct the entire research process for conducting a replication according to open science standards and (b) contribute to cumulative science by increasing the number of direct replications. We describe the procedural steps of the project from choosing suitable replication studies to guiding students through the process of conducting a replication, and processing results in a meta-analysis. Based on the experience of more than 80 replications, we summarize how such a project can be implemented. We present practical solutions that have been shown to be successful as well as discuss typical obstacles and how they can be solved. We argue that replication projects are beneficial for all groups involved: Students benefit by being guided through a highly structured protocol and making actual contributions to science. Instructors benefit by using time resources effectively for cumulative science and fulfilling teaching obligations in a meaningful way. The scientific community benefits from the resulting greater number of replications and teaching state-of-the-art methodology. We encourage the use of student thesis-based replication projects for thesis work in academic bachelor and master curricula.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\QZJDV538\\Jekel et al. - 2020 - How to Teach Open Science Principles in the Underg.pdf},
  journal = {Psychology Learning \& Teaching},
  language = {en},
  number = {1}
}

@article{kleinInvestigatingVariationReplicability2014,
  title = {Investigating Variation in Replicability: {{A}} ``Many Labs'' Replication Project},
  shorttitle = {Investigating Variation in Replicability},
  author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams Jr., Reginald B. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Jane, S. and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and E, A. and Vaughn, Leigh Ann and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
  year = {2014},
  volume = {45},
  pages = {142--152},
  issn = {2151-2590(Electronic);1864-9335(Print)},
  doi = {10.1027/1864-9335/a000178},
  abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect\textemdash imagined contact reducing prejudice\textemdash showed weak support for replicability. And two effects\textemdash flag priming influencing conservatism and currency priming influencing system justification\textemdash did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
  copyright = {(c) 2014 APA, all rights reserved},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\Y942FCUV\\Klein et al. - 2014 - Investigating variation in replicability A “many .pdf},
  journal = {Social Psychology},
  keywords = {*Cross Cultural Differences,Experimental Replication},
  number = {3}
}

@article{kleinManyLabsInvestigating2018,
  title = {Many Labs 2: {{Investigating}} Variation in Replicability across Samples and Settings},
  shorttitle = {Many {{Labs}} 2},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E. and {de Bruijn}, Maaike and De Schutter, Leander and Devos, Thierry and {de Vries}, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and {Innes-Ker}, {\AA}se H. and {Jim{\'e}nez-Leal}, William and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Kamilo{\u g}lu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\dj}edovi{\'c}, Janko and {Mena-Pacheco}, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'e}lix and Lee Nichols, Austin and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and {P{\'e}rez-S{\'a}nchez}, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'u}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and {Smith-Castro}, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M. and {van der Hulst}, Marije and {van Lange}, Paul A. M. and {van 't Veer}, Anna Elisabeth and {V{\'a}squez- Echeverr{\'i}a}, Alejandro and Ann Vaughn, Leigh and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  year = {2018},
  month = dec,
  volume = {1},
  pages = {443--490},
  issn = {2515-2459},
  doi = {10.1177/2515245918810225},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {$<$} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {$<$} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({$<$} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\KWH4LYRL\\Klein et al. - 2018 - Many Labs 2 Investigating Variation in Replicabil.pdf},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en},
  number = {4}
}

@article{mertonNoteScienceDemocracy1942,
  title = {A Note on Science and Democracy},
  author = {Merton, Robert K.},
  year = {1942},
  volume = {1},
  pages = {115--126},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\5SSDNIVM\\merton 1942 A note on Science and Democracy.pdf},
  journal = {Journal of Legal and Political Sociology}
}

@book{mertonSociologyScienceTheoretical1973,
  title = {The Sociology of Science : Theoretical and Empirical Investigations},
  shorttitle = {The Sociology of Science},
  author = {Merton, Robert King},
  year = {1973},
  publisher = {{Chicago : University of Chicago Press}},
  abstract = {xxxi, 605 p. ; 24 cm; Includes index; Bibliography: p. [561]-575},
  collaborator = {{Internet Archive}},
  isbn = {978-0-226-52091-9},
  keywords = {Science -- Social aspects},
  language = {eng}
}

@article{meyerPracticalTipsEthical2018,
  title = {Practical Tips for Ethical Data Sharing},
  author = {Meyer, Michelle N.},
  year = {2018},
  month = mar,
  volume = {1},
  pages = {131--144},
  issn = {2515-2459},
  doi = {10.1177/2515245917747656},
  abstract = {This Tutorial provides practical dos and don'ts for sharing research data in ways that are effective, ethical, and compliant with the federal Common Rule. I first consider best practices for prospectively incorporating data-sharing plans into research, discussing what to say\textemdash and what not to say\textemdash in consent forms and institutional review board applications, tools for data de-identification and how to think about the risks of re-identification, and what to consider when selecting a data repository. Turning to data that have already been collected, I discuss the ethical and regulatory issues raised by sharing data when the consent form either was silent about data sharing or explicitly promised participants that the data would not be shared. Finally, I discuss ethical issues in sharing ``public'' data.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\UC48N6IA\\Meyer - 2018 - Practical Tips for Ethical Data Sharing.pdf},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en},
  number = {1}
}

@article{nosekPreregistrationRevolution2018,
  title = {The Preregistration Revolution},
  author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
  year = {2018},
  month = mar,
  volume = {115},
  pages = {2600--2606},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708274114},
  abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes\textemdash a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
  copyright = {\textcopyright{} 2018 . Published under the PNAS license.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\9CWZQV48\\Nosek et al. - 2018 - The preregistration revolution.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {confirmatory analysis,exploratory analysis,methodology,open science,preregistration},
  language = {en},
  number = {11},
  pmid = {29531091}
}

@article{ogradyPsychologyReplicationCrisis2020,
  title = {Psychology's Replication Crisis Inspires Ecologists to Push for More Reliable Research},
  author = {O'Grady, Cathleen and {2020} and Pm, 2:05},
  year = {2020},
  month = dec,
  abstract = {New society aims for transparency and culture change in ecology},
  journal = {Science},
  language = {en}
}

@techreport{openscienceworkinggroup2017SurveyOpen2018,
  title = {2017 Survey on Open Research Practices Administered by the {{School}} of {{Psychology}}, {{Cardiff University}}},
  author = {Open Science Working Group},
  year = {2018},
  month = jun,
  institution = {{Cardiff University}},
  abstract = {Hosted on the Open Science Framework},
  language = {en}
}

@article{piperScienceHasBeen2020,
  title = {Science Has Been in a "Replication Crisis" for a Decade. {{Have}} We Learned Anything?},
  author = {Piper, Kelsey},
  year = {2020},
  month = oct,
  abstract = {Bad papers are still published. But some other things might be getting better.},
  journal = {Vox},
  language = {en}
}

@article{schonbrodtTrainingStudentsOpen2019,
  title = {Training Students for the {{Open Science}} Future},
  author = {Sch{\"o}nbrodt, Felix},
  year = {2019},
  month = oct,
  volume = {3},
  pages = {1031--1031},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0726-z},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\WQEP3GAM\\Schönbrodt - 2019 - Training students for the Open Science future.pdf},
  journal = {Nature Human Behaviour},
  language = {en},
  number = {10}
}

@article{schopfelReadyFutureSurvey2016,
  title = {Ready for the Future? {{A}} Survey on Open Access with Scientists from the {{French National Research Center}} ({{CNRS}})},
  shorttitle = {Ready for the Future?},
  author = {Sch{\"o}pfel, Joachim and Ferrant, Coline and Andr{\'e}, Francis and Fabre, Renaud},
  year = {2016},
  month = jan,
  volume = {44},
  pages = {141--149},
  publisher = {{Emerald Group Publishing Limited}},
  issn = {0264-1615},
  doi = {10.1108/ILDS-06-2016-0023},
  abstract = {Purpose This paper aims to present empirical evidence on the opinion and behaviour of French scientists (senior management level) regarding open access (OA) to scientific and technical information. Design/methodology/approach The results are part of a nationwide survey on scientific information and documentation with 432 directors of French public research laboratories conducted by the French National Research Center (CNRS) in 2014. Findings The CNRS senior research managers (laboratory directors) globally share the positive opinion towards OA revealed by other studies with researchers from the UK, Germany, the USA and other countries. However, they are more supportive of open repositories (green road) than of OA journal publishing (gold). The response patterns reveal a gap between generally positive opinions about OA and less supportive behaviours, principally publishing articles with article processing charges (APCs). A small group of senior research managers does not seem to be interested in green or gold OA and reluctant to self-archiving and OA publishing. Similar to other studies, the French survey confirms disciplinary differences, i.e. a stronger support for self-archiving of records and documents in HAL by scientists from Mathematics, Physics and Informatics than from Biology, Earth Sciences and Chemistry; and more experience and positive feelings with OA publishing and payment of APCs in Biology than in Mathematics or in Social Sciences and Humanities. Disciplinary differences and specific French factors are discussed, in particular in the context of the new European policy in favour of Open Science. Originality/value For the first time, a nationwide survey was conducted with the senior research management level from all scientific disciplines. The response rate was high ({$>$}30 per cent), and the results provide good insight into the real awareness, support and uptake of OA by senior research managers who provide both models (examples for good practice) and opinion leadership.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\7KBPT724\\Schöpfel et al. - 2016 - Ready for the future A survey on open access with.pdf},
  journal = {Interlending \& Document Supply},
  keywords = {OA publishing,Open access,Open repositories,Open science,Research management,Self-archiving},
  number = {4}
}

@techreport{simmons21WordSolution2012,
  title = {A 21 Word Solution},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2012},
  month = oct,
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  abstract = {One year after publishing "False-Positive Psychology," we propose a simple implementation of disclosure that requires but 21 words to achieve full transparency. This article is written in a casual tone. It includes phone-taken pictures of milk-jars and references to ice-cream and sardines.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\8G7XK2RD\\Simmons et al. - 2012 - A 21 word solution.pdf},
  keywords = {disclosure,false positive,p-hacking,psycholog,transparency},
  number = {ID 2160588},
  type = {{{SSRN Scholarly Paper}}}
}

@article{thalmayerNeglected95Revisited2021,
  title = {The Neglected 95\% Revisited: {{Is American}} Psychology Becoming Less {{American}}?},
  shorttitle = {The Neglected 95\% Revisited},
  author = {Thalmayer, Amber Gayle and Toscanelli, Cecilia and Arnett, Jeffrey Jensen},
  year = {2021},
  volume = {76},
  pages = {116--129},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1935-990X(Electronic),0003-066X(Print)},
  doi = {10.1037/amp0000622},
  abstract = {The field of psychology prides itself on being a data-driven science. In 2008, however, Arnett brought to light a major weakness in the evidence on which models, measures, and theories in psychology rest. He demonstrated that the most prominent journals in six subdisciplines of psychology focused almost exclusively (over 70\% of samples and authors) on a cultural context, the United States, shared by only 5\% of the world's population. How can psychologists trust that these models and results generalize to all humans, if the evidence comes from a small and unrepresentative portion of the global population? Arnett's analysis, cited over 1,300 times since its publication, appears to have galvanized researchers to think more globally. Social scientists from the United States have increasingly sought ways to collaborate with colleagues abroad. Ten years later, an analysis of the same 6 journals for the period of 2014 to 2018 indicates that the authors and samples are now on average a little over 60\% American based. The change is mainly due to an increase in authorship and samples from other English-speaking and Western European countries. Thus, it might be said that 11\% of the world's population is now represented in these top psychology journals, but that 89\% of the world's population continues to be neglected. Majority world authors and samples (4\textendash 5\%) are still sorely lacking from the evidence base. Psychology still has a long way to go to become a science truly representative of human beings. Several specific recommendations are discussed. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\XSQLXBV6\\Thalmayer et al. - 2021 - The neglected 95% revisited Is American psycholog.pdf},
  journal = {American Psychologist},
  keywords = {Cross Cultural Psychology,Majority Groups,Meta Analysis,Philosophies,Psychologists,Reporting Standards,Sciences,Scientific Communication},
  number = {1}
}

@article{wagenmakersAgendaPurelyConfirmatory2012,
  title = {An Agenda for Purely Confirmatory Research},
  author = {Wagenmakers, Eric-Jan and Wetzels, Ruud and Borsboom, Denny and {van der Maas}, Han L. J. and Kievit, Rogier A.},
  year = {2012},
  month = nov,
  volume = {7},
  pages = {632--638},
  issn = {1745-6916},
  doi = {10.1177/1745691612463078},
  abstract = {The veracity of substantive research claims hinges on the way experimental data are collected and analyzed. In this article, we discuss an uncomfortable fact that threatens the core of psychology's academic enterprise: almost without exception, psychologists do not commit themselves to a method of data analysis before they see the actual data. It then becomes tempting to fine tune the analysis to the data in order to obtain a desired result\textemdash a procedure that invalidates the interpretation of the common statistical tests. The extent of the fine tuning varies widely across experiments and experimenters but is almost impossible for reviewers and readers to gauge. To remedy the situation, we propose that researchers preregister their studies and indicate in advance the analyses they intend to conduct. Only these analyses deserve the label ``confirmatory,'' and only for these analyses are the common statistical tests valid. Other analyses can be carried out but these should be labeled ``exploratory.'' We illustrate our proposal with a confirmatory replication attempt of a study on extrasensory perception.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\VV2NL9PV\\Wagenmakers et al. - 2012 - An agenda for purely confirmatory research.pdf},
  journal = {Perspectives on Psychological Science},
  language = {en},
  number = {6}
}

@article{yongPsychologyReplicationCrisis2016,
  title = {Psychology's Replication Crisis Can't Be Wished Away},
  author = {Yong, Ed},
  year = {2016},
  month = mar,
  abstract = {It has a real and heartbreaking cost.},
  journal = {The Atlantic},
  language = {en-US}
}

@article{yongPsychologyReplicationCrisis2018,
  title = {Psychology's Replication Crisis Is Running out of Excuses},
  author = {Yong, Ed},
  year = {2018},
  month = nov,
  abstract = {Another big project has found that only half of studies can be repeated. And this time, the usual explanations fall flat.},
  chapter = {Science},
  journal = {The Atlantic},
  language = {en}
}


@misc{duygu_uygun-tunc_uygun_tunc_last_2021,
	title = {Last night {I} was talking to a friend who was rather unfamiliar with the publication scene and {I} described how @{Meta}\_Psy requires open data/code and has open {PR}. {Then} she asked: {Of} course that makes total sense, but isn't what all journals already do?},
	url = {https://twitter.com/uygun_tunc/status/1368634914611073029},
	language = {en},
	urldate = {2021-04-16},
	author = {{Duygu Uygun-Tunc [@uygun\_tunc]}},
	month = mar,
	year = {2021},
	keywords = {twitter},
	file = {Snapshot:C\:\\Users\\ekothe\\Zotero\\storage\\4Y5SQSFM\\1368634914611073029.html:text/html},
}


@misc{burak_tunca_uygun_tunc_2021,
	type = {Tweet},
	title = {@uygun\_tunc @rmounce @{Meta}\_Psy {Most} naïve answers {I} heard so far during open science seminars: -(re open access) {I} thought those publishers were nonprofit and they give back to academia somehow (senior academic) -(ms student) {I} assumed peer review checks for stats and the journal store a copy of the dataset},
	url = {https://twitter.com/tuncaburak/status/1368892552305377288},
	language = {en},
	urldate = {2021-04-16},
	journal = {@tuncaburak},
	author = {{Burak Tunca [@tuncaburak]}},
	month = mar,
	year = {2021},
	keywords = {twitter},
	annote = {“@uygun\_tunc @rmounce @Meta\_Psy Most naïve answers I heard so far during open science seminars:-(re open access) I thought those publishers were nonprofit and they give back to academia somehow (senior academic)-(ms student) I assumed peer review checks for stats and the journal store a copy of the dataset”},
}

@misc{lorne_campbell_distributary_academic_minzlicht_2018,
	type = {Tweet},
	title = {@minzlicht @blackgoatpod {Listened} to this episode after seeing your reaction, and yes, wow. {When} {I} teach open science approaches to undergrads they typically are surprised the scientific process has not always been open. {Will} advise students to ask potential advisors about research philosophy.},
	url = {https://twitter.com/LorneJCampbell/status/992076618616266758},
	language = {en},
	urldate = {2021-04-16},
	journal = {@LorneJCampbell},
	author = {{Lorne Campbell [@LorneJCampbell]}},
	month = may,
	year = {2018},
	keywords = {twitter},
}


