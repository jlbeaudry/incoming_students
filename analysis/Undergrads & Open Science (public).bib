
@article{andersonExtendingMertonianNorms2010,
  title = {Extending the {{Mertonian}} Norms: {{Scientists}}' Subscription to Norms of Research},
  shorttitle = {Extending the Mertonian Norms},
  author = {Anderson, Melissa S. and Ronning, Emily A. and DeVries, Raymond and Martinson, Brian C.},
  year = {2010},
  month = may,
  journal = {The Journal of higher education},
  volume = {81},
  number = {3},
  pages = {366--393},
  issn = {0022-1546},
  doi = {10.1353/jhe.0.0095},
  abstract = {This analysis, based on focus groups and a national survey, assesses scientists' subscription to the Mertonian norms of science and associated counternorms. It also supports extension of these norms to governance (as opposed to administration), as a norm of decision-making, and quality (as opposed to quantity), as a evaluative norm.},
  pmcid = {PMC2995462},
  pmid = {21132074},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\JZTHS5RW\\Anderson et al. - 2010 - Extending the Mertonian Norms Scientists’ Subscri.pdf}
}

@article{andersonGraduateStudentExperience1994,
  title = {The Graduate Student Experience and Subscription to the Norms of Science},
  author = {Anderson, Melissa S. and Louis, Karen Seashore},
  year = {1994},
  month = may,
  journal = {Research in Higher Education},
  volume = {35},
  number = {3},
  pages = {273--299},
  issn = {1573-188X},
  doi = {10.1007/BF02496825},
  abstract = {This paper examines the normative orientations of doctoral students with respect to academic research. In particular, it analyzes the effects of graduate department structure, department climate, and students' mentoring experiences on students' subscription to the traditional norms of science and to alternative counternorms. Findings are based on data from a nationwide survey of students in chemistry, civil engineering, microbiology, and sociology. The analysis demonstrates substantial ambivalence among graduate students about the traditional norms of academic research. It also reveals significant differences in the normative orientations of U.S. and international students.},
  langid = {english},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\UUE5L93T\\Anderson and Louis - 1994 - The graduate student experience and subscription t.pdf}
}

@article{andersonNormativeDissonanceScience2007,
  title = {Normative Dissonance in Science: {{Results}} from a National Survey of {{U}}.{{S}}. Scientists},
  shorttitle = {Normative {{Dissonance}} in {{Science}}},
  author = {Anderson, Melissa S. and Martinson, Brian C. and De Vries, Raymond},
  year = {2007},
  month = dec,
  journal = {Journal of Empirical Research on Human Research Ethics},
  volume = {2},
  number = {4},
  pages = {3--14},
  issn = {1556-2646},
  doi = {10.1525/jer.2007.2.4.3},
  abstract = {Norms of behavior in scientific research represent ideals to which most scientists subscribe. Our analysis of the extent of dissonance between these widely espoused ideals and scientists' perceptions of their own and others' behavior is based on survey responses from 3,247 mid- and early-career scientists who had research funding from the U.S. National Institutes of Health. We found substantial normative dissonance, particularly between espoused ideals and respondents' perceptions of other scientists' typical behavior. Also, respondents on average saw other scientists' behavior as more counternormative than normative. Scientists' views of their fields as cooperative or competitive were associated with their normative perspectives, with competitive fields showing more counternormative behavior. The high levels of normative dissonance documented here represent a persistent source of stress in science.},
  langid = {english},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\YAR2V3YF\\Anderson et al. - 2007 - Normative dissonance in science Results from a na.pdf}
}

@article{baker500ScientistsLift2016,
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  author = {Baker, Monya},
  year = {2016},
  month = may,
  journal = {Nature News},
  volume = {533},
  number = {7604},
  pages = {452},
  doi = {10.1038/533452a},
  abstract = {Survey sheds light on the `crisis' rocking research.},
  chapter = {News Feature},
  langid = {english},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\BIRUZYKM\\Baker - 2016 - 1,500 scientists lift the lid on reproducibility.pdf}
}

@techreport{beaudrySwinburneOpenScience2019,
  title = {Swinburne {{Open Science Task Force Survey Report}}},
  author = {Beaudry, Jennifer L. and Kaufman, Jordy and Johnstone, Tom and Given, Lisa},
  year = {2019},
  pages = {17},
  langid = {english},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\3SLVEDCT\\Swinburne Open Science Task Force Survey Report.pdf}
}

@incollection{bemWritingEmpiricalJournal2004,
  title = {Writing the {{Empirical Journal Article}}},
  booktitle = {The {{Compleat Academic}}: {{A Career Guide}}},
  author = {Bem, Daryl},
  editor = {Darley, John M. and Zanna, Mark P. and Roediger III, Henry L.},
  year = {2004},
  edition = {Second},
  pages = {185--219},
  publisher = {{American Psychologcial Association}},
  address = {{Washington D.C.}},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\FYJEQ45V\\Writing the Empirical Journal Article.pdf}
}

@misc{bottesiniParticipantsCareIf2019,
  type = {Poster},
  title = {Do Participants Care If We P-Hack Their Data? {{A}} Registered Report},
  shorttitle = {Do Participants Care If We P-Hack Their Data?},
  author = {Bottesini, Julia and Vazire, S.},
  year = {2019},
  month = sep,
  abstract = {Winning Poster by Julia G. Bottesini at the Metascience 2019 Symposium (September 5th-8th, 2019 | Stanford University)},
  langid = {american}
}

@article{bowenCashCreditCompensation2017,
  title = {Cash or {{Credit}}? {{Compensation}} in {{Psychology Studies}}: {{Motivation Matters}}},
  shorttitle = {Cash or {{Credit}}?},
  author = {Bowen, Holly J. and Kensinger, Elizabeth A.},
  year = {2017},
  month = may,
  journal = {Collabra: Psychology},
  volume = {3},
  number = {12},
  issn = {2474-7394},
  doi = {10.1525/collabra.77},
  abstract = {It is common practice for psychology researchers to recruit their sample of participants from the undergraduate student population. Participants are typically compensated with partial course credit or a monetary payment. The current study reveals that the motivation to participate in a study (cash versus course credit) can relate to performance on a behavioral task of rewarded memory. In Experiment 1, undergraduate participants were recruited and compensated for their time with either partial course credit or cash. Potential performance-based cash rewards were earned during a rewarded memory task, where correct recognition of half the stimuli was worth a high reward and the other half a low reward. Memory for high reward items was better than low reward items, but only for the cash group. The credit group did not modulate their performance based on the value of the stimuli. In Experiment 2, undergraduates were compensated with partial course credit for their time and given the opportunity to earn a bonus credit for performance on a memory test. The findings were in line with the results from the credit group of Experiment 1, suggesting that the modulation of performance in the cash group of Experiment 1 cannot be accounted for by congruency between motivation to participate and reward for task performance. Of methodological importance, the findings indicate that recruiting and compensating participants with cash versus course credit may influence the results on a rewarded memory task. This factor should be taken into consideration in studies of reward motivation.},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\XJ9HZ4TH\\Bowen and Kensinger - 2017 - Cash or Credit Compensation in Psychology Studies.pdf}
}

@article{brandtReplicationRecipeWhat2014,
  title = {The Replication Recipe: {{What}} Makes for a Convincing Replication?},
  shorttitle = {The {{Replication Recipe}}},
  author = {Brandt, Mark J. and IJzerman, Hans and Dijksterhuis, Ap and Farach, Frank J. and Geller, Jason and {Giner-Sorolla}, Roger and Grange, James A. and Perugini, Marco and Spies, Jeffrey R. and {van 't Veer}, Anna},
  year = {2014},
  month = jan,
  journal = {Journal of Experimental Social Psychology},
  volume = {50},
  pages = {217--224},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2013.10.005},
  abstract = {Psychological scientists have recently started to reconsider the importance of close replications in building a cumulative knowledge base; however, there is no consensus about what constitutes a convincing close replication study. To facilitate convincing close replication attempts we have developed a Replication Recipe, outlining standard criteria for a convincing close replication. Our Replication Recipe can be used by researchers, teachers, and students to conduct meaningful replication studies and integrate replications into their scholarly habits.},
  langid = {english},
  keywords = {Pre-registration,Replication,Research method,Solid Science,Statistical power},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\BRCJE93T\\Brandt et al. - 2014 - The Replication Recipe What makes for a convincin.pdf}
}

@article{chopikHowWhetherTeach2018,
  title = {How (and Whether) to Teach Undergraduates about the Replication Crisis in Psychological Science},
  author = {Chopik, William J. and Bremner, Ryan H. and Defever, Andrew M. and Keller, Victor N.},
  year = {2018},
  month = apr,
  journal = {Teaching of Psychology},
  volume = {45},
  number = {2},
  pages = {158--163},
  issn = {0098-6283, 1532-8023},
  doi = {10.1177/0098628318762900},
  abstract = {Over the past 10 years, crises surrounding replication, fraud, and best practices in research methods have dominated discussions in the field of psychology. However, no research exists examining how to communicate these issues to undergraduates and what effect this has on their attitudes toward the field. We developed and validated a 1-hr lecture communicating issues surrounding the replication crisis and current recommendations to increase reproducibility. Pre- and post-lecture surveys suggest that the lecture serves as an excellent pedagogical tool. Following the lecture, students trusted psychological studies slightly less but saw greater similarities between psychology and natural science fields. We discuss challenges for instructors taking the initiative to communicate these issues to undergraduates in an evenhanded way.},
  langid = {english},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\RFEYNL7L\\Chopik et al. - 2018 - How (and Whether) to Teach Undergraduates About th.pdf}
}

@article{dallmeier-tiessenHighlightsSOAPProject2011,
  title = {Highlights from the {{SOAP}} Project Survey. {{What}} Scientists Think about Open Access Publishing},
  author = {{Dallmeier-Tiessen}, Suenje and Darby, Robert and Goerner, Bettina and Hyppoelae, Jenni and {Igo-Kemenes}, Peter and Kahn, Deborah and Lambert, Simon and Lengenfelder, Anja and Leonard, Chris and Mele, Salvatore and Nowicka, Malgorzata and Polydoratou, Panayiota and Ross, David and {Ruiz-Perez}, Sergio and Schimmer, Ralf and Swaisland, Mark and {van der Stelt}, Wim},
  year = {2011},
  month = jan,
  abstract = {The SOAP (Study of Open Access Publishing) project has run a large-scale survey of the attitudes of researchers on, and the experiences with, open access publishing. Around forty thousands answers were collected across disciplines and around the world, showing an overwhelming support for the idea of open access, while highlighting funding and (perceived) quality as the main barriers to publishing in open access journals. This article serves as an introduction to the survey and presents this and other highlights from a preliminary analysis of the survey responses. To allow a maximal re-use of the information collected by this survey, the data are hereby released under a CC0 waiver, so to allow libraries, publishers, funding agencies and academics to further analyse risks and opportunities, drivers and barriers, in the transition to open access publishing.},
  langid = {english},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\Z82KBGHN\\Dallmeier-Tiessen et al. - 2011 - Highlights from the SOAP project survey. What Scie.pdf}
}

@article{friedrichReplicatingNationalSurvey2018,
  title = {Replicating a National Survey on Statistical Training in Undergraduate Psychology Programs: {{Are}} There ``New Statistics'' in the New Millennium?},
  shorttitle = {Replicating a National Survey on Statistical Training in Undergraduate Psychology Programs},
  author = {Friedrich, James and Childress, Julia and Cheng, David},
  year = {2018},
  month = oct,
  journal = {Teaching of Psychology},
  volume = {45},
  number = {4},
  pages = {312--323},
  publisher = {{SAGE Publications Inc}},
  issn = {0098-6283},
  doi = {10.1177/0098628318796414},
  abstract = {This study describes a close replication of Friedrich, Buday, and Kerr's late 1990s survey of statistics instruction in undergraduate psychology programs. Disciplinary reform efforts at that time such as the report of the APA Task Force on Statistical Inference, together with recent progress in the new statistics movement, raise important questions about whether undergraduate instruction has kept pace. Other than increases in effect size coverage, instructors' estimates of class time devoted to critical reform topics have changed relatively little over nearly two decades, with significant attention often reserved for a rarely offered second-level, advanced class. We consider the importance of addressing the statistics curriculum in ways that meet the reading access, critical thinking, and research skill needs of current majors.},
  langid = {english},
  keywords = {confidence intervals,effect sizes,meta-analysis,new statistics,statistics}
}

@article{graheHarnessingUndiscoveredResource2012,
  title = {Harnessing the Undiscovered Resource of Student Research Projects},
  author = {Grahe, Jon E. and Reifman, Alan and Hermann, Anthony D. and Walker, Marie and Oleson, Kathryn C. and {Nario-Redmond}, Michelle and Wiebe, Richard P.},
  year = {2012},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {605--607},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691612459057},
  abstract = {This article suggests that undergraduate research can help advance the science of psychology. We introduce a hypothetical ``question-list paradigm'' as a mechanism to do this. Each year, thousands of undergraduate projects are completed as part of the educational experience. Although many of these studies may not contain sufficient contributions for publication, they provide a good test of the replicability of established findings across populations at different institutions and geographic locations. Thus, these projects could meet the needs of recent calls for increased replications of psychological studies while simultaneously benefiting the student researchers, their instructors, and the field in general.},
  langid = {english},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\EM7NGBW3\\Grahe et al. - 2012 - Harnessing the Undiscovered Resource of Student Re.pdf}
}

@article{hallerMisinterpretationsSignificanceProblem2002,
  title = {Misinterpretations of Significance: {{A}} Problem Students Share with Their Teachers?},
  author = {Haller, Heiko and Krauss, Stefan},
  year = {2002},
  volume = {7},
  number = {1},
  pages = {20},
  abstract = {The use of significance tests in science has been debated from the invention of these tests until the present time. Apart from theoretical critiques on their appropriateness for evaluating scientific hypotheses, significance tests also receive criticism for inviting misinterpretations. We presented six common misinterpretations to psychologists who work in German universities and found out that they are still surprisingly widespread \textendash even among instructors who teach statistics to psychology students. Although these misinterpretations are well documented among students, until now there has been little research on pedagogical methods to remove them. Rather, they are considered ``hard facts'' that are impervious to correction. We discuss the roots of these misinterpretations and propose a pedagogical concept to teach significance tests, which involves explaining the meaning of statistical significance in an appropriate way.},
  langid = {english},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\PPEZDWT6\\Haller and Krauss - 2002 - Misinterpretations of Significance A Problem Stud.pdf}
}

@article{hardwickeEstimatingPrevalenceTransparency2021,
  title = {Estimating the Prevalence of Transparency and Reproducibility-Related Research Practices in Psychology (2014\textendash 2017)},
  author = {Hardwicke, Tom E. and Thibault, Robert T. and Kosie, Jessica E. and Wallach, Joshua D. and Kidwell, Mallory C. and Ioannidis, John P. A.},
  year = {2021},
  month = mar,
  journal = {Perspectives on Psychological Science},
  pages = {1745691620979806},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620979806},
  abstract = {Psychologists are navigating an unprecedented period of introspection about the credibility and utility of their discipline. Reform initiatives emphasize the benefits of transparency and reproducibility-related research practices; however, adoption across the psychology literature is unknown. Estimating the prevalence of such practices will help to gauge the collective impact of reform initiatives, track progress over time, and calibrate future efforts. To this end, we manually examined a random sample of 250 psychology articles published between 2014 and 2017. Over half of the articles were publicly available (154/237, 65\%, 95\% confidence interval [CI] = [59\%, 71\%]); however, sharing of research materials (26/183; 14\%, 95\% CI = [10\%, 19\%]), study protocols (0/188; 0\%, 95\% CI = [0\%, 1\%]), raw data (4/188; 2\%, 95\% CI = [1\%, 4\%]), and analysis scripts (1/188; 1\%, 95\% CI = [0\%, 1\%]) was rare. Preregistration was also uncommon (5/188; 3\%, 95\% CI = [1\%, 5\%]). Many articles included a funding disclosure statement (142/228; 62\%, 95\% CI = [56\%, 69\%]), but conflict-of-interest statements were less common (88/228; 39\%, 95\% CI = [32\%, 45\%]). Replication studies were rare (10/188; 5\%, 95\% CI = [3\%, 8\%]), and few studies were included in systematic reviews (21/183; 11\%, 95\% CI = [8\%, 16\%]) or meta-analyses (12/183; 7\%, 95\% CI = [4\%, 10\%]). Overall, the results suggest that transparency and reproducibility-related research practices were far from routine. These findings establish baseline prevalence estimates against which future progress toward increasing the credibility and utility of psychology research can be compared.},
  langid = {english},
  keywords = {meta-research,open science,psychology,reproducibility,transparency},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\CELL69NE\\Hardwicke et al. - 2021 - Estimating the Prevalence of Transparency and Repr.pdf}
}

@article{harrisUseReproducibleResearch2018,
  title = {Use of Reproducible Research Practices in Public Health: {{A}} Survey of Public Health Analysts},
  shorttitle = {Use of Reproducible Research Practices in Public Health},
  author = {Harris, Jenine K. and Johnson, Kimberly J. and Carothers, Bobbi J. and Combs, Todd B. and Luke, Douglas A. and Wang, Xiaoyan},
  editor = {Gilligan, Conor},
  year = {2018},
  month = sep,
  journal = {PLOS ONE},
  volume = {13},
  number = {9},
  pages = {e0202447},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0202447},
  abstract = {Objective Use of reproducible research practices improves the quality of science and the speed of scientific development. We sought to understand use of reproducible research practices in public health and associated barriers and facilitators. Methods In late 2017, we surveyed members of the American Public Health Association Applied Public Health Statistics section and others; 247 of 278 who screened eligible answered the survey, and 209 answered every applicable question. The survey included questions about file management, code annotation and documentation, reproducibility of analyses, and facilitators and barriers of using reproducible practices. Results Just 14.4\% of participants had shared code, data, or both. Many participants reported their data (33\%) and code (43.2\%) would be difficult for colleagues to find if they left their institution. Top reported barriers to using reproducible practices were data privacy (49.8\%) and lack of time (41.7\%). Participants suggested training (50.9\%) and requirements by journals (44.4\%) and funders (40.2\%) to increase use of reproducible research practices. Conclusions Increasing use of reproducible research practices is important for public health and requires action from researchers, training programs, funders, and journals.},
  langid = {english},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\CM7SULP6\\Harris et al. - 2018 - Use of reproducible research practices in public h.pdf}
}

@article{jekelHowTeachOpen2020,
  title = {How to Teach Open Science Principles in the Undergraduate Curriculum\textemdash the {{Hagen}} Cumulative Science Project},
  author = {Jekel, Marc and Fiedler, Susann and Allstadt Torras, Ramona and Mischkowski, Dorothee and Dorrough, Angela Rachael and Gl{\"o}ckner, Andreas},
  year = {2020},
  month = mar,
  journal = {Psychology Learning \& Teaching},
  volume = {19},
  number = {1},
  pages = {91--106},
  issn = {1475-7257, 1475-7257},
  doi = {10.1177/1475725719868149},
  abstract = {The Hagen Cumulative Science Project is a large-scale replication project based on students' thesis work. In the project, we aim to (a) teach students to conduct the entire research process for conducting a replication according to open science standards and (b) contribute to cumulative science by increasing the number of direct replications. We describe the procedural steps of the project from choosing suitable replication studies to guiding students through the process of conducting a replication, and processing results in a meta-analysis. Based on the experience of more than 80 replications, we summarize how such a project can be implemented. We present practical solutions that have been shown to be successful as well as discuss typical obstacles and how they can be solved. We argue that replication projects are beneficial for all groups involved: Students benefit by being guided through a highly structured protocol and making actual contributions to science. Instructors benefit by using time resources effectively for cumulative science and fulfilling teaching obligations in a meaningful way. The scientific community benefits from the resulting greater number of replications and teaching state-of-the-art methodology. We encourage the use of student thesis-based replication projects for thesis work in academic bachelor and master curricula.},
  langid = {english},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\QZJDV538\\Jekel et al. - 2020 - How to Teach Open Science Principles in the Underg.pdf}
}

@article{kleinInvestigatingVariationReplicability2014,
  title = {Investigating Variation in Replicability: {{A}} ``Many Labs'' Replication Project},
  shorttitle = {Investigating Variation in Replicability},
  author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams Jr., Reginald B. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Jane, S. and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and E, A. and Vaughn, Leigh Ann and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
  year = {2014},
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {142--152},
  issn = {2151-2590(Electronic);1864-9335(Print)},
  doi = {10.1027/1864-9335/a000178},
  abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect\textemdash imagined contact reducing prejudice\textemdash showed weak support for replicability. And two effects\textemdash flag priming influencing conservatism and currency priming influencing system justification\textemdash did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
  copyright = {(c) 2014 APA, all rights reserved},
  keywords = {*Cross Cultural Differences,Experimental Replication},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\Y942FCUV\\Klein et al. - 2014 - Investigating variation in replicability A “many .pdf}
}

@article{kleinManyLabsInvestigating2018,
  title = {Many Labs 2: {{Investigating}} Variation in Replicability across Samples and Settings},
  shorttitle = {Many {{Labs}} 2},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E. and {de Bruijn}, Maaike and De Schutter, Leander and Devos, Thierry and {de Vries}, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and {Innes-Ker}, {\AA}se H. and {Jim{\'e}nez-Leal}, William and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Kamilo{\u g}lu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\dj}edovi{\'c}, Janko and {Mena-Pacheco}, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'e}lix and Lee Nichols, Austin and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and {P{\'e}rez-S{\'a}nchez}, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'u}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and {Smith-Castro}, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M. and {van der Hulst}, Marije and {van Lange}, Paul A. M. and {van 't Veer}, Anna Elisabeth and {V{\'a}squez- Echeverr{\'i}a}, Alejandro and Ann Vaughn, Leigh and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  year = {2018},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {4},
  pages = {443--490},
  issn = {2515-2459},
  doi = {10.1177/2515245918810225},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {$<$} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {$<$} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({$<$} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  langid = {english},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\KWH4LYRL\\Klein et al. - 2018 - Many Labs 2 Investigating Variation in Replicabil.pdf}
}

@article{mertonNoteScienceDemocracy1942,
  title = {A Note on Science and Democracy},
  author = {Merton, Robert K.},
  year = {1942},
  journal = {Journal of Legal and Political Sociology},
  volume = {1},
  pages = {115--126},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\5SSDNIVM\\merton 1942 A note on Science and Democracy.pdf}
}

@book{mertonSociologyScienceTheoretical1973,
  title = {The Sociology of Science : Theoretical and Empirical Investigations},
  shorttitle = {The Sociology of Science},
  author = {Merton, Robert King},
  year = {1973},
  publisher = {{Chicago : University of Chicago Press}},
  abstract = {xxxi, 605 p. ; 24 cm; Includes index; Bibliography: p. [561]-575},
  collaborator = {{Internet Archive}},
  isbn = {978-0-226-52091-9},
  langid = {english},
  keywords = {Science -- Social aspects}
}

@article{meyerPracticalTipsEthical2018,
  title = {Practical Tips for Ethical Data Sharing},
  author = {Meyer, Michelle N.},
  year = {2018},
  month = mar,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {1},
  pages = {131--144},
  issn = {2515-2459},
  doi = {10.1177/2515245917747656},
  abstract = {This Tutorial provides practical dos and don'ts for sharing research data in ways that are effective, ethical, and compliant with the federal Common Rule. I first consider best practices for prospectively incorporating data-sharing plans into research, discussing what to say\textemdash and what not to say\textemdash in consent forms and institutional review board applications, tools for data de-identification and how to think about the risks of re-identification, and what to consider when selecting a data repository. Turning to data that have already been collected, I discuss the ethical and regulatory issues raised by sharing data when the consent form either was silent about data sharing or explicitly promised participants that the data would not be shared. Finally, I discuss ethical issues in sharing ``public'' data.},
  langid = {english},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\UC48N6IA\\Meyer - 2018 - Practical Tips for Ethical Data Sharing.pdf}
}

@misc{moranKnowItBad2021,
  title = {``{{I}} Know It's Bad but {{I}} Have Been Pressured into It'': {{Questionable}} Research Practices among Psychology Students in {{Canada}}},
  shorttitle = {``{{I}} Know It's Bad but {{I}} Have Been Pressured into It''},
  author = {Moran, Chelsea and Richard, Alexandra and Wilson, Kate and Twomey, Rosie and Coroiu, Adina},
  year = {2021},
  month = jul,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/kjby3},
  abstract = {Questionable research practices (QRPs) have been identified as a driving force of the replication crisis in the field of psychological science. The aim of this study was to assess the frequency and reasons for QRP use among psychology students in Canadian universities. Participants were psychology students attending Canadian universities recruited via online advertising and email invitations. Respondents were asked how often they and others engaged in seven QRPs, to estimate the proportion of psychology research impacted by each QRP and how acceptable they found each QRP. Data were collected through Likert-scale survey items and open-ended text responses between May 2020 and January 2021, and analyzed using descriptive statistics and thematic analysis. 425 psychology students completed the survey (40\% undergraduate, 59\% graduate, 1\% post-doctoral fellows). Overall, 64\% of participants reported using at least one QRP, while 79\% reported having observed others engaging in at least one QRP. The most frequently reported QRPs were p-hacking (46\%), not submitting null results for publication (31\%) and excluding outcome measures (30\%). These QRPs were also the most frequently observed in others, estimated to be the most prevalent in the field, and rated as the most acceptable. Qualitative findings revealed that students are aware of external pressures that promote QRP use and offered several ideas for alternatives and solutions. The results of this study highlight the need to examine the pedagogical standards and cultural norms in academia that may promote or normalize QRPs in psychological science.},
  keywords = {Meta-science,other,Psychology,Social and Behavioral Sciences},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\477WU3TG\\Moran et al. - 2021 - “I know it's bad but I have been pressured into it.pdf}
}

@article{nosekPreregistrationRevolution2018,
  title = {The Preregistration Revolution},
  author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
  year = {2018},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2600--2606},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708274114},
  abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes\textemdash a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
  copyright = {\textcopyright{} 2018 . Published under the PNAS license.},
  langid = {english},
  pmid = {29531091},
  keywords = {confirmatory analysis,exploratory analysis,methodology,open science,preregistration},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\9CWZQV48\\Nosek et al. - 2018 - The preregistration revolution.pdf}
}

@article{nuijtenPrevalenceStatisticalReporting2016,
  title = {The Prevalence of Statistical Reporting Errors in Psychology (1985\textendash 2013)},
  author = {Nuijten, Mich{\`e}le B. and Hartgerink, Chris H. J. and {van Assen}, Marcel A. L. M. and Epskamp, Sacha and Wicherts, Jelte M.},
  year = {2016},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {48},
  number = {4},
  pages = {1205--1226},
  issn = {1554-3528},
  doi = {10.3758/s13428-015-0664-2},
  abstract = {This study documents reporting errors in a sample of over 250,000 p-values reported in eight major psychology journals from 1985 until 2013, using the new R package ``statcheck.'' statcheck retrieved null-hypothesis significance testing (NHST) results from over half of the articles from this period. In line with earlier research, we found that half of all published psychology papers that use NHST contained at least one p-value that was inconsistent with its test statistic and degrees of freedom. One in eight papers contained a grossly inconsistent p-value that may have affected the statistical conclusion. In contrast to earlier findings, we found that the average prevalence of inconsistent p-values has been stable over the years or has declined. The prevalence of gross inconsistencies was higher in p-values reported as significant than in p-values reported as nonsignificant. This could indicate a systematic bias in favor of significant results. Possible solutions for the high prevalence of reporting inconsistencies could be to encourage sharing data, to let co-authors check results in a so-called ``co-pilot model,'' and to use statcheck to flag possible inconsistencies in one's own manuscript or during the review process.},
  langid = {english},
  keywords = {False positives,NHST,p-values,Publication bias,Questionable research practices,Reporting errors,Significance},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\W4PWWNTV\\Nuijten et al. - 2016 - The prevalence of statistical reporting errors in .pdf}
}

@article{ogradyPsychologyReplicationCrisis2020,
  title = {Psychology's Replication Crisis Inspires Ecologists to Push for More Reliable Research},
  author = {O'Grady, Cathleen and {2020} and Pm, 2:05},
  year = {2020},
  month = dec,
  journal = {Science},
  abstract = {New society aims for transparency and culture change in ecology},
  langid = {english}
}

@techreport{openscienceworkinggroup2017SurveyOpen2018,
  title = {2017 Survey on Open Research Practices Administered by the {{School}} of {{Psychology}}, {{Cardiff University}}},
  author = {Open Science Working Group},
  year = {2018},
  month = jun,
  institution = {{Cardiff University}},
  abstract = {Hosted on the Open Science Framework},
  langid = {english}
}

@article{piperScienceHasBeen2020,
  title = {Science Has Been in a "Replication Crisis" for a Decade. {{Have}} We Learned Anything?},
  author = {Piper, Kelsey},
  year = {2020},
  month = oct,
  journal = {Vox},
  abstract = {Bad papers are still published. But some other things might be getting better.},
  langid = {english}
}

@article{rubinWhenDoesHARKing2017,
  title = {When Does {{HARKing}} Hurt? {{Identifying}} When Different Types of Undisclosed Post Hoc Hypothesizing Harm Scientific Progress},
  shorttitle = {When Does Harking Hurt?},
  author = {Rubin, Mark},
  year = {2017},
  month = dec,
  journal = {Review of General Psychology},
  volume = {21},
  number = {4},
  pages = {308--320},
  publisher = {{SAGE Publications Inc}},
  issn = {1089-2680},
  doi = {10.1037/gpr0000128},
  abstract = {Hypothesizing after the results are known, or HARKing, occurs when researchers check their research results and then add or remove hypotheses on the basis of those results without acknowledging this process in their research report (Kerr, 1998). In the present article, I discuss 3 forms of HARKing: (a) using current results to construct post hoc hypotheses that are then reported as if they were a priori hypotheses; (b) retrieving hypotheses from a post hoc literature search and reporting them as a priori hypotheses; and (c) failing to report a priori hypotheses that are unsupported by the current results. These 3 types of HARKing are often characterized as being bad for science and a potential cause of the current replication crisis. In the present article, I use insights from the philosophy of science to present a more nuanced view. Specifically, I identify the conditions under which each of these 3 types of HARKing is most and least likely to be bad for science. I conclude with a brief discussion about the ethics of each type of HARKing.},
  langid = {english},
  keywords = {accommodation,falsification,HARKing,prediction,replication crisis},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\956Q2S4P\\Rubin - 2017 - When Does HARKing Hurt Identifying When Different.pdf}
}

@article{schonbrodtTrainingStudentsOpen2019,
  title = {Training Students for the {{Open Science}} Future},
  author = {Sch{\"o}nbrodt, Felix},
  year = {2019},
  month = oct,
  journal = {Nature Human Behaviour},
  volume = {3},
  number = {10},
  pages = {1031--1031},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0726-z},
  langid = {english},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\WQEP3GAM\\Schönbrodt - 2019 - Training students for the Open Science future.pdf}
}

@article{schopfelReadyFutureSurvey2016,
  title = {Ready for the Future? {{A}} Survey on Open Access with Scientists from the {{French National Research Center}} ({{CNRS}})},
  shorttitle = {Ready for the Future?},
  author = {Sch{\"o}pfel, Joachim and Ferrant, Coline and Andr{\'e}, Francis and Fabre, Renaud},
  year = {2016},
  month = jan,
  journal = {Interlending \& Document Supply},
  volume = {44},
  number = {4},
  pages = {141--149},
  publisher = {{Emerald Group Publishing Limited}},
  issn = {0264-1615},
  doi = {10.1108/ILDS-06-2016-0023},
  abstract = {Purpose This paper aims to present empirical evidence on the opinion and behaviour of French scientists (senior management level) regarding open access (OA) to scientific and technical information. Design/methodology/approach The results are part of a nationwide survey on scientific information and documentation with 432 directors of French public research laboratories conducted by the French National Research Center (CNRS) in 2014. Findings The CNRS senior research managers (laboratory directors) globally share the positive opinion towards OA revealed by other studies with researchers from the UK, Germany, the USA and other countries. However, they are more supportive of open repositories (green road) than of OA journal publishing (gold). The response patterns reveal a gap between generally positive opinions about OA and less supportive behaviours, principally publishing articles with article processing charges (APCs). A small group of senior research managers does not seem to be interested in green or gold OA and reluctant to self-archiving and OA publishing. Similar to other studies, the French survey confirms disciplinary differences, i.e. a stronger support for self-archiving of records and documents in HAL by scientists from Mathematics, Physics and Informatics than from Biology, Earth Sciences and Chemistry; and more experience and positive feelings with OA publishing and payment of APCs in Biology than in Mathematics or in Social Sciences and Humanities. Disciplinary differences and specific French factors are discussed, in particular in the context of the new European policy in favour of Open Science. Originality/value For the first time, a nationwide survey was conducted with the senior research management level from all scientific disciplines. The response rate was high ({$>$}30 per cent), and the results provide good insight into the real awareness, support and uptake of OA by senior research managers who provide both models (examples for good practice) and opinion leadership.},
  keywords = {OA publishing,Open access,Open repositories,Open science,Research management,Self-archiving},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\7KBPT724\\Schöpfel et al. - 2016 - Ready for the future A survey on open access with.pdf}
}

@techreport{simmons21WordSolution2012,
  type = {{{SSRN Scholarly Paper}}},
  title = {A 21 Word Solution},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2012},
  month = oct,
  number = {ID 2160588},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  abstract = {One year after publishing "False-Positive Psychology," we propose a simple implementation of disclosure that requires but 21 words to achieve full transparency. This article is written in a casual tone. It includes phone-taken pictures of milk-jars and references to ice-cream and sardines.},
  keywords = {disclosure,false positive,p-hacking,psycholog,transparency},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\8G7XK2RD\\Simmons et al. - 2012 - A 21 word solution.pdf}
}

@article{szollosiPreregistrationWorthwhile2020,
  title = {Is Preregistration Worthwhile?},
  author = {Szollosi, Aba and Kellen, David and Navarro, Danielle J. and Shiffrin, Richard and {van Rooij}, Iris and Van Zandt, Trisha and Donkin, Chris},
  year = {2020},
  month = feb,
  journal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {2},
  pages = {94--95},
  issn = {1879-307X},
  doi = {10.1016/j.tics.2019.11.009},
  langid = {english},
  pmid = {31892461},
  keywords = {inference,preregistration,theory development},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\S3S93QWY\\Szollosi et al. - 2020 - Is Preregistration Worthwhile.pdf}
}

@article{thalmayerNeglected95Revisited2021,
  title = {The Neglected 95\% Revisited: {{Is American}} Psychology Becoming Less {{American}}?},
  shorttitle = {The Neglected 95\% Revisited},
  author = {Thalmayer, Amber Gayle and Toscanelli, Cecilia and Arnett, Jeffrey Jensen},
  year = {2021},
  journal = {American Psychologist},
  volume = {76},
  number = {1},
  pages = {116--129},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1935-990X(Electronic),0003-066X(Print)},
  doi = {10.1037/amp0000622},
  abstract = {The field of psychology prides itself on being a data-driven science. In 2008, however, Arnett brought to light a major weakness in the evidence on which models, measures, and theories in psychology rest. He demonstrated that the most prominent journals in six subdisciplines of psychology focused almost exclusively (over 70\% of samples and authors) on a cultural context, the United States, shared by only 5\% of the world's population. How can psychologists trust that these models and results generalize to all humans, if the evidence comes from a small and unrepresentative portion of the global population? Arnett's analysis, cited over 1,300 times since its publication, appears to have galvanized researchers to think more globally. Social scientists from the United States have increasingly sought ways to collaborate with colleagues abroad. Ten years later, an analysis of the same 6 journals for the period of 2014 to 2018 indicates that the authors and samples are now on average a little over 60\% American based. The change is mainly due to an increase in authorship and samples from other English-speaking and Western European countries. Thus, it might be said that 11\% of the world's population is now represented in these top psychology journals, but that 89\% of the world's population continues to be neglected. Majority world authors and samples (4\textendash 5\%) are still sorely lacking from the evidence base. Psychology still has a long way to go to become a science truly representative of human beings. Several specific recommendations are discussed. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {Cross Cultural Psychology,Majority Groups,Meta Analysis,Philosophies,Psychologists,Reporting Standards,Sciences,Scientific Communication},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\XSQLXBV6\\Thalmayer et al. - 2021 - The neglected 95% revisited Is American psycholog.pdf}
}

@article{TQMP16-4-376,
  title = {Does Preregistration Improve the Credibility of Research Findings?},
  author = {Rubin, Mark},
  year = {2020},
  journal = {The Quantitative Methods for Psychology},
  volume = {16},
  number = {4},
  pages = {376--390},
  publisher = {{TQMP}},
  doi = {10.20982/tqmp.16.4.p376},
  abstract = {Preregistration entails researchers registering their planned research hypotheses, methods, and analyses in a time-stamped document before they undertake their data collection and analyses. This document is then made available with the published research report to allow readers to identify discrepancies between what the researchers originally planned to do and what they actually ended up doing. This historical transparency is supposed to facilitate judgments about the credibility of the research findings. The present article provides a critical review of 17 of the reasons behind this argument. The article covers issues such as HARKing, multiple testing, p-hacking, forking paths, optional stopping, researchers' biases, selective reporting, test severity, publication bias, and replication rates. It is concluded that preregistration's historical transparency does not facilitate judgments about the credibility of research findings when researchers provide contemporary transparency in the form of (a) clear rationales for current hypotheses and analytical approaches, (b) public access to research data, materials, and code, and (c) demonstrations of the robustness of research conclusions to alternative interpretations and analytical approaches.}
}

@misc{tuncaUygunTuncRmounce2021,
  type = {Tweet},
  title = {@uygun\_tunc @rmounce @{{Meta}}\_{{Psy Most}} Na\"ive Answers {{I}} Heard so Far during Open Science Seminars: -(Re Open Access) {{I}} Thought Those Publishers Were Nonprofit and They Give Back to Academia Somehow (Senior Academic) -(Ms Student) {{I}} Assumed Peer Review Checks for Stats and the Journal Store a Copy of the Dataset},
  author = {Tunca,, Barak},
  year = {2021},
  month = mar,
  journal = {@tuncaburak},
  langid = {english}
}

@misc{uygun-tuncLastNightWas2021,
  type = {Tweet},
  title = {Last Night {{I}} Was Talking to a Friend Who Was Rather Unfamiliar with the Publication Scene and {{I}} Described How @{{Meta}}\_{{Psy}} Requires Open Data/Code and Has Open {{PR}}. {{Then}} She Asked: {{Of}} Course That Makes Total Sense, but Isn't What All Journals Already Do?😅},
  author = {{Uygun-Tunc}, Duygu},
  year = {2021},
  month = mar,
  journal = {@uygun\_tunc},
  langid = {english}
}

@inproceedings{vazireWeWantBe2019,
  title = {Do We Want to Be Credible or Incredible?},
  booktitle = {Association for {{Interdisciplinary Meta-research}} \& {{Open Science}} Conference},
  author = {Vazire, S.},
  year = {2019},
  address = {{Melbourne}}
}

@misc{vazireWhereAreSelfcorrecting2020,
  title = {Where Are the Self-Correcting Mechanisms in Science?},
  author = {Vazire, S. and Holcombe, Alex O.},
  year = {2020},
  month = aug,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/kgqzt},
  abstract = {It is often said that science is self-correcting, but the replication crisis suggests that, at least in some fields, self-correction mechanisms have fallen short of what we might hope for. How can we know whether a particular scientific field has effective self-correction mechanisms, that is, whether its findings are credible? The usual processes that supposedly provide mechanisms for scientific self-correction \textendash{} mainly peer review and disciplinary committees \textendash{} have been inadequate. We argue for more verifiable indicators of a field's commitment to self-correction. These include transparency, which is already a target of many reform efforts, and critical appraisal, which has received less attention. Only by obtaining Measurements of Observable Self-Correction (MOSCs) can we begin to evaluate the claim that ``science is self-correcting.''  We expect the validity of this claim to vary across fields and subfields, and suggest that some fields, such as psychology and biomedicine, fall far short of an appropriate level of transparency and, especially, critical appraisal.  Fields without robust, verifiable mechanisms for transparency and critical appraisal cannot reasonably be said to be self-correcting, and thus do not warrant the credibility often imputed to science as a whole.},
  keywords = {criticism,metaresearch,metascience,research quality,self-correction,Social and Behavioral Sciences,Theory and Philosophy of Science,transparency},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\24E4MRG9\\Vazire and Holcombe - 2020 - Where Are The Self-Correcting Mechanisms In Scienc.pdf}
}

@article{wagenmakersAgendaPurelyConfirmatory2012,
  title = {An Agenda for Purely Confirmatory Research},
  author = {Wagenmakers, Eric-Jan and Wetzels, Ruud and Borsboom, Denny and {van der Maas}, Han L. J. and Kievit, Rogier A.},
  year = {2012},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {632--638},
  issn = {1745-6916},
  doi = {10.1177/1745691612463078},
  abstract = {The veracity of substantive research claims hinges on the way experimental data are collected and analyzed. In this article, we discuss an uncomfortable fact that threatens the core of psychology's academic enterprise: almost without exception, psychologists do not commit themselves to a method of data analysis before they see the actual data. It then becomes tempting to fine tune the analysis to the data in order to obtain a desired result\textemdash a procedure that invalidates the interpretation of the common statistical tests. The extent of the fine tuning varies widely across experiments and experimenters but is almost impossible for reviewers and readers to gauge. To remedy the situation, we propose that researchers preregister their studies and indicate in advance the analyses they intend to conduct. Only these analyses deserve the label ``confirmatory,'' and only for these analyses are the common statistical tests valid. Other analyses can be carried out but these should be labeled ``exploratory.'' We illustrate our proposal with a confirmatory replication attempt of a study on extrasensory perception.},
  langid = {english},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\VV2NL9PV\\Wagenmakers et al. - 2012 - An agenda for purely confirmatory research.pdf}
}

@article{wichertsPoorAvailabilityPsychological2006,
  title = {The Poor Availability of Psychological Research Data for Reanalysis},
  author = {Wicherts, Jelte M. and Borsboom, Denny and Kats, Judith and Molenaar, Dylan},
  year = {2006},
  journal = {American Psychologist},
  volume = {61},
  number = {7},
  pages = {726--728},
  issn = {1935-990X(Electronic),0003-066X(Print)},
  doi = {10.1037/0003-066X.61.7.726},
  abstract = {The origin of the present comment lies in a failed attempt to obtain, through e-mailed requests, data reported in 141 empirical articles recently published by the American Psychological Association (APA). Our original aim was to reanalyze these data sets to assess the robustness of the research findings to outliers. We never got that far. In June 2005, we contacted the corresponding author of every article that appeared in the last two 2004 issues of four major APA journals. Because their articles had been published in APA journals, we were certain that all of the authors had signed the APA Certification of Compliance With APA Ethical Principles, which includes the principle on sharing data for reanalysis. Unfortunately, 6 months later, after writing more than 400 e-mails--and sending some corresponding authors detailed descriptions of our study aims, approvals of our ethical committee, signed assurances not to share data with others, and even our full resumes-we ended up with a meager 38 positive reactions and the actual data sets from 64 studies (25.7\% of the total number of 249 data sets). This means that 73\% of the authors did not share their data. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Compliance,Data Collection,Data Sharing,Experimentation,Professional Ethics,Scientific Communication}
}

@article{wigboldusEncouragePlayingData2016,
  title = {Encourage Playing with Data and Discourage Questionable Reporting Practices},
  author = {Wigboldus, Daniel H. J. and Dotsch, Ron},
  year = {2016},
  month = mar,
  journal = {Psychometrika},
  volume = {81},
  number = {1},
  pages = {27--32},
  issn = {1860-0980},
  doi = {10.1007/s11336-015-9445-1},
  langid = {english},
  keywords = {reporiting},
  file = {C\:\\Users\\mnwillia\\Zotero\\storage\\ISFL8W62\\Wigboldus and Dotsch - 2016 - Encourage Playing with Data and Discourage Questio.pdf}
}

@article{yongPsychologyReplicationCrisis2016,
  title = {Psychology's Replication Crisis Can't Be Wished Away},
  author = {Yong, Ed},
  year = {2016},
  month = mar,
  journal = {The Atlantic},
  abstract = {It has a real and heartbreaking cost.},
  langid = {american}
}

@article{yongPsychologyReplicationCrisis2018,
  title = {Psychology's Replication Crisis Is Running out of Excuses},
  author = {Yong, Ed},
  year = {2018},
  month = nov,
  journal = {The Atlantic},
  abstract = {Another big project has found that only half of studies can be repeated. And this time, the usual explanations fall flat.},
  chapter = {Science},
  langid = {english}
}

@article{errington_challenges_2021,
	title = {Challenges for assessing replicability in preclinical cancer biology},
	volume = {10},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.67995},
	doi = {10.7554/eLife.67995},
	abstract = {We conducted the Reproducibility Project: Cancer Biology to investigate the replicability of preclinical research in cancer biology. The initial aim of the project was to repeat 193 experiments from 53 high-impact papers, using an approach in which the experimental protocols and plans for data analysis had to be peer reviewed and accepted for publication before experimental work could begin. However, the various barriers and challenges we encountered while designing and conducting the experiments meant that we were only able to repeat 50 experiments from 23 papers. Here we report these barriers and challenges. First, many original papers failed to report key descriptive and inferential statistics: the data needed to compute effect sizes and conduct power analyses was publicly accessible for just 4 of 193 experiments. Moreover, despite contacting the authors of the original papers, we were unable to obtain these data for 68\% of the experiments. Second, none of the 193 experiments were described in sufficient detail in the original paper to enable us to design protocols to repeat the experiments, so we had to seek clarifications from the original authors. While authors were extremely or very helpful for 41\% of experiments, they were minimally helpful for 9\% of experiments, and not at all helpful (or did not respond to us) for 32\% of experiments. Third, once experimental work started, 67\% of the peer-reviewed protocols required modifications to complete the research and just 41\% of those modifications could be implemented. Cumulatively, these three factors limited the number of experiments that could be repeated. This experience draws attention to a basic and fundamental concern about replication – it is hard to assess whether reported findings are credible.},
	urldate = {2022-03-14},
	journal = {eLife},
	author = {Errington, Timothy M and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
	editor = {Rodgers, Peter and Franco, Eduardo},
	month = dec,
	year = {2021},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {open data, open science, preregistration, replication, reproducibility, Reproducibility Project: Cancer Biology},
	pages = {e67995},
	file = {Full Text PDF:C\:\\Users\\beau0075\\Zotero\\storage\\CUKRQ6W5\\Errington et al. - 2021 - Challenges for assessing replicability in preclini.pdf:application/pdf},
}

@article{camerer_evaluating_2016,
	title = {Evaluating replicability of laboratory experiments in economics},
	volume = {351},
	url = {https://www-science-org.ezproxy.flinders.edu.au/doi/10.1126/science.aaf0918},
	doi = {10.1126/science.aaf0918},
	number = {6280},
	urldate = {2022-03-14},
	journal = {Science},
	author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
	month = mar,
	year = {2016},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1433--1436},
	file = {Full Text PDF:C\:\\Users\\beau0075\\Zotero\\storage\\6EKMI3C7\\Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf:application/pdf},
}


