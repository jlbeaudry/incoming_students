---
title: What do incoming university students believe about open science practices in
  psychology?
author:
- name: Jennifer L. Beaudry
  affiliation: '1'
  corresponding: yes
  address: Postal address
  email: jbeaudry@swin.edu.au
  role: null
- name: Michael C. Philipp
  affiliation: '2'
  role: null
- name: Matt N. Williams
  affiliation: '2'
  role: null
shorttitle: Psychology students' beliefs about open science
output:
  papaja::apa6_pdf: default
  papaja::apa6_word: default
authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.
  Enter author note here.
abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  One or two sentences to put the results into a more **general context**.
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  <!-- https://tinyurl.com/ybremelq -->
keywords: open science, psychology, teaching, reproducibility, replication
wordcount: X
bibliography: ['r-references.bib','Undergrads & Open Science (public).bib']
floatsintext: yes
figurelist: no
tablelist: no
footnotelist: no
linenumbers: yes
mask: yes
draft: no
documentclass: apa6
classoption: man
affiliation:
- id: '1'
  institution: Swinburne University of Technology
- id: '2'
  institution: Massey University
---
<!-- I've only partially entered authorship deets for the moment and set it to mask authorship info when knitting
It's set to default to knit to .docx, but can also knit to pdf (if you have a tex distro installed)
The pdf looks nicer and might be good for a preprint but .docx probably easiest to use for journal submissions.
To knit to RMarkdown you will probably need the papaja package installed, and to knit to .pdf you will need a tex distro (e.g., TinyTex).
However, it is possible to run all the R code (without knitting to .docx or .pdf) by clicking Run > Run All Chunks Below.
This could be useful in future for readers interested in reproducing results but who don't have or wish to install 
papaja and its many dependencies.
-->

<!-- The .bib file is now just from our shared library -->

```{r setup, include = FALSE}
library(papaja)
library(here)
library(tidyverse)

r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r import data}

# import the data 
df <- read.csv (here::here("data", "students_processed.csv"))

# convert variables to factors as needed
df$age <- as_factor(df$age)
df$eligibility <- as_factor(df$eligibility)

# convert factors to characters as needed
df$university <- as.character(df$university)
df$degree <- as.character(df$degree)
df$major <- as.character(df$major)
df$major <- as.character(df$major)
# df$uni_country <- as.character(df$uni_country)
# df$nationality <- as.character(df$nationality) [i think these should be factors]

# # import the metadata [check if I need it for these analyses?]
# metadata <- here::here("survey", "data", "os_metadata_raw_data.csv") %>% 
#   read_csv(col_names = TRUE, skip_empty_rows = TRUE) %>% 
#   filter(!is.na(OldVariable))
```

<!--MP ➔ The paragraph below is an attempt to segue to the aim of our study before getting into the lit review – I'm always of fan of making the general aim of the study clear by the end of the first paragraph. I've summarised the sentiment of the original first two paragraphs a bit (and struck out tangential sentences). I'd suggest this could make the first paragraph of the paper – highlighting the the research context and the gap that is driving our study--> 

The last decade has seen unprecedented change in methodological and reporting practices in psychology. These changes were partly precipitated by what is popularly known as the "replication crisis": The discovery that close replications of published psychological studies are often unable to replicate the original findings[@kleinInvestigatingVariationReplicability2014; @kleinManyLabsInvestigating2018]. These apparent problems with replication have lead to a variety of potential solutions to make research practices more transparent, including more frequent publication of replication studies [see @brandtReplicationRecipeWhat2014], more thorough reporting of methods and results [@simmons21WordSolution2012], open sharing of data [see @meyerPracticalTipsEthical2018], and preregistration of data collection and analysis plans [@nosekPreregistrationRevolution2018]. These calls for more transparent research practices prompted the discipline to reflect on the norms and beliefs that underpin its practice of research. Many studies have polled researchers' understandings of and adherence to open science norms, beliefs, and practices [e.g., @baker500ScientistsLift2016; @openscienceworkinggroup2017SurveyOpen2018, @harrisUseReproducibleResearch2018). In turn, initiatives have been directed toward training undergraduate students in the adoption of an open science ethos [e.g., @chopikHowWhetherTeach2018; @graheHarnessingUndiscoveredResource2012, @jekelHowTeachOpen2020, @schonbrodtTrainingStudentsOpen2019].  These initiatives help engrain open science norms and change attitudes about research practices, but we know little about what these students know or believe about open science research practices prior to entering the university classroom.

Many of these recent changes have links to the classic *Mertonian* norms of science [@mertonSociologyScienceTheoretical1973]. For example, the practice of sharing open source software directly corresponds to the Mertonian norm of *communism*: Scientists should have common ownership of scientific goods. Similarly, the practices of sharing of preprints for open peer review and open data for checking of reproducibility corresponds to the norm of communality, but also to that of *organised skepticism*: Scientific claims should be subjected to critical scrutiny. Preregistration can be connected to the norm of *disinterestedness*: By making (and preregistering) decisions about how to analyse data before results are produced, a researcher can limit the degree to which the substantive results produced by different analytic strategies affect their decisions regarding which analyses to report. In this sense, the ongoing reform in psychological research can partly be understood as simultaneously a set of new practices and a re-affirmation of old norms.

Nevertheless, rapid changes in methodological practice and empirical findings present significant pedagogical challenges for the teacher of psychology. Keeping textbooks and other instructional materials up to date is  difficult when supposedly well-established findings are being contradicted by new replications emerging at a rapid pace. Furthermore, training in emerging methodological practices is crucial for graduate students who may go on to apply psychological research methods themselves, so that the next generation of researchers can produce research which is more replicable than that of the last. Even for students who do not go on to conduct research themselves, an understanding of contemporary methodological practices - and problems with methodological practices in psychology - is essential for these students to become informed and critical consumers of knowledge about psychology. However, the question of how best to provide this understanding is by no means trivial to answer. Indeed, there are several reasons why it might be unwise to assume a simple knowledge deficit, where students lack knowledge about reproducibility and open science practices and the teacher's only role is to communicate this knowledge. 

First, unlike some areas of psychology covered in undergraduate courses (e.g., models of working memory, or the internal workings of human senses), the replication crisis is frequently discussed in mainstream and social media [e.g., @yongPsychologyReplicationCrisis2016; @yongPsychologyReplicationCrisis2018; @ogradyPsychologyReplicationCrisis2020]. Many students may plausibly have some knowledge about these issues obtained prior to (or independently of) their formal studies. Teaching methods should thus be informed by some understanding of what students' pre-existing levels of knowledge are. 

Second, when psychology students are beginning their university studies, they are often learning for the first time about how and why it is useful to apply scientific methods to studying human behaviour (rather than only relying on alternative sources of knowledge such as intuition or anecdote or authority). Could an over-emphasis on problems with replicability leave such students unconvinced that scientific methods for studying human behaviour are valuable *at all*, leaving them to favour even less credible alternative sources of knowledge? It is therefore important to determine what levels of trust in psychological research (and its replicability) are prevalent in incoming undergraduate students. In their examination of the usefulness of a one-hour lecture about the replication crisis, @chopikHowWhetherTeach2018 found that undergraduate students trusted the results of studies by psychologists less after the lecture than they did before it, although the effect size was fairly small (d = -.36), and mean trust levels remained fairly high after the lecture (M = 4.94 on a scale of 1 to 7). 

Third, anecdotal evidence suggests that students being taught about open science practices and reforms to improve reproducibility (such as open sharing of data and analysis code) are often surprised that this is not *already* standard practice. In other words, students' naive conception of how science works may in some ways be closer to that embodied in recent reforms rather than "business as usual" practices. Indeed, surveys of scientists' subscriptions to Mertonian norms have often found that while most scientists endorse Mertonian norms such as communality, universalism and disinterestedness, this is by no means the case for all scientists. Furthermore, they typically perceive the behaviour of other scientists as being less consistent with these norms than their own [@andersonNormativeDissonanceScience2007]. It is thus possible that, for some students, teaching about reform may be less a matter of conveying new information and more one of reinforcing "naive" assumptions. In fact, although beyond the scope of this article, it is *possible* that incoming undergraduate students more strongly endorse open science norms than do academics.

<!-- I think it'd be useful here if we were able to point to examples of the anecdotal evidence mentioned above - anyone come across anything (even just on social media?) 
Also maybe add more about what we know about academics' endorsement of open science norms.
-->

These considerations suggest that it is important that teachers wishing to inform undergraduate psychology students about the replication crisis and open science practices have some understanding of what such students actually know and believe about these topics already. In addition, undergraduate psychology students themselves represent one of the most important audiences for psychological research: The number of undergraduate students enrolled in psychology courses in any given year dwarfs the number of academics working in psychology. As such, the *preferences* of these students with respect to reproducibility and open science practices are themselves important as a phenomenon of interest. If, for example, undergraduate psychology students have a strong preference that journal articles are freely available to members of the public, this preference on the part of some of the consumers of the knowledge we produce should have some bearing on our choices in relation to sharing of manuscripts. This is especially the case given that undergraduate students are directly or indirectly responsible for much of the funding which allows universities to operate and research to be conducted. In fact, in serving as participants for course credit in many universities, undergraduate psychology students also provide the data underlying much psychological research.

<!-- I'm not sure about the argument in the above paragraph - might be somewhat of a stretch? -->

For these reasons, we aimed to conduct a study describing what incoming undergraduate students of psychology believe about reproducibility and open science practices in psychology. Our survey encompassed norms (how students felt research *should* be conducted), norms in practice (how students believe psychological research *is* conducted) and replicability (how replicable students believe psychological research is). In doing so we hope to provide knowledge which can inform the pedagogy of teaching about replication and open science practices. This study is exploratory [see @wagenmakersAgendaPurelyConfirmatory2012] and descriptive, and does not involve the specification or testing of hypotheses.

# Methods
We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

```{r inclusion}

# participant numbers (all & included cases)
df_all <- df
n_all <- nrow(df_all)

# only participants who met the inclusion criterion
df <- df_all %>% 
  filter(exclude %in% "include") 
n <- nrow(df)

ex <- df_all %>% 
  filter(exclude %in% "exclude") %>% # n for those excluded
  nrow()

# note re: exclusion [all coded in `exclude` in preprocessing, 
# but need to summarise exclusion criteria here]

age_ex <- df_all %>%
  filter(age_criteria %in% "exclude") %>% 
  nrow()

el <- df_all %>%
  filter(eligibility_criteria %in% "exclude") %>% 
  dplyr::count(eligibility_lab) # need the breakdown by condition

status <- df_all %>% 
  filter(status_criteria %in% "exclude") %>% 
  dplyr::count(status_lab) # need the breakdown by condition

nmiss <- df_all %>% 
  filter(nmiss_criteria %in% "exclude") %>% 
  nrow()

```


```{r demographics for those included in analyses}

# age
age <- df %>% dplyr::count(age_lab)

# gender --> need to run through gendercoder [breadcrumb!]

# high school status
hs_status <- df %>% dplyr::count(high_school_lab)

# high school psyc 
hs_psyc <- df %>% dplyr::count(psych_hs_lab)

# country of university 
uni_country <- df %>% dplyr::count(uni_country)

# country of nationality
nationality <- df %>% dplyr::count(nationality)

# whether parents' attended uni
first_gen <- df %>% dplyr::count(first_gen_lab)
```


## Participants

Of those who started the survey (*n* = `r n_all`), we screened out `r ex` 
participants who were not eligible to participate based on the combination of our
preregistered exclusion criteria. Specifically, we screened out people who were younger than 
18 years (*n* = `r age_ex`). We also screened out individuals who had already 
started a psychology unit at a university, regardless of whether they had 
completed it (*n* = `r el[1,2] + el[3,2] + el[4,2]`); were not enrolled in a 
psychology unit at a university (*n* = `r el[2,2]`); or did not answer this 
question (*n* = `r el[5,2]`). We then excluded additional responses that 
Qualtrics flagged as spam (*n* = `r status [1,2]`) or as a survey preview 
(*n* = `r status [2,2]`). Finally, we excluded participants who met all other 
eligibility criteria, but did not respond to any of the main items
in the study (*n* = `r nmiss`). 

The remaining `r n` participants were eligible based on starting their first unit of study in psychology at 
a university within the next month. For those individuals who were screened out, 
our Qualtrics program automatically directed them to the debriefing form.  

Of the remaining `r n` participants, most reported that they were
18-24 years old (*n* = `r age[1,2]`), `r age [2,2]` 
reported that they were 25-34 years old, `r age [3,2]` were 35-44 years old,
`r age [4,2]` were 45-54 years old, and `r age [5,2]` was 55-64 years old. ADD GENDER HERE!

Nearly all reported that they graduated from high
school or secondary school (*n* = `r hs_status[3,2]`). Of those participants who 
attended high school, a little less than half completed psychology courses in 
high school (*n* = `r hs_psyc[2,2]`). In terms of their nationality, most 
participants reported that they were Australian (*n* = `r nationality[3,2]`), 
from the United Kingdom (*n* = `r nationality[24,2]`), or Chinese 
(*n* = `r nationality[6,2]`). In terms of where the participants were attending 
university, most reported that they were in Australia (*n* = `r uni_country[1,2]`), 
the United Kingdom (*n* = `r uni_country[5,2]`), or New Zealand (*n* = `r uni_country[4,2]`). 
Finally, about 2/3rds of participants reported that at least one of their parents 
attended university (*n* = `r first_gen[2,2]`). 


```{r qual responses}

# once qual responses are coded, I will need to return here to 
 # include details about which univrsity the participant is enrolled in
 # name of qualification
 # major [how many are majoring in psyc; NAs; will decide later?]

```



## Material

## Procedure

## Data analysis
<!-- Can we move this to a reproducible code statement at the end? It works now -->

We used `r cite_r("r-references.bib")` for all our analyses.


# Results



# Discussion


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>

\endgroup
