---
title: What do incoming university students believe about open science practices in
  psychology?
author:
- name: Jennifer L. Beaudry
  affiliation: '1'
  corresponding: yes
  address: Postal address
  email: jbeaudry@swin.edu.au
  role: null
- name: Michael C. Philipp
  affiliation: '2'
  role: null
- name: Matt N. Williams
  affiliation: '2'
  role: null
- name: Emily Kothe
  affiliation: '3'
  role: null
shorttitle: Psychology students' beliefs about open science
output:
  papaja::apa6_word: default
  papaja::apa6_pdf: default
authornote: |
  1 Flinders University, Australia
  2 Massey University, New Zealand
  3 Deakin University, Australia
abstract: |
 BACKGROUND: Understanding students’ naive conceptions about how science works and the norms that guide scientific best practice is important so that teachers can adapt their teaching to the students’ existing understandings.
 OBJECTIVE: To describe what incoming undergraduate students of psychology believe about reproducibility and open science practices in spsychology.
 METHOD: International online survey with participants who were about to start their first course in psychology at a university (N = 239) .
 RESULTS: When asked about how research *should* be done, most students endorsed most (but not all) of ten open science practices. When asked to estimate the proportion of published psychological studies that apply each of a set of 10 open science practices, participants' estimates tended to average near 50%. Only 18% of participants had heard of the term "replication crisis".
 CONCLUSION: Despite relatively significant media attention on the replication crisis, few incoming psychology students are familiar with the term. Incoming students nevertheless appear to be sympathetic toward most open science practices, although they may overestimate the prevalence of these practices in psychology.
 TEACHING IMPLICATIONS: Teaching materials aimed at incoming psychology students should not assume pre-existing knowledge about open science or replicability.

keywords: open science, psychology, teaching, reproducibility, replication
wordcount: X
bibliography: ['r-references.bib','Undergrads & Open Science (public).bib']
floatsintext: yes
figurelist: no
tablelist: no
footnotelist: no
linenumbers: yes
mask: yes
draft: no
documentclass: apa6
classoption: man
affiliation:
- id: '1'
  institution: Swinburne University of Technology
- id: '2'
  institution: Massey University
- id: '3'
  institution: Deakin University, Geelong, Australia, Misinformation Lab, School of Psychology
editor_options: 
  chunk_output_type: console
csl               : "apa.csl" 
---

<!--
Teaching in Psychology ask for structured abstracts - 'For these types of article submissions, please include (in this order) a section on the background, objective(s), method, results, conclusion, and teaching implications.' 200 word limit
Abstract currently includes hand-typed numbers because it's hard to get them to knit up here since data processing is further below.
-->

<!-- I've only partially entered authorship deets for the moment and set it to mask authorship info when knitting. JB: We'll need to discuss authorship order. I would 
like to use CREDIT for our contributions. 
It's set to default to knit to .docx, but can also knit to pdf (if you have a tex distro installed)
The pdf looks nicer and might be good for a preprint but .docx probably easiest to use for journal submissions. 
JB: Yes, that makes sense. 

To knit to RMarkdown you will probably need the papaja package installed, and to knit to .pdf you will need a tex distro (e.g., TinyTex).
However, it is possible to run all the R code (without knitting to .docx or .pdf) by clicking Run > Run All Chunks Below.
This could be useful in future for readers interested in reproducing results but who don't have or wish to install 
papaja and its many dependencies. 
JB: Yes, that's a good note to include for folks.
-->

<!-- The .bib file is now just from our shared library -->

```{r setup, include = FALSE}
library(papaja) #install with devtools1
library(here)
library(tidyverse)
library(finalfit) # to keep trailing zeros & round_tidy
library(psych) #for correlation matrices
library(tidyselect) 
library(psychometric) # for correlation confidence intervals - might not need...
library(broom)

r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
# JB: We aren't generating random numbers though--any reason to keep it?
set.seed(42)
knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed,
  results = 'as.is')
```

```{r functions}

p_tidy <- function (x, digits, prefix = "= ") {
    x.out = paste0(prefix, round_tidy(x, digits))
    all_zeros = paste0(prefix, round_tidy(0, digits))
    less_than = paste0("< ", format(10^-digits, scientific = FALSE))
    x.out[x.out == all_zeros] = less_than
    return(x.out)
}
```


```{r import data}

# import the main data 
df <- read.csv (here::here("data", "students_processed.csv"))

# upload the replication crisis responses, marked as accurate and inaccurate (outside of R)
crisis_descriptions = read.csv(here::here ("data", "crisis_descriptions_marked.csv"))

# convert variables to factors as needed
df$age <- as_factor(df$age)
df$eligibility <- as_factor(df$eligibility)

# convert factors to characters as needed
df$university <- as.character(df$university)
df$degree <- as.character(df$degree)
df$major <- as.character(df$major)
df$major <- as.character(df$major)
df$uni_country <- as.character(df$uni_country)
df$nationality <- as.character(df$nationality)

# import the metadata [if using]
# metadata <- here::here("survey", "data", "students_metadata.csv") %>%
# read_csv(col_names = TRUE, skip_empty_rows = TRUE) %>%
# filter(!is.na(old_variable))

```

The last decade has seen unprecedented change in methodological and reporting practices in psychology. These changes were partly precipitated by what is popularly known as the "replication crisis": The discovery that close replications of published psychological studies are often unable to replicate the original findings [@kleinInvestigatingVariationReplicability2014; @kleinManyLabsInvestigating2018]. These apparent problems with replication have lead to a variety of potential solutions to make research practices more reproducible and transparent, including more frequent publication of replication studies [see @brandtReplicationRecipeWhat2014], more thorough reporting of methods and results [@simmons21WordSolution2012; @wigboldus_encourage_2016], open sharing of data [see @meyerPracticalTipsEthical2018], and preregistration of data collection and analysis plans [@nosekPreregistrationRevolution2018]. These calls for more reproducible and transparent research practices prompted the discipline to reflect on the norms and beliefs that underpin its practice of research. Many studies have polled researchers' understandings of and adherence to open science norms, beliefs, and practices [e.g., @baker500ScientistsLift2016; @openscienceworkinggroup2017SurveyOpen2018; @harrisUseReproducibleResearch2018].

<!-- If we need to cut more words, we could cut the last sentence and combine the references into the previous sentence and add a phrase like "....prompted the discipline to reflect on and measure researchers norms, attitudes, and practices..." -->

The most comprehensive set of principles for how science *ought* to be practiced are Merton's norms of science [@mertonNoteScienceDemocracy1942]. Scientists [@andersonExtendingMertonianNorms2010] and graduate students [@andersonGraduateStudentExperience1994] alike have historically endorsed the normative value of Merton's principles. Furthermore, many of the recent practices designed to make science more transparent and open reflect Mertonian norms [see @vazireWeWantBe2019; @vazireWhereAreSelfcorrecting2020]. For example, the practices of sharing of preprints for open peer review and open data for reproducibility checks corresponds to the norm of communality, and also to that of *organised skepticism*: Scientific claims should be subjected to critical scrutiny. Preregistration can be connected to the norm of *disinterestedness*: By making (and preregistering) decisions about how to analyse data before results are produced, a researcher can limit the degree to which the substantive results produced by different analytic strategies affect their decisions regarding which analyses to report.

Rapid changes in methodological practice and empirical findings present significant pedagogical challenges for the teacher of psychology. Keeping textbooks and other instructional materials up to date is difficult when supposedly well-established findings are contradicted by new replications emerging at a rapid pace. Furthermore, training in emerging methodological practices is crucial for graduate students who may go on to apply psychological research methods themselves. Even for students who do not go on to conduct research themselves, an understanding of contemporary methodological practices – and problems with methodological practices in psychology – is essential for becoming informed and critical consumers of psychological knowledge. Studies have therefore begun to explore strategies for educating psychology students about replicability and open science practices [e.g., @chopikHowWhetherTeach2018; @graheHarnessingUndiscoveredResource2012; @jekelHowTeachOpen2020; @schonbrodtTrainingStudentsOpen2019]. These initiatives may help ingrain open science norms and change attitudes about research practices, but we know little about what these students know or believe about open science research practices prior to entering the university classroom. There are several reasons why this knowledge could be useful.

First, unlike some psychological phenomena in undergraduate courses (e.g., models of working memory, or the internal workings of human senses), the replication crisis is frequently discussed in mainstream and social media [e.g., @ogradyPsychologyReplicationCrisis2020; @yongPsychologyReplicationCrisis2016; @yongPsychologyReplicationCrisis2018]. Students may have plausibly obtained some knowledge about these issues prior to (or independently of) their formal studies. Teaching methods should thus be informed by some understanding of students' pre-existing levels of knowledge. 

Second, anecdotal evidence suggests that some students being taught about open science practices and reforms to improve reproducibility (such as open sharing of data and analysis code) are often surprised that these are not *already* standard practice. This could imply a need for educators to reinforce and build on students' “naive” impressions rather than radically altering their understanding of how science should be done. On the other hand, some evidence suggests that undergraduate students may quickly begin to engage in practices that hamper reproducibility. For example, @moranKnowItBad2021 found that 26.5% of the undergraduate students in their Canadian sample admitted, with regard to their own analyses, “Conducting multiple statistical analyses on the same dataset in an attempt to find a statistically significant result” (i.e., p-hacking), while 9.6% reported rounding down p-values. The extent to which incoming psychology students may – or may not – be “naive open scientists” is useful knowledge for guiding pedagogical approaches.

We conducted a descriptive study examining the beliefs of incoming undergraduate students in a first-year psychology unit about reproducibility and open science practices in psychology. Our survey encompassed questions about norms (how students felt research *should* be conducted), norms in practice (how students believe psychological research *is* conducted) and replicability (how replicable students believe psychological research is). Our study was exploratory [see @wagenmakersAgendaPurelyConfirmatory2012] and descriptive, and does not involve the specification or testing of hypotheses. We preregistered the exploratory nature of this study at https://tinyurl.com/4p64mvwp.

<!-- I wonder about saying something somewhere about how the items we developed (and openly shared) in this study could be useful for teachers who might wish to assess their own students' beliefs about open science practices in psychology? JB: That'd be great in the 
abstract and discussion?
-->

# Methods
We report how we determined our sample size, all data exclusions, all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) --> Our materials, data, and analysis script are available at https://tinyurl.com/yz76f4s6. 

<!-- JB: Is there are a reason we aren't including Simmons et al as a cite? -->

```{r inclusion}

# participant numbers (all & included cases)
df_all <- df
n_all <- nrow(df_all)

# only participants who met the inclusion criterion
df <- df_all %>% 
  filter(exclude %in% "include") 
n <- nrow(df)

ex <- df_all %>% 
  filter(exclude %in% "exclude") %>% # n for those excluded
  nrow()

# note re: exclusion [all coded in `exclude` in preprocessing, 
# but need to summarise exclusion criteria here]

age_ex <- df_all %>%
  filter(age_criteria %in% "exclude") %>% 
  nrow()

el <- df_all %>%
  filter(eligibility_criteria %in% "exclude") %>% 
  dplyr::count(eligibility_lab) # need the breakdown by condition

status <- df_all %>% 
  filter(status_criteria %in% "exclude") %>% 
  dplyr::count(status_lab) # need the breakdown by condition

nmiss <- df_all %>% 
  filter(nmiss_criteria %in% "exclude") %>% 
  nrow()

```


```{r demographics for those included in analyses}

# age
age <- df %>% dplyr::count(age_lab)

# gender --> need to summarise once we've recoded it in preprocessing script
gender <- df %>% dplyr::count(gender)

# high school status
hs_status <- df %>% dplyr::count(high_school_lab)

# high school psyc 
hs_psyc <- df %>% dplyr::count(psych_hs_lab)

# country of university 
uni_country <- df %>% dplyr::count(uni_country)

# country of nationality
nationality <- df %>% dplyr::count(nationality)

# whether parents' attended uni
first_gen <- df %>% dplyr::count(first_gen_lab)
```

## Procedure

Contacts from tertiary institutions in Australia, New Zealand, the UK, and the USA invited students enrolled in their first-year psychology course to participate in the online survey hosted on Qualtrics. An invitation was sent to participants via email or a course website 1–2 weeks before their course began. The invitation specified that student needed to be 18 years or older and starting their first course/unit of study in psychology at a university within the following month, and included a link to the survey.

Ethical approval for this study was granted by (institution blinded).

<!-- JB: I've renamed collaborators to contacts because they aren't on this paper and aren't necessarily people that we've previously collaborated with. -->

<!--MW: I've moved Procedure up above Participants - I think that works better for flow since otherwise we have to describe screening within the survey before we've introduced the survey itself. Hope this is ok. JB: Fine with me. If it's an issue, we can describe the final sample of participants first and then describe the inclusion criteria later.-->

## Participants

Of those who started the survey (*n* = `r n_all`), we screened out `r ex` 
participants who were not eligible to participate based on the combination of our
preregistered exclusion criteria. Specifically, we screened out people who were younger than 18 years (*n* = `r age_ex`). We also screened out individuals who had already 
started a psychology unit at a university, regardless of whether they had 
completed it (*n* = `r el[1,2] + el[3,2] + el[4,2]`); were not enrolled in a 
psychology unit at a university (*n* = `r el[2,2]`); or did not answer this 
question (*n* = `r el[5,2]`). We then excluded additional responses that 
Qualtrics flagged as spam (*n* = `r status [1,2]`) or as a survey preview 
(*n* = `r status [2,2]`). Finally, we excluded participants who met all other 
eligibility criteria, but did not respond to any of the main items
in the study (*n* = `r nmiss`). 

The remaining `r n` participants were eligible based on starting their first unit 
of study in psychology at a university within the next month. Of the eligible 
participants, most reported that they were 18–24 years old (*n* = `r age[1,2]`);
`r age [2,2]` reported that they were 25–34 years old, `r age [3,2]` were 35–44 
years old, `r age [4,2]` were 45–54 years old, and `r age [5,2]` was 55–64 years old. 
The majority of participants (*n* = `r gender[1,2]`) reported that they were female, 
`r gender[2,2]` reported that they were male, `r gender[3,2]` participant reported 
their gender identity as "queer man", and `r gender[3,2]` participant reported 
non-binary gender.

<!--We had a footnote here (below) but given the word limit it might be safely left out unless a reviewer queries it...

^[While the response "queer man" could potentially be coded as
"male", we are aware that some individuals who report gender in this way would
not feel that such reclassification was appropriate. We have retained the original
response in order to provide a faithful representation of the participant's 
self-reported gender.] -->

Most participants reported attending university in Australia (*n* = `r uni_country[1,2]`), 
the United Kingdom (*n* = `r uni_country[5,2]`), or New Zealand (*n* = 
`r uni_country[4,2]`). Nearly all reported graduating from high school or secondary 
school (*n* = `r hs_status[3,2]`). Of those participants who attended high school, 
fewer than half completed psychology courses in high school (*n* = `r hs_psyc[2,2]`).
Participants reported `r nrow(nationality)` different nationalities; the most 
common were Australia (*n* = `r nationality[3,2]`), the United Kingdom (*n* = 
`r nationality[24,2]`), and China (*n* = `r nationality[6,2]`). About two-thirds of 
participants reported that at least one of their parents attended university 
(*n* = `r first_gen[2,2]`). 

## Material

### Open Science Norms and Counternorms

Our approach to measuring norms and counternorms focussed on participants' evaluation of ten specific open science practices that reflected diverse open science norms rather than trying to measure endorsement of the more abstract Mertonian norms (cf. Anderson et al., 2007). <!-- Mention here our discussions workshopping of common open science norms at SIPs? JB: I think we have to, don't we?-->
The practices assessed in our survey included: critical thinking, preregistration, registered reports, p-hacking, hypothesising after the results are known (HARKing), providing sufficent information of replicability, preprints, open materials, open data, and open access publishing. Each practice was elucidated by a pair of items (pro-open science; counter-open science) in the context of a common scenario, "Imagine that Deborah is a psychology researcher who has designed a study to test a specific hypothesis." Although we acknowledge the existence of important debate about the merits of some of these practices [@TQMP16-4-376; @rubinWhenDoesHARKing2017; @szollosiPreregistrationWorthwhile2020], we selected the items to be emblematic of common concerns in the open science movement.

We presented 20 scenarios (see Table \@ref(tab:normcounternormitems)). Participants rated their agreement with each prescriptive statement on a 1 (strongly disagree) to 5 (strongly agree) scale.


```{r normcounternormitems}
norm_items = data.frame(
  "Practice" = c("Critical thinking", "Critical thinking", "HARKing", "HARKing", "Info for replicability", "Info for replicability",
                 "Open access", "Open access", "Open data", "Open data", "Open materials", "Open materials",
                 "p-hacking", "p-hacking", "Preprints", "Preprints", "Preregistration", "Preregistration",
                 "Registered reports", "Registered reports"),
  "Category" = rep(c("Norm", "Counternorm"), times = 10),
  "Item" = c("Deborah used previously published research to inform her research study. Deborah should be critical of the findings published in journals because published research can be wrong.",
             "Deborah used previously published research to inform her research study. Deborah should accept the findings published in journals because journals would not publish research with errors.",
             "Deborah should only describe her study as a test of a hypothesis if she decided on her hypothesis and how she would test it before she started collecting data.",
             "Once Deborah has analysed her results, it would be good scientific practice for her to write her manuscript as if she predicted those results from the beginning.",
             "When reporting the findings of her study, Deborah should describe how she completed the study in enough detail that another researcher could repeat her entire study without having to check any details with her - even if this means including lots of “boring” practical details in her report, or in an appendix.",
             "When reporting the findings of her study, it would be good scientific practice for Deborah to gloss over some of the practical details so she can tell a good story.",
             "Once her study is complete, Deborah should publish her findings in a journal that is free for others to access.",
             "Once her study is complete, Deborah should publish the findings in the most prestigious journal she can, even if that journal charges others a fee to access the report.",
             "When Deborah publishes her study, she should post the anonymous responses from participants online so that anyone can access and use the responses in their own research.",
             "Deborah should keep the participants’ responses from her study protected, so that only she and her research team can access them.",
             "Deborah should share the written materials and measures for her study openly online so that other researchers and members of the public can access and use them.",
             "Deborah should keep the written materials and measures for her study protected, so that only she and her research team can access them.",
             "Deborah should report the findings of all analyses of her data that she conducts.",
             "It would be good scientific practice for Deborah to run many different analyses of her data, and report only those that produce interesting findings.",
             "Deborah should post a manuscript describing the findings of her study openly online as soon as it is complete, even if the manuscript has not yet been checked by experts (peer reviewed) and accepted for publication in a journal.",
             "Deborah should not post a manuscript describing the findings of her study online until after it has been checked by experts (peer reviewed) and accepted for publication in a journal.",
             "Before collecting her data, Deborah should write down what her hypotheses are and how she plans to collect and analyse data. She should then save her plan in an online registry so others can tell what methods and analyses she will use to test her hypotheses after collecting data.",
             "Deborah should decide which data analyses are suitable to test her hypotheses only after looking at her data.",
             "Deborah should submit a plan for her study to a journal to be checked by experts (peer reviewed) before she collects and analyses data.",
             "Deborah should submit her study to a journal to be checked by experts (peer reviewed) only after she has finished collecting and analysing her data."))

# [breadcrumb Sept 10: pull in this info from the meta data]


# c <- metadata %>%
#     filter(scale == "norms")

# add a practice column in metadata

# update this text  
# d <- full_join(OSmost_long, c, by = c("OSmost" = "NewVariable")) %>%
#   rename ("OSmost_label" = "ItemText")


norm_items_wide <- pivot_wider(
  norm_items,
  names_from = Category,
  values_from = Item)

knitr::kable(norm_items_wide,
             caption = "Norm and counternorm items",
             align = c('lll'))
```

\ 

### Beliefs About the Practice of Psychological Science

We also sought to measure participants' normative beliefs about how psychological science is actually conducted by researchers around the world. To that end we asked participants, "When you consider all of the psychology studies conducted globally each year, what percentage of them would have the following characteristics?" These practices corresponded approximately to the ten norm–counternorm pairs described above, with two exceptions (see Table \@ref(tab:normsinpracticeitems)). We excluded one norm because it did not refer to externally observable behaviour (critically evaluating published studies vs. accepting them), and we included one practice out of interest (data sharing on request). Participants indicated their estimate on a slider bar ranging from 0% to 100%.


```{r normsinpracticeitems}
practice_items = data.frame(
  "Practice" = c("Preregistration", "Registered reports", "p-hacking", "HARKing", "Info for replicability",
                 "Preprint", "Open materials", "Open data", "Data on request", "Open access"),
  "Item" = c("The plan for collecting and analysing data for the study is posted in an online registry before data collection starts",
             "A plan for the study is submitted to a journal to be checked by experts (peer reviewed) before the data is collected and analysed",
             "The researchers run many different analyses of the data, and report only those that produce interesting findings",
             "The researchers describe their results as if they predicted those results from the beginning, even if that isn’t actually true",
             "The researchers provide enough practical detail in their written report that another researcher could fully repeat (replicate) their study without needing to ask any questions of the original researchers",
             "A manuscript describing the findings of the study is posted openly online as soon as it is complete, even if the manuscript has not yet been checked by experts (peer reviewed) and accepted for publication in a journal",
             "The written materials and measures for the study are posted openly online so that other researchers and members of the public can access and use them",
             "Once the study is published, the anonymous responses from participants are posted openly online so that anyone can access and use the responses in their own research",
             "Once the study is published, the anonymous responses from participants are available to other researchers on request",
             "The research report describing the study and its findings is published in a journal that is free for anyone to access")
)

  
knitr::kable(practice_items,
             caption = "Norms in practice items",
             align = c('ll'))

#The ordering of this table also doesn't match the one above, which I'm leaving for the moment since we probably want to set
#an order of items/norms and stick to it consistently throughout

```

\ 

### Beliefs About Replications

Participants' beliefs about the replicability of results in psychological science were indexed by two questions. The first asked participants to, 

> "Imagine that a set of researchers selected 100 published studies about psychology, and repeat each study again, exactly how it was described in the original report but with a new set of participants. We call repeating a study like this a 'replication study.' At a guess, how many of these replication studies do you think would produce the same conclusions as the original studies?”

Estimates were recorded using a slider bar ranging from 0% to 100%. 
A second question asked participants whether they had heard of the replication crisis. Those who had heard of the replication crisis were asked two additional open-ended questions. First, they were asked to describe what the replication crisis is. Second, they were asked which scientific disciplines were most affected by the replication crisis. 

# Results


In line with our preregistered analysis plan, we calculated the difference in means
between responses to each norm item and its accompanying counternorm, along with 
95% confidence intervals. For this analysis, we removed three cases with missing values. 
Table \@ref(tab:differencebetweennormandcounternorm) displays the means and standard deviations for each norm, counternorm, and the difference values. 

In general, the students seemed to endorse open science practices: For seven of the ten open science practices, agreement with the norm item was stronger than for the corresponding counternorm. However, the extent to which participants endorsed open science norms varied substantially across different practices. Participants most strongly endorsed norms that researchers should provide sufficient information about studies to permit replicability, be critical of published findings, and avoid p-hacking (means > 4; 4 representing 'agree'). They also tended to agree that researchers should avoid HARKing, share papers as open access, share open materials, and preregister their studies; the means for these items varied between 3.5 and 4. On the other hand, they tended to disagree with the practice of sharing preprints prior to peer review and, to a lesser extent, with sharing open data.

<!--MW: The paragraphs above contain some numeric info that's written in manually rather than being R outputs (e.g., "three", "seven") 
Not sure if that's a worry. JB: It sort of has to be because a) it's a holistic summary
and b) APA requires numbers below 10 to written out.
-->

<!-- JB: More of a concern for me is that we don't say anything about the 
correlations. If we aren't going to say anything about them at all, we can 
include them as a supplemental material? We preregistered them, so we have to 
include them, but we don't really have to say anything about them.-->


```{r differencebetweennormandcounternorm}

# select columns that relate to norms & pivot it long
norm_long <- df %>% 
  dplyr::select(c(id, ends_with("_norm"))) %>% 
    pivot_longer (cols = ends_with("norm"),
                names_to = "practice",
                values_to = "norm rating") 

# remove "_norm" from the practice variables
norm_long$practice = gsub(pattern = "_norm", replacement = "", x = norm_long$practice)

# select columns that relate to counternorms
cnorm_long <- df %>% 
  dplyr::select(c(id, ends_with("_cnorm"))) %>% 
  pivot_longer (cols = ends_with("cnorm"),
                names_to = "practice",
                values_to = "cnorm rating") 

# remove "_cnorm" from the practice variables
cnorm_long$practice = gsub(pattern = "_cnorm", replacement = "", x = cnorm_long$practice)

# join the norm and counternorm tibbles
norm_cnorm <- norm_long %>% inner_join(cnorm_long)

# create a `diff` variable that is the counternorm subtracted from the norm
norm_cnorm_long <- norm_cnorm %>% 
  mutate (diff = (`norm rating` - `cnorm rating`)) %>% 
    drop_na() %>% # drop the three NA values
      dplyr::select(-(`id`)) # remove this now that I've confirmed it works

# get the n values for each of the practices
prac_counts <- norm_cnorm_long %>% 
    dplyr::group_by(practice) %>%  
    count() %>% 
  ungroup()

# calculate means
prac_ms_sds <- norm_cnorm_long %>% 
    dplyr::group_by(practice) %>%  
    summarise(norm_mean = mean(`norm rating`), 
              norm_sd = sd(`norm rating`),
              cnorm_mean = mean(`cnorm rating`), 
              cnorm_sd = sd(`cnorm rating`),
              diff_mean = mean(`diff`), 
              diff_sd = sd(`diff`)) %>% 
  ungroup()

# join the n values with the means
prac_means <- prac_counts %>% inner_join(prac_ms_sds)
  
# calculate 95% CIs
error <- qnorm(0.975) * prac_means$diff_sd/sqrt(prac_means$n)
prac_means$lower <- prac_means$diff_mean - error
prac_means$upper <- prac_means$diff_mean + error


# combine means & SDs
prac_means$Norm <- paste0(round_tidy(prac_means$norm_mean,2),
                          " (", 
                          round_tidy(prac_means$norm_sd,2), 
                          ")")
prac_means$Counternorm <- paste0(round_tidy(prac_means$cnorm_mean,2), 
                            " (", 
                            round_tidy(prac_means$cnorm_sd,2), 
                            ")")
prac_means$Difference <- paste0(round_tidy(prac_means$diff_mean,2), 
                            " (", 
                            round_tidy(prac_means$diff_sd,2),
                            ")", 
                            " [",
                            round_tidy(prac_means$lower,2),
                            ", ", 
                            round_tidy(prac_means$upper,2),
                            "]")

# clean it up into a new dataframe with only the columns we need
practice_means <- prac_means %>% 
  dplyr::select(practice, Norm, Counternorm, Difference)

# ADD CORRELATIONS (per participant, use df)

# notes for the correlations....
# just the correlation with CI
# critical_corr <- round(cor(df$critical_norm, df$critical_cnorm),2)
# critical_CI <- round(CIr(r = critical_corr, n = nrow(df), level = 0.95),2)

#correlation test with CI and p-values in a clean tibble
# critical_corr_test <- cor.test(df$critical_norm, df$critical_cnorm)
# tidy(critical_corr_test)

# use this tutorial to try to get the correlations for each practice (using the long data)
# https://cran.r-project.org/web/packages/broom/vignettes/broom_and_dplyr.html

# this will create a tibble for each practice

# calculates the correlation test for each tibble & then tidies it up
corr <- norm_cnorm_long %>% 
  nest(data = -practice) %>% # create a tibble for each practice
  mutate (test = map(data, ~ cor.test(.x$`norm rating`, .x$`cnorm rating`)), 
          tidied = map(test,tidy)) %>% # run the corr & tidy it
  unnest(tidied) # unnest it to look at the results

# remove unnecessary columns, round the values, and move columns
corr <- corr %>% 
  dplyr::select(-c(data, test, statistic, parameter, method, alternative))  %>%
  mutate(p.value = p_tidy(p.value, 3, prefix = "")) %>% 
  mutate(estimate = round_tidy(estimate,2), 
         conf.low = round_tidy(conf.low, 2), 
         conf.high = round_tidy(conf.high, 2)) %>% 
  dplyr::relocate(estimate, .before = conf.low) %>%  
    dplyr::relocate(p.value, .after = last_col())

# join the corr tibble with the means tibble 
practice_means_corr <- practice_means %>% inner_join(corr, by = "practice")

# combine the confidence intervals into one column
practice_means_corr$Correlation <- paste0(practice_means_corr$estimate, 
                            " [",
                            practice_means_corr$conf.low,
                            ", ", 
                            practice_means_corr$conf.high,
                            "]")

# clean up the tibble                          
practice_means_corr <- practice_means_corr %>% 
  relocate (practice, .before = Norm) %>% 
  relocate (p.value, .after = last_col())

practice_means_corr <- practice_means_corr %>% 
  rename(Practice = practice) %>% 
  rename("Correlation p-value" = p.value) %>% 
  dplyr::select(-c(estimate, conf.low, conf.high))


practice_means_corr$Practice <- c("Critical thinking", "HARKing", "Info for replicability", "Open access", "Open data", "Open materials", "p-hacking", "Preprint", "Preregistration", "Registered reports")


# need to figure out whether I can rotate a page in papaja
# EK - You can rotate the table, but I don't think it will rotate the page and it won't
# carry over into word. 
# JB: Moving correlations to a supplement will solve the problem I'm trying to resolve anyway...
  
knitr::kable(practice_means_corr,
             caption = "Means (sd) for norms, counternorms, the difference between norms and counternorms, and correlations between norms and counternorms",
             digits = 2,
             align = c('lccc'))
  

```

<!-- I imagine we will remove the p.value column, but we might want this for now.  -->
<!-- It's not clear which stat the pvalue relates to, I would support removing it  -->
<!-- JB: I've changed the heading of the p.value column for clarity. If we pull the corrs out
into a new table in the supplemental material, then we will want the p-value with it. -->

## Norms in practice
Participants were also asked to estimate the proportion of psychology studies globally that apply each of ten practices (see Table \@ref(tab:normsinpractice)). For many of these practices, participants’ estimates were near the midpoint of 50%. This said, participants perceived some practices as being applied relatively infrequently: For example, the mean estimate of the prevalence of HARKing, preprint sharing before publication, and open data sharing were all near 30%. See Figure A in Supplemental Materials to see the distribution for each item. Participants perceived the most common practice was providing enough information in written reports to permit replication without asking any questions of the original researchers.


```{r normsinpractice}

# select columns that relate to norms in practice & pivot it long & drop the na values
in_prac_long <- df %>% 
  dplyr::select(c(id,preregistration:open_access)) %>% 
      pivot_longer (cols = -id,
                names_to = "practice",
                values_to = "prevalence rating") %>% 
  drop_na()

# get the n values for each of the practices
in_prac_counts <- in_prac_long %>% 
  dplyr::group_by(practice) %>% 
  count() %>% 
  ungroup()

# calculate means
in_prac_ms_sds <- in_prac_long %>%
  dplyr::group_by(practice) %>%
  summarise(mean = mean(`prevalence rating`),
            sd = sd(`prevalence rating`)) %>%
  ungroup()

# join the n values with the means
in_prac_means <- in_prac_counts %>% inner_join(in_prac_ms_sds)
  
# calculate 95% CIs
error <- qnorm(0.975) * in_prac_means$sd/sqrt(in_prac_means$n)
in_prac_means$lower <- in_prac_means$mean - error
in_prac_means$upper <- in_prac_means$mean + error


# combine means & SDs
in_prac_means$`Prevalence Ratings` <- paste0(round_tidy(in_prac_means$mean,2),
                          " (", 
                          round_tidy(in_prac_means$sd,2),
                            ")", 
                            " [",
                            round_tidy(in_prac_means$lower,2),
                            ", ", 
                            round_tidy(in_prac_means$upper,2),
                            "]")

# clean it up into a new dataframe with only the columns we need
in_practice_means <- in_prac_means %>% 
  dplyr::select(practice, `Prevalence Ratings`) %>% 
  rename(Practice = practice)

# breadcrumbs:
# I need to use apa_tables.... (in line with papaja)
# update practice descriptions so they make sense to readers

in_practice_means$Practice <- c("Data on request", "Info for replicability", "HARKing", "p-hacking", "Open access", "Open data", "Open materials", "Preprint", "Pre-registration", "Registered report")

  
knitr::kable(in_practice_means,
             caption = "Means, standard deviations, and 95% confidence intervals for the prevalence ratings for each research practice",
             digits = 2,
             align = c('lc'))
  


```


## The replication crisis
Interestingly, only `r round(sum(df$crisis==1, na.rm = TRUE)/length(df$crisis)*100, 0)`% of participants indicated they had heard of "the replication crisis", with `r round(sum(df$crisis==2, na.rm = TRUE)/length(df$crisis)*100, 0)`% indicating that they had not heard of the crisis and `r round(sum(is.na(df$crisis))/length(df$crisis)*100, 0)`% missing.

Participants who indicated they had heard of the replication crisis were asked what "What do you think 'the replication crisis' is"? Of the `r length(df$crisis_text[is.na(df$crisis_text)==FALSE])` participants who answered this question, most provided responses roughly consistent with the conventional interpretation of the term. Example responses included "The crisis where most studies were unable to be replicated when researchers decided to replicate previously done studies" and "Where previously trusted studies have been replicated the results were not able to be replicated". Only `r sum(crisis_descriptions$Accurate == "n")` responses were suggestive of a misunderstanding of the term (e.g., "Plagiaris[m]", "only same results being published").

Participants' estimates of the replicability of psychological studies were variable. The mean, _M_ =`r mean(df$rep_perc, na.rm = TRUE)` was near the midpoint of 50%, but with substantial variability around this estimate, _SD_ = `r sd(df$rep_perc, na.rm = TRUE)`, minimum = `r min(df$rep_perc, na.rm = TRUE)`,  maximum = `r max(df$rep_perc, na.rm = TRUE)`. A histogram of participants' responses is provided in Figure \@ref(fig:repperchistogram).

```{r repperchistogram, fig.cap="Histogram of participants' responses to the question: \"Imagine that a set of researchers select 100 published studies about psychology, and repeat each study again, exactly how it was described in the original report but with a new set of participants. We call repeating a study like this a \"replication study\". At a guess, how many of these replication studies do you think would produce the same conclusions as the original studies?\" Responses provided on a visual slider."}
hist(df$rep_perc, main = NULL, xlab = "Percentage")
```

<!-- Participants were also asked "Which scientific disciplines do you think are most affected by the replication crisis?" As we discussed, I'm not sure this response will really be useful to look at given that we have "primed" them to think of psychology as being most affected (and indeed that's what the responses largely indicate).-->


# Discussion

Despite efforts to improve the replicability of research the past decade, and corresponding media attention, most incoming psychology students in our sample were unaware of the replication crisis. Encouragingly, on the other hand, most tended to endorse open science norms more-so than counternorms. Notably this wasn't the case for all open science practices (namely sharing of pre-prints and open data). It may well be that scientific training has enshrined peer review as an essential practice that is robust to change.

<!--Bem begins his guide for writing articles as, “There are two possible articles you can write: (a) the article you planned to write when you designed your study or (b) the article that makes the most sense now that you have seen the results. They are rarely the same, and the correct answer is (b).”
MW: I think the Bem paper would work here if students tended to be pro-HARKing but they tended to be against it.
-->

Participants' estimates of the proportion of psychological studies published which apply each of a set of ten open science practices tended to hover around a mean of 50%. Although this could be read as implying literally that participants believe these practices are roughly as commonly applied as not, it could imply that some participants provided responses near 50% as an indication of uncertainty or indifference. This said, 50% was the modal response for only a minority of practices (open access, data availability on request).

The fact that the mean estimated prevalence was no higher than 64% for any practice is inconsistent with the sometime-reported claim that incoming students tend to naively assume that science is *usually* conducted openly. Nevertheless, the students appear to have at least somewhat over-estimated the prevalence of some open science practices. For example, on average students estimated that nearly a third of published psychological studies share data openly and that nearly half of studies are preregistered. Although we have not conducted a systematic review it is safe to say the true figures are rather lower [see @wichertsPoorAvailabilityPsychological2006].


## Strengths and limitations
Despite a relatively large sample size, our results provide an imperfect picture of new psychology students’ methodological understandings. Our use of convenience samples coupled with the imprecision of polling students’ impressions at one point in time suggest that we can make only very tentative generalisations to a wider population of incoming psychology students. However, collectively these results demonstrate some clear topics that might be targeted in our teaching: for example, emphasising the value of pre-prints and open-data to research practice.  

Our questions focused primarily on attitudes and beliefs so that we could better document the implicit understanding that guided students’ thoughts about the research process. However, we did not probe students' factual knowledge of methodological findings or concepts. It is quite possible that reform in secondary school instruction has improved students’ understandings of more fundamental statistical (e.g., confidence interval estimation) and design (e.g., direct replications) open science considerations without necessarily impacting their attitudes and beliefs. Yet, research at the tertiary level casts doubt that the disciplinary reform from the past two decades has had much impact on the content of our teaching [@friedrichReplicatingNationalSurvey2018].

<!--
MW: The bit from "It's quite possible..." is sensible, but if needed could possibly shave some words off by excluding? 
See how we go when we check full word count
-->


<!-- ## Directions for future research

MW: Do we need directions.. or just leave out to conserve word count?-->


## Conclusion and teaching implications
Overall these findings suggest that incoming psychology students have a degree of sympathy for open science norms. Nevertheless, despite significant media attention on the replication crisis, few have heard of this crisis. Similarly, students seemed to be relatively unfamiliar with responses to the replication crisis (e.g., open science practices). It is therefore important that teachers of psychology not assume pre-existing knowledge among incoming students.

<!-- MW: I've added this subsection because it's signalled as a required heading in the abstract. Is there more to say here?-->


# Supplemental Materials

Figure A displays the histograms for each of the norms in practice items. 

```{r normsinpracticehistogram}

practice_labels <- c(
  available_data = "Data on\nrequest",
  detailed_methodology = "Info\nrequired\nfor replication",
  harking = "HARKing",
  incomplete_results = "p-hacking",
  open_access = "Open access",
  registered_reports = "Registered reports",
  open_data = "Open data",
  open_materials = "Open\nmaterials",
  preprint_pre = "Preprints",
  preregistration = "Preregistration",
  registered_report = "Registered\nreport"
)

in_prac_long %>%
  ggplot(aes(x = `prevalence rating`)) +
  geom_histogram(aes(y=stat(count)), binwidth = 10) +
    xlab("Prevalence Ratings (%)") +
    ylab("Frequency") +
    theme_classic () +
    theme(axis.text = element_text(size = 9), axis.title = element_text(size = 9)) +
    facet_wrap(~practice, nrow = 2, labeller = labeller(practice = practice_labels))



#[breadcrumb: need to update the theme]

```
*Figure A*

\newpage
# Reproducible Code Statement

We used `r cite_r("r-references.bib")` for all our analyses.

<!-- This is nice to do, but it generate a 150+ word paragraph of citations to packages, including citations to packages our code doesn't directly use (e.g. nlme, multilevel, etc.). We might need to cut it down manually to a more concise list?

EK: You can edit the r-refs bib directly to remove the packages that are just dependencies
-->

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>

\endgroup
