---
title: What do incoming university students believe about open science practices in
  psychology?
author:
- name: Jennifer L. Beaudry
  affiliation: '1'
  corresponding: yes
  address: Postal address
  email: jbeaudry@swin.edu.au
  role: null
- name: Michael C. Philipp
  affiliation: '2'
  role: null
- name: Matt N. Williams
  affiliation: '2'
  role: null
shorttitle: Psychology students' beliefs about open science
output:
  papaja::apa6_word: default
  papaja::apa6_pdf: default
authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.
  Enter author note here.
abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  One or two sentences to put the results into a more **general context**.
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  <!-- https://tinyurl.com/ybremelq -->
keywords: open science, psychology, teaching, reproducibility, replication
wordcount: X
bibliography: ['r-references.bib','Undergrads & Open Science (public).bib']
floatsintext: yes
figurelist: no
tablelist: no
footnotelist: no
linenumbers: yes
mask: yes
draft: no
documentclass: apa6
classoption: man
affiliation:
- id: '1'
  institution: Swinburne University of Technology
- id: '2'
  institution: Massey University
---
<!-- I've only partially entered authorship deets for the moment and set it to mask authorship info when knitting
It's set to default to knit to .docx, but can also knit to pdf (if you have a tex distro installed)
The pdf looks nicer and might be good for a preprint but .docx probably easiest to use for journal submissions.
To knit to RMarkdown you will probably need the papaja package installed, and to knit to .pdf you will need a tex distro (e.g., TinyTex).
However, it is possible to run all the R code (without knitting to .docx or .pdf) by clicking Run > Run All Chunks Below.
This could be useful in future for readers interested in reproducing results but who don't have or wish to install 
papaja and its many dependencies.
-->

<!-- The .bib file is now just from our shared library -->

```{r setup, include = FALSE}
library(papaja) #install with devtools1
library(here)
library(tidyverse)
library(finalfit) # to keep trailing zeros & round_tidy
library(psych) #for correlation matrices
library(tidyselect) 
library(psychometric) # for correlation confidence intervals - might not need...
library(broom)

r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed,
  results = 'as.is')
```

```{r functions}

p_tidy <- function (x, digits, prefix = "= ") {
    x.out = paste0(prefix, round_tidy(x, digits))
    all_zeros = paste0(prefix, round_tidy(0, digits))
    less_than = paste0("< ", format(10^-digits, scientific = FALSE))
    x.out[x.out == all_zeros] = less_than
    return(x.out)
}
```


```{r import data}

# import the main data 
df <- read.csv (here::here("data", "students_processed.csv"))

# upload the replication crisis responses, marked as accurate and inaccurate (outside of R)
crisis_descriptions = read.csv(here::here ("data", "crisis_descriptions_marked.csv"))

# convert variables to factors as needed
df$age <- as_factor(df$age)
df$eligibility <- as_factor(df$eligibility)

# convert factors to characters as needed
df$university <- as.character(df$university)
df$degree <- as.character(df$degree)
df$major <- as.character(df$major)
df$major <- as.character(df$major)
# df$uni_country <- as.character(df$uni_country)
# df$nationality <- as.character(df$nationality) [i think these should be factors]

# # import the metadata [check if I need it for these analyses?]
# metadata <- here::here("survey", "data", "os_metadata_raw_data.csv") %>% 
#   read_csv(col_names = TRUE, skip_empty_rows = TRUE) %>% 
#   filter(!is.na(OldVariable))
```

The last decade has seen unprecedented change in methodological and reporting practices in psychology. These changes were partly precipitated by what is popularly known as the "replication crisis": The discovery that close replications of published psychological studies are often unable to replicate the original findings [@kleinInvestigatingVariationReplicability2014; @kleinManyLabsInvestigating2018]. These apparent problems with replication have lead to a variety of potential solutions to make research practices more transparent, including more frequent publication of replication studies [see @brandtReplicationRecipeWhat2014], more thorough reporting of methods and results [@simmons21WordSolution2012], open sharing of data [see @meyerPracticalTipsEthical2018], and preregistration of data collection and analysis plans [@nosekPreregistrationRevolution2018]. These calls for more transparent research practices prompted the discipline to reflect on the norms and beliefs that underpin its practice of research. Many studies have polled researchers' understandings of and adherence to open science norms, beliefs, and practices [e.g., @baker500ScientistsLift2016; @openscienceworkinggroup2017SurveyOpen2018; @harrisUseReproducibleResearch2018]. In turn, initiatives have been directed toward training undergraduate students in the adoption of an open science ethos [e.g., @chopikHowWhetherTeach2018; @graheHarnessingUndiscoveredResource2012; @jekelHowTeachOpen2020; @schonbrodtTrainingStudentsOpen2019].  These initiatives help ingrain open science norms and change attitudes about research practices, but we know little about what these students know or believe about open science research practices prior to entering the university classroom.

The most comprehensive set of principles for how science *ought* to be practiced are Merton's norms of science [@mertonNoteScienceDemocracy1942]. Scientists [@andersonExtendingMertonianNorms2010] and graduate students [@andersonGraduateStudentExperience1994] alike have historically endorsed the normative value of Merton's principles. Furthermore, many of the recent practices designed to make science more transparent and open reflect Mertonian norms. For example, the practice of sharing open source software directly corresponds to the Mertonian norm of *communism*: Scientists should have common ownership of scientific goods. Similarly, the practices of sharing of preprints for open peer review and open data for checking of reproducibility corresponds to the norm of communality, but also to that of *organised skepticism*: Scientific claims should be subjected to critical scrutiny. Preregistration can be connected to the norm of *disinterestedness*: By making (and preregistering) decisions about how to analyse data before results are produced, a researcher can limit the degree to which the substantive results produced by different analytic strategies affect their decisions regarding which analyses to report. In this sense, the ongoing reform in psychological research can partly be understood as simultaneously a set of new practices and a re-affirmation of established norms.<!--Should we consider citing someone for these links? Personally I have seen Simine draw these parallels between norms and open science practices and would favour crediting her here, however I'm sure that others have made similar analyses and so would be happy to cite others or to see this as such a commonly held set of beliefs that no citations are required--->

Nevertheless, rapid changes in methodological practice and empirical findings present significant pedagogical challenges for the teacher of psychology. Keeping textbooks and other instructional materials up to date is difficult when supposedly well-established findings are contradicted by new replications emerging at a rapid pace. Furthermore, training in emerging methodological practices is crucial for graduate students who may go on to apply psychological research methods themselves, so that they may produce research that is even more replicable than that of previous generations. Even for students who do not go on to conduct research themselves, an understanding of contemporary methodological practices - and problems with methodological practices in psychology - is essential for becoming informed and critical consumers of psychological knowledge. However, how we best provide this understanding is no trivial task. Indeed, there are several reasons why it might be unwise to assume a simple knowledge deficit, where the teacher's only role is to dispense an understanding of open science and transparent research practices. 

First, unlike some areas of psychology covered in undergraduate courses (e.g., models of working memory, or the internal workings of human senses), the replication crisis is frequently discussed in mainstream and social media [e.g., @ogradyPsychologyReplicationCrisis2020; @yongPsychologyReplicationCrisis2016; @yongPsychologyReplicationCrisis2018]. Many students may plausibly have some knowledge about these issues obtained prior to (or independently of) their formal studies. Teaching methods should thus be informed by some understanding of what students' pre-existing levels of knowledge are. 

Second, when psychology students are beginning their university studies, they are often learning for the first time about how and why it is useful to apply scientific methods to studying human behaviour (rather than only relying on alternative sources of knowledge such as intuition or anecdote or authority). An over-emphasis on problems with replicability could leave these students unconvinced that scientific methods for studying human behaviour are valuable *at all*, leaving them to favour even less credible alternative sources of knowledge. For instance, @chopikHowWhetherTeach2018 found that undergraduate students tended to trust the results of psychological studies *less* after a one-hour educational lecture about the replication crisis 
^[1 Notably, the effect of the lecture on trust of psychological studies was relatively small (d = -.36), and mean trust levels remained fairly high after the lecture (M = 4.94 on a scale of 1 to 7).].

Third, anecdotal evidence suggests that students being taught about open science practices and reforms to improve reproducibility (such as open sharing of data and analysis code) are often surprised that this is not *already* standard practice [e.g. see tweets on this topic: @burak_tunca_uygun_tunc_2021; @duygu_uygun-tunc_uygun_tunc_last_2021; @lorne_campbell_distributary_academic_minzlicht_2018].<!--I'm not sure what happened to these citations but I can't find them in our library anymore - need to fix --> In other words, students' naive conception of how science works may, in some ways, be closer to that embodied in recent reforms rather than "business as usual" practices. Indeed, surveys of scientists' subscriptions to Mertonian norms have often found that while most scientists endorse Mertonian norms such as communality, universalism and disinterestedness, this is by no means the case for all scientists. Furthermore, they typically perceive the behaviour of other scientists as being less consistent with these norms than their own [@andersonNormativeDissonanceScience2007]. Therefore - for some students - teaching about reform may be less a matter of conveying new information and more one of reinforcing "naive" assumptions. In fact, although beyond the scope of this article, it is *possible* that incoming undergraduate students more strongly endorse open science norms than do academics.

<!-- I think it'd be useful here if we were able to point to examples of the anecdotal evidence mentioned above - anyone come across anything (even just on social media?) 
Also maybe add more about what we know about academics' endorsement of open science norms.
-->

These considerations suggest that it is important that teachers wishing to inform undergraduate psychology students about the replication crisis and open science practices have some understanding of what such students actually know and believe about these topics already. In addition, undergraduate psychology students themselves represent one of the most important audiences for psychological research: The number of undergraduate students enrolled in psychology courses in any given year dwarfs the number of academics working in psychology. As such, the *preferences* of these students with respect to reproducibility and open science practices are themselves important as a phenomenon of interest. If, for example, undergraduate psychology students have a strong preference that journal articles are freely available to members of the public, this preference on the part of some of the consumers of the knowledge we produce should have some bearing on our choices in relation to sharing of manuscripts. This is especially the case given that undergraduate students are directly or indirectly responsible for much of the funding which allows universities to operate and research to be conducted. In fact, in serving as participants for course credit in many universities, undergraduate psychology students also provide the data underlying much psychological research.

<!-- I'm not sure about the argument in the above paragraph - might be somewhat of a stretch? 
MP: Maybe a stretch, but given we have a good rationale already, it might be worth trying to keep it. We may need a reference to two to help make some of the claims more concrete though.-->

For these reasons, we aimed to conduct a study describing what incoming undergraduate students of psychology believe about reproducibility and open science practices in psychology. Our survey encompassed norms (how students felt research *should* be conducted), norms in practice (how students believe psychological research *is* conducted) and replicability (how replicable students believe psychological research is). In doing so we hope to provide knowledge which can inform the pedagogy of teaching about replication and open science practices. This study is exploratory [see @wagenmakersAgendaPurelyConfirmatory2012] and descriptive, and does not involve the specification or testing of hypotheses.

# Methods
We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

```{r inclusion}

# participant numbers (all & included cases)
df_all <- df
n_all <- nrow(df_all)

# only participants who met the inclusion criterion
df <- df_all %>% 
  filter(exclude %in% "include") 
n <- nrow(df)

ex <- df_all %>% 
  filter(exclude %in% "exclude") %>% # n for those excluded
  nrow()

# note re: exclusion [all coded in `exclude` in preprocessing, 
# but need to summarise exclusion criteria here]

age_ex <- df_all %>%
  filter(age_criteria %in% "exclude") %>% 
  nrow()

el <- df_all %>%
  filter(eligibility_criteria %in% "exclude") %>% 
  dplyr::count(eligibility_lab) # need the breakdown by condition

status <- df_all %>% 
  filter(status_criteria %in% "exclude") %>% 
  dplyr::count(status_lab) # need the breakdown by condition

nmiss <- df_all %>% 
  filter(nmiss_criteria %in% "exclude") %>% 
  nrow()

```


```{r demographics for those included in analyses}

# age
age <- df %>% dplyr::count(age_lab)

# gender --> need to summarise once we've recoded it in preprocessing script
  # [breadcrumb]
gender <- df %>% dplyr::count(gender)

# high school status
hs_status <- df %>% dplyr::count(high_school_lab)

# high school psyc 
hs_psyc <- df %>% dplyr::count(psych_hs_lab)

# country of university 
uni_country <- df %>% dplyr::count(uni_country)

# country of nationality
nationality <- df %>% dplyr::count(nationality)

# whether parents' attended uni
first_gen <- df %>% dplyr::count(first_gen_lab)
```


## Participants

Of those who started the survey (*n* = `r n_all`), we screened out `r ex` 
participants who were not eligible to participate based on the combination of our
preregistered exclusion criteria. Specifically, we screened out people who were younger than 
18 years (*n* = `r age_ex`). We also screened out individuals who had already 
started a psychology unit at a university, regardless of whether they had 
completed it (*n* = `r el[1,2] + el[3,2] + el[4,2]`); were not enrolled in a 
psychology unit at a university (*n* = `r el[2,2]`); or did not answer this 
question (*n* = `r el[5,2]`). We then excluded additional responses that 
Qualtrics flagged as spam (*n* = `r status [1,2]`) or as a survey preview 
(*n* = `r status [2,2]`). Finally, we excluded participants who met all other 
eligibility criteria, but did not respond to any of the main items
in the study (*n* = `r nmiss`). 

The remaining `r n` participants were eligible based on starting their first unit 
of study in psychology at a university within the next month. For those individuals 
who were screened out, our Qualtrics program automatically directed them to the 
debriefing form.  

Of the remaining `r n` participants, most reported that they were
18-24 years old (*n* = `r age[1,2]`), `r age [2,2]` 
reported that they were 25-34 years old, `r age [3,2]` were 35-44 years old,
`r age [4,2]` were 45-54 years old, and `r age [5,2]` was 55-64 years old. 

The majority of participants (*n* = `r gender[1,2]`) reported that they were female, 
`r gender[2,2]` reported that they were male, `r gender[3,2]` participant reported 
their gender identity as "queer man", and `r gender[3,2]` participant reported 
non-binary gender.^[While the response "queer man" could potentially be coded as
"male", we are aware that some individuals who report gender in this way would
not feel that such reclassification was appropriate. We have retained the original
response in order to provide a faithful representation of the participant's 
self-reported gender.]

Nearly all reported that they graduated from high
school or secondary school (*n* = `r hs_status[3,2]`). Of those participants who 
attended high school, a little less than half completed psychology courses in 
high school (*n* = `r hs_psyc[2,2]`). In terms of their nationality, most 
participants reported that they were Australian (*n* = `r nationality[3,2]`), 
from the United Kingdom (*n* = `r nationality[24,2]`), or Chinese 
(*n* = `r nationality[6,2]`). In terms of where the participants were attending 
university, most reported that they were in Australia (*n* = `r uni_country[1,2]`), 
the United Kingdom (*n* = `r uni_country[5,2]`), or New Zealand (*n* = `r uni_country[4,2]`). About 2/3rds of participants reported that at least one of their parents attended university (*n* = `r first_gen[2,2]`). Students were recruited from nine universities, with the largest number of responses recorded from participants enrolled at the University of Queensland (*n* = `r nrow(filter(df, university_recoded == "University of Queensland"))`), University of Adelaide (*n* = `r nrow(filter(df, university_recoded == "University of Adelaide"))`), and the University of Bath (*n* = `r nrow(filter(df, university_recoded == "University of Bath"))`). While, the majority of participants (*n* = `r nrow(filter(df, degree_type == "Non-psychology"))`) indicated that they were enrolled in a non-psychology degree such as a Bachelor of Science or Bachelor of Arts (*n* = `r nrow(filter(df, degree_recoded == "Bachelor of Science"))`, *n* = `r nrow(filter(df, degree_recoded == "Bachelor of Arts"))` respectively), most were completing a psychology major (*n* = `r nrow(filter(df, major_recoded == "Psychology"))`).

## Material

### Open Science Norms and Counternorms

Our approach to measuring norms and counternorms focussed on participants' evaluation of ten specific open science practices that reflected diverse open science norms rather than trying to measure endorsement of the more abstract Mertonian norms (cf. Anderson et al., 2007). <!-- Mention here our discussions workshopping of common open science norms at SIPs? ! -->
The open science practices assessed in our survey included: critical thinking, preregistration, registered reports, p-hacking, hypothesising after the results are known (HARKing), providing sufficent information of replicability, preprints, open materials, open data, and open access publishing. Each practice was elucidated by a pair of items (pro-open science; counter-open science) in the context of a common scenario, "Imagine that Deborah is a psychology researcher who has designed a study to test a specific hypothesis." There were 20 scenarios in total (see Table \@ref(tab:norm_counternorm_items)). Participants rated their agreement with each prescriptive statement on a 1 (strongly disagree) to 5 (strongly agree) scale.

<!-- Add Table X of norms and counternorms statements that map to each practice -->

```{r norm_counternorm_items}
norm_items = data.frame(
  "Practice" = c("critical", "critical", "HARKing", "HARKing", "Info for replicability", "Info for replicability",
                 "Open access", "Open access", "Open data", "Open data", "Open materials", "Open materials",
                 "p-hacking", "p-hacking", "Preprints", "Preprints", "Preregistration", "Preregistration",
                 "Registered reports", "Registered reports"),
  "Category" = rep(c("Norm", "Counternorm"), times = 10),
  "Item" = c("Deborah used previously published research to inform her research study. Deborah should be critical of the findings published in journals because published research can be wrong.",
             "Deborah used previously published research to inform her research study. Deborah should accept the findings published in journals because journals would not publish research with errors.",
             "Deborah should only describe her study as a test of a hypothesis if she decided on her hypothesis and how she would test it before she started collecting data.",
             "Once Deborah has analysed her results, it would be good scientific practice for her to write her manuscript as if she predicted those results from the beginning.",
             "When reporting the findings of her study, Deborah should describe how she completed the study in enough detail that another researcher could repeat her entire study without having to check any details with her - even if this means including lots of “boring” practical details in her report, or in an appendix.",
             "When reporting the findings of her study, it would be good scientific practice for Deborah to gloss over some of the practical details so she can tell a good story.",
             "Once her study is complete, Deborah should publish her findings in a journal that is free for others to access.",
             "Once her study is complete, Deborah should publish the findings in the most prestigious journal she can, even if that journal charges others a fee to access the report.",
             "When Deborah publishes her study, she should post the anonymous responses from participants online so that anyone can access and use the responses in their own research.",
             "Deborah should keep the participants’ responses from her study protected, so that only she and her research team can access them.",
             "Deborah should share the written materials and measures for her study openly online so that other researchers and members of the public can access and use them.",
             "Deborah should keep the written materials and measures for her study protected, so that only she and her research team can access them.",
             "Deborah should report the findings of all analyses of her data that she conducts.",
             "It would be good scientific practice for Deborah to run many different analyses of her data, and report only those that produce interesting findings.",
             "Deborah should post a manuscript describing the findings of her study openly online as soon as it is complete, even if the manuscript has not yet been checked by experts (peer reviewed) and accepted for publication in a journal.",
             "Deborah should not post a manuscript describing the findings of her study online until after it has been checked by experts (peer reviewed) and accepted for publication in a journal.",
             "Before collecting her data, Deborah should write down what her hypotheses are and how she plans to collect and analyse data. She should then save her plan in an online registry so others can tell what methods and analyses she will use to test her hypotheses after collecting data.",
             "Deborah should decide which data analyses are suitable to test her hypotheses only after looking at her data.",
             "Deborah should submit a plan for her study to a journal to be checked by experts (peer reviewed) before she collects and analyses data.",
             "Deborah should submit her study to a journal to be checked by experts (peer reviewed) only after she has finished collecting and analysing her data."))

norm_items_wide <- pivot_wider(
  norm_items,
  names_from = Category,
  values_from = Item)

knitr::kable(norm_items_wide,
             caption = "Norm and counternorm items",
             align = c('lll'))
```


### Beliefs About the Practice of Psychological Science

We also sought to measure participants' normative beliefs about how psychological science was actually conducted by researchers around the world. To that end we asked participants, "When you consider all of the psychology studies conducted globally each year, what percentage of them would have the following characteristics?" The ten characteristics explicitly described each of the ten open science practices (see Table XX). Participants were given a slider bar ranging from 0% to 100% to record their estimate. 

<!-- Add Table XX of 10 belief statements -->

```{r norms_in_practice_items}
practice_items = data.frame(
  "Practice" = c("Preregistration", "Registered reports", "p-hacking", "HARKing", "Info for replicability",
                 "Preprint", "open materials", "Open data", "Data on request", "Open access"),
  "Item" = c("The plan for collecting and analysing data for the study is posted in an online registry before data collection starts",
             "A plan for the study is submitted to a journal to be checked by experts (peer reviewed) before the data is collected and analysed",
             "The researchers run many different analyses of the data, and report only those that produce interesting findings",
             "The researchers describe their results as if they predicted those results from the beginning, even if that isn’t actually true",
             "The researchers provide enough practical detail in their written report that another researcher could fully repeat (replicate) their study without needing to ask any questions of the original researchers",
             "A manuscript describing the findings of the study is posted openly online as soon as it is complete, even if the manuscript has not yet been checked by experts (peer reviewed) and accepted for publication in a journal",
             "The written materials and measures for the study are posted openly online so that other researchers and members of the public can access and use them",
             "Once the study is published, the anonymous responses from participants are posted openly online so that anyone can access and use the responses in their own research",
             "Once the study is published, the anonymous responses from participants are available to other researchers on request",
             "The research report describing the study and its findings is published in a journal that is free for anyone to access")
)

  
knitr::kable(practice_items,
             caption = "Norms in practice items",
             align = c('ll'))

#Note to selves: The norm in practice items map fairly closely to norms and counternorms, but not exactly
#I.e., there is no norm in practice item about critically evaluating published literature, but there is one about sharing
#data on request (which isn't in the norms/counternorms)
#I can't remember why we did this, but so it goes.
#The ordering of this table also doesn't match the one above, which I'm leaving for the moment since we probably want to set
#an order of items/norms and stick to it consistently throughout

```


### Beliefs About Replications

Participants' beliefs about the replicability of results in psychological science were indexed by two questions. The first asked participants to, 
> "Imagine that a set of researchers selected 100 published studies about psychology, and repeat each study again, exactly how it was described in the original report but with a new set of participants. We call repeating a study like this a 'replication study.' At a guess, how many of these replication studies do you think would produce the same conclusions as the original studies?”

Estimates were recorded using a slider bar ranging from 0% to 100%. 
A second question asked participants whether they had heard of the replication crisis. Those who had heard of the replication crisis were asked two additional open-ended questions. First, they were asked to describe what the replication crisis is. Second, they were asked which scientific disciplines were most affected by the replication crisis. 


## Procedure

Collaborators/Contributors/Colleagues from ## tertiary institutions in Australia (n=), New Zealand (n=), the UK (n=), and the USA (n=) invited students enrolled in their first-year psychology course to participate in the online survey hosted on Qualtrics (CITE?). An invitation was sent to participants via email or a course website 1-2 weeks before their course began. After completing demographic information, participants completed measures of Open Science Norms and Counternorms, Norms in Practice, and Beliefs about Replications in that order. At the conclusion of the study participants were invited to share any comments about the survey before they were thanked for their participation and provided an informative debriefing about the aims of the study and links to further reading about open science practices in psychology. The survey took most participants approximately #-# minutes to complete.

## Data analysis

# Results


In line with our preregisterd analysis plan, we calculated the difference in means
between responses to each norm item and its accompanying counternorm, along with 
95% confidence intervals. For this analysis, we removed three cases with missing values. 
Table XX displays the means and standard deviations for each norm, counternorm, 
and the difference values. 


\@ref(tab:chunk-name)

In general, the students seemed to endorse open science practices: For seven of the ten open science practices, agreement with the norm item was stronger than for the corresponding counternorm. However, the extent to which participants endorsed open science norms varied very substantially across different practices. In terms of open science norms, participants most strongly endorsed the idea that researchers should provide sufficient information about studies to permit replicability, be critical of published findings, and avoid p-hacking (means > 4). They also tended to agree that researchers should avoid HARKing, share papers as open access, share open materials, and preregister their studies; the means for these items were all between 3.5 and 4 (4 representing 'agree'). On the other hand, they tended to disagree with the practice of sharing preprints prior to peer review, and to a lesser extent with sharing open data. The practice of registered reports was the one about which participants were most equivocal, with very similar levels of agreement (near the midpoint of 3) with both norm and counternorm for this item. This may be relatively unsurprising, given that the registered reports are perhaps the most recent innovation in the list of practices.

Unsurprisingly, there were fairly substantial negative correlations between the norm and counternorm pairs, although this was not the case for all pairs: For the norm and counternorm items relating to preregistrations, the correlation was `r sprintf('%.2f', cor(df$prereg_norm, df$prereg_cnorm))`. Similarly for the norm and counternorm items relating to HARKing, the correlation was negative but very small.


```{r difference between norm and counternorm}

# select columns that relate to norms & pivot it long
norm_long <- df %>% 
  dplyr::select(c(id, ends_with("_norm"))) %>% 
    pivot_longer (cols = ends_with("norm"),
                names_to = "practice",
                values_to = "norm rating") 

# remove "_norm" from the practice variables
norm_long$practice = gsub(pattern = "_norm", replacement = "", x = norm_long$practice)

# select columns that relate to counternorms
cnorm_long <- df %>% 
  dplyr::select(c(id, ends_with("_cnorm"))) %>% 
  pivot_longer (cols = ends_with("cnorm"),
                names_to = "practice",
                values_to = "cnorm rating") 

# remove "_cnorm" from the practice variables
cnorm_long$practice = gsub(pattern = "_cnorm", replacement = "", x = cnorm_long$practice)

# join the norm and counternorm tibbles
norm_cnorm <- norm_long %>% inner_join(cnorm_long)

# create a `diff` variable that is the counternorm subtracted from the norm
norm_cnorm_long <- norm_cnorm %>% 
  mutate (diff = (`norm rating` - `cnorm rating`)) %>% 
    drop_na() %>% # drop the three NA values
      dplyr::select(-(`id`)) # remove this now that I've confirmed it works

# get the n values for each of the practices
prac_counts <- norm_cnorm_long %>% 
    dplyr::group_by(practice) %>%  
    count() %>% 
  ungroup()

# calculate means
prac_ms_sds <- norm_cnorm_long %>% 
    dplyr::group_by(practice) %>%  
    summarise(norm_mean = mean(`norm rating`), 
              norm_sd = sd(`norm rating`),
              cnorm_mean = mean(`cnorm rating`), 
              cnorm_sd = sd(`cnorm rating`),
              diff_mean = mean(`diff`), 
              diff_sd = sd(`diff`)) %>% 
  ungroup()

# join the n values with the means
prac_means <- prac_counts %>% inner_join(prac_ms_sds)
  
# calculate 95% CIs
error <- qnorm(0.975) * prac_means$diff_sd/sqrt(prac_means$n)
prac_means$lower <- prac_means$diff_mean - error
prac_means$upper <- prac_means$diff_mean + error


# combine means & SDs
prac_means$Norm <- paste0(round_tidy(prac_means$norm_mean,2),
                          " (", 
                          round_tidy(prac_means$norm_sd,2), 
                          ")")
prac_means$Counternorm <- paste0(round_tidy(prac_means$cnorm_mean,2), 
                            " (", 
                            round_tidy(prac_means$cnorm_sd,2), 
                            ")")
prac_means$Difference <- paste0(round_tidy(prac_means$diff_mean,2), 
                            " (", 
                            round_tidy(prac_means$diff_sd,2),
                            ")", 
                            " [",
                            round_tidy(prac_means$lower,2),
                            ", ", 
                            round_tidy(prac_means$upper,2),
                            "]")

# clean it up into a new dataframe with only the columns we need
practice_means <- prac_means %>% 
  dplyr::select(practice, Norm, Counternorm, Difference)

# ADD CORRELATIONS (per participant, use df)

# notes for the correlations....
# just the correlation with CI
# critical_corr <- round(cor(df$critical_norm, df$critical_cnorm),2)
# critical_CI <- round(CIr(r = critical_corr, n = nrow(df), level = 0.95),2)

#correlation test with CI and p-values in a clean tibble
# critical_corr_test <- cor.test(df$critical_norm, df$critical_cnorm)
# tidy(critical_corr_test)

# use this tutorial to try to get the correlations for each practice (using the long data)
# https://cran.r-project.org/web/packages/broom/vignettes/broom_and_dplyr.html

# this will create a tibble for each practice

# calculates the correlation test for each tibble & then tidies it up
corr <- norm_cnorm_long %>% 
  nest(data = -practice) %>% # create a tibble for each practice
  mutate (test = map(data, ~ cor.test(.x$`norm rating`, .x$`cnorm rating`)), 
          tidied = map(test,tidy)) %>% # run the corr & tidy it
  unnest(tidied) # unnest it to look at the results

# remove unnecessary columns, round the values, and move columns
corr <- corr %>% 
  dplyr::select(-c(data, test, statistic, parameter, method, alternative))  %>% 
  mutate(p.value = p_tidy(p.value, 3, prefix = "")) %>% 
  mutate(estimate = round_tidy(estimate,2), 
         conf.low = round_tidy(conf.low, 2), 
         conf.high = round_tidy(conf.high, 2)) %>% 
  dplyr::relocate(estimate, .before = conf.low) %>%  
    dplyr::relocate(p.value, .after = last_col())

# join the corr tibble with the means tibble 
practice_means_corr <- practice_means %>% inner_join(corr, by = "practice")

# combine the confidence intervals into one column
practice_means_corr$Correlation <- paste0(practice_means_corr$estimate, 
                            " [",
                            practice_means_corr$conf.low,
                            ", ", 
                            practice_means_corr$conf.high,
                            "]")

# clean up the tibble                          
practice_means_corr <- practice_means_corr %>% 
  rename(Practice = practice) %>% 
  dplyr::select(-c(estimate, conf.low, conf.high)) %>% 
  relocate (Practice, .before = Norm) %>% 
  relocate (p.value, .after = last_col())


# breadcrumbs:
# need to figure out whether I can rotate a page in papaja
# I need to use apa_tables.... (in line with papaja)
# update practice descriptions so they make sense to readers

  
knitr::kable(practice_means_corr,
             caption = "Means (sd) for norms, counternorms, the difference between norms and counternorms, and correlations between norms and counternorms",
             digits = 2,
             align = c('lccc'))
  

```

<!-- I imagine we will remove the p.value column, but we might want this for now.  -->

## Norms in practice
Participants were also asked to estimate the proportion of psychology studies estimated globally which apply ten practices. These practices corresponded approximately to the ten norm-counternorm pairs, albeit with one norm excluded on the basis of not referring to behaviour (critically evaluating published studies vs. accepting them), and one practice added out of interest (data sharing on request). For many of these practices, participants estimates were near the midpoint of 50%. This said, participants perceived some practices as being applied relatively infrequently: For example, the mean estimate of the prevalence of HARKing, preprint sharing before publication, and open data sharing were all near 30%. Participants seemed to perceive that the most common practice was providing enough information in written reports to permit replication without asking any questions of the original researchers. It is perhaps fair to say that participants' mean estimate that `r paste(round(mean(df$detailed_methodology, na.rm = TRUE), 2), "%", sep = "")` of studies have this characteristic represents a rather optimistic projection.


```{r norms in practice}

# select columns that relate to norms in practice & pivot it long & drop the na values
in_prac_long <- df %>% 
  dplyr::select(c(id,preregistration:open_access)) %>% 
      pivot_longer (cols = -id,
                names_to = "practice",
                values_to = "prevalence rating") %>% 
  drop_na()

# get the n values for each of the practices
in_prac_counts <- in_prac_long %>% 
  dplyr::group_by(practice) %>% 
  count() %>% 
  ungroup()

# calculate means
in_prac_ms_sds <- in_prac_long %>%
  dplyr::group_by(practice) %>%
  summarise(mean = mean(`prevalence rating`),
            sd = sd(`prevalence rating`)) %>%
  ungroup()

# join the n values with the means
in_prac_means <- in_prac_counts %>% inner_join(in_prac_ms_sds)
  
# calculate 95% CIs
error <- qnorm(0.975) * in_prac_means$sd/sqrt(in_prac_means$n)
in_prac_means$lower <- in_prac_means$mean - error
in_prac_means$upper <- in_prac_means$mean + error


# combine means & SDs
in_prac_means$`Prevalence Ratings` <- paste0(round_tidy(in_prac_means$mean,2),
                          " (", 
                          round_tidy(in_prac_means$sd,2),
                            ")", 
                            " [",
                            round_tidy(in_prac_means$lower,2),
                            ", ", 
                            round_tidy(in_prac_means$upper,2),
                            "]")

# clean it up into a new dataframe with only the columns we need
in_practice_means <- in_prac_means %>% 
  dplyr::select(practice, `Prevalence Ratings`) %>% 
  rename(Practice = practice)

# breadcrumbs:
# I need to use apa_tables.... (in line with papaja)
# update practice descriptions so they make sense to readers

  
knitr::kable(in_practice_means,
             caption = "Means, standard deviations, and 95% confidence intervals for the prevalence ratings for each research practice",
             digits = 2,
             align = c('lc'))
  


```


<!-- There's a key point for the discussion here, I think, in that the fact that many participants guessed near 50% for these practices suggests that students may have little idea of the prevalence of these practices in reality, and certainly education is needed on that point. 

We can add a table here with descriptive statistics (means? more?) for the norms in practice items.

--> 

## The replication crisis
Interestingly, only `r round(sum(df$crisis==1, na.rm = TRUE)/length(df$crisis)*100, 0)`% of participants indicated they had heard of "the replication crisis", with `r round(sum(df$crisis==2, na.rm = TRUE)/length(df$crisis)*100, 0)`% indicating that they had not heard of the crisis and `r round(sum(is.na(df$crisis))/length(df$crisis)*100, 0)`% missing.

Participants who indicated they had heard of the replication crisis were asked what "What do you think 'the replication crisis' is"? Of the `r length(df$crisis_text[is.na(df$crisis_text)==FALSE])` participants who answered this question, most provided responses roughly consistent with the conventional interpretation of the term. Example responses included "The crisis where most studies were unable to be replicated when researchers decided to replicate previously done studies" and "Where previously trusted studies have been replicated the results were not able to be replicated". Only `r sum(crisis_descriptions$Accurate == "n")` responses were suggestive of a misunderstanding of the term (e.g., "Plagiaris[m]", "only same results being published").

Participants' estimates of the replicability of psychological studies were variable. The mean, _M_ =`r mean(df$rep_perc, na.rm = TRUE)` was near the midpoint of 50%, but with substantial variability around this estimate, _SD_ = `r sd(df$rep_perc, na.rm = TRUE)`, minimum = `r min(df$rep_perc, na.rm = TRUE)`,  maximum = `r max(df$rep_perc, na.rm = TRUE)`. A histogram of participants' responses is provided in Figure X.

```{r rep_perc_histogram}
hist(df$rep_perc, main = NULL, xlab = "Percentage")
```
_Figure X_. Histogram of participants' responses to the question: "Imagine that a set of researchers select 100 published studies about psychology, and repeat each study again, exactly how it was described in the original report but with a new set of participants. We call repeating a study like this a “replication study”. At a guess, how many of these replication studies do you think would produce the same conclusions as the original studies?" Responses provided on a visual slider.

<!-- Participants were also asked "Which scientific disciplines do you think are most affected by the replication crisis?" As we discussed, I'm not sure this response will really be useful to look at given that we have "primed" them to think of psychology as being most affected (and indeed that's what the responses largely indicate).-->


# Discussion

Our findings suggest that despite fairly significant media attention, most incoming psychology students are unaware of the replication crisis in psychology. In our study they tended to agree more with open science norms than they did counternorms, although this wasn't the case for all open science practices (and especially not sharing of preprints). 

Participants' estimates of the proportion of psychological studies published each which apply each of a set of ten open science practices mostly tended to hover near 50%. Although this could be read as implying literally that participants believe these practices are roughly as commonly applied as they are not, it could also be read as implying a great deal of uncertainty on the part of participants [might need to unpack this idea - i.e., that if people have no idea they'll probably pick 50%].

Overall these findings suggest that while incoming psychology students may have a degree of sympathy for open science norms, they know relatively little about replicability and open science. 

<!--
Obviously more to say - just starting with a very rough summary.
-->

## Strengths and limitations
Our study had a relatively large sample size, but used convenience/haphazard sampling. The results also reflect responses of just one sample of students at one point in time. Generalisations to a wider population of incoming students (or students beginning psychology students in the future) can be made only very tentatively. Our questions also focused primarily on attitudes and beliefs, rather than probing students' factual knowledge of findings or concepts.

<!--
More to say here.
-->


## Directions for future research


\newpage
# Reproducible Code Statement

We used `r cite_r("r-references.bib")` for all our analyses.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>

\endgroup
