---
title: What do incoming university students believe about open science practices in
  psychology?
author:
  - name: Jennifer L. Beaudry
    affiliation: '1,4'
    role:
      - Conceptualization
      - Data curation
      - Formal analysis
      - Investigation
      - Methodology
      - Project administration
      - Validation
      - Visualization
      - Writing - original draft
      - Writing - review & editing
    corresponding: yes
    email: jen.beaudry@flinders.edu.au
    address: Research Development and Support, Flinders University, Bedford Park, SA, 5042
  - name: Matt N. Williams
    affiliation: '2'
    role:
      - Investigation
      - Methodology
      - Validation
      - Writing - original draft
    email: m.n.williams@massey.ac.nz
  - name: Michael C. Philipp
    affiliation: '2'
    role:
      - Conceptualization
      - Investigation
      - Methodology
      - Project administration
      - Writing - original draft
    email: m.philipp@massey.ac.nz
  - name: Emily J. Kothe
    affiliation: '3'
    role:
      - Conceptualization
      - Data curation
      - Validation
      - Visualization
      - Writing - review & editing
    email: emily.kothe@deakin.edu.au
affiliation:
  - id: '1'
    institution: Flinders University, Australia
  - id: '2'
    institution: Massey University, New Zealand
  - id: '3'
    institution: Deakin University, Australia
  - id: '4'
    institution: Swinburne University of Technology, Australia
shorttitle: Psychology students' beliefs about open science
output:
  papaja::apa6_word: default
  papaja::apa6_pdf: default
acknowledgement: |
  The idea of this research was conceived at the 2018 UQ Open Science Conference and further developed at the 2019 Society for the Improvement of Psychological Science Unconference, What do first year psychology students think about (open) science?
  Our special thanks to those who contributed to the data collection effort including: Benjamin Le, Katie Gilligan-Lee, Annayah Prosser, Ian Fairholm, Jennie Ferrell, Kait Clark, Catherine Orr, Nick Burns, Rachel Searston, Rachel Stephens, Matthew Thompson, Aaron Drummond, Kathryn McGuigan, and Peter Cannon.
abstract: |
 Background: Understanding students’ naive conceptions about the norms that guide scientific best practice is important so that teachers can adapt to students’ existing understandings.
 Objective: We examined what incoming undergraduate students of psychology believe about reproducibility and open science practices.
 Method: We conducted an online survey with participants who were about to start their first course in psychology at a university (*N* = 239).
 Results: When asked to indicate how a researcher should conduct her study, most students endorsed several open science practices. When asked to estimate the proportion of published psychological studies that follow various open science practices, participants' estimates averaged near 50%. Only 18% of participants reported that they had heard the term "replication crisis".
 Conclusion: Despite media attention about the replication crisis, few incoming psychology students in our sample were familiar with the term. The students were nevertheless in favour of most open science practices, although they overestimated the prevalence of some of these practices in psychology.
 Teaching Implications: Teachers of incoming psychology students should not assume pre-existing knowledge about open science or replicability.
keywords: open science, psychology, teaching, reproducibility, replication
wordcount: 3009
bibliography: ['r-references.bib','Undergrads & Open Science (public).bib']
floatsintext: yes
figurelist: no
tablelist: no
footnotelist: no
linenumbers: yes
mask: no
draft: no
documentclass: apa6
classoption: man
editor_options: 
  chunk_output_type: console
csl: "apa.csl" 
---

<!--
Teaching in Psychology ask for structured abstracts - 'For these types of article submissions, please include (in this order) a section on the background, objective(s), method, results, conclusion, and teaching implications.' 200 word limit
Abstract currently includes hand-typed numbers because it's hard to get them to knit up here since data processing is further below.
-->

<!-- To knit to RMarkdown you will probably need the papaja package installed, and to knit to .pdf you will need a tex distro (e.g., TinyTex). However, it is possible to run all the R code (without knitting to .docx or .pdf) by clicking Run > Run All Chunks Below. This could be useful in future for readers interested in reproducing results but who don't have or wish to install papaja and its many dependencies. -->

```{r setup, include = FALSE}

if(! requireNamespace('box')) {
  install.packages('box')
}

library(papaja) #install with devtools
library(here)
library(tidyverse)
library(finalfit) # to keep trailing zeros & round_tidy
library(tidyselect) 
library(broom)

box::use(irr[agree,kappa2]) # load just these two functions from the `irr` package

r_refs("r-references.bib")

```

```{r analysis-preferences}

knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed,
  results = 'as.is')

```

```{r functions}

p_tidy <- function (x, digits, prefix = "= ") {
    x.out = paste0(prefix, round_tidy(x, digits))
    all_zeros = paste0(prefix, round_tidy(0, digits))
    less_than = paste0("< ", format(10^-digits, scientific = FALSE))
    x.out[x.out == all_zeros] = less_than
    return(x.out)
}

meta_rename <-  function(df, metadata, old, new) {
  
  keys   <- metadata[[deparse(substitute(old))]]
  values <- metadata[[deparse(substitute(new))]]
  
  matchlist <- setNames(values, keys)
  names(df) <- matchlist[names(df)]
  return(df)
  
}
```


```{r import data}

# import the main data 
df <- read.csv (here::here("data", "students_processed.csv"))

# upload the replication crisis responses, marked as accurate and inaccurate (outside of R)
crisis_descriptions = read.csv(here::here ("data", "crisis_descriptions_marked.csv"))
inter_rater <- read.csv(here::here("data", "crisis_descriptions_marked_comparison.csv"))

# import the metadata

metadata <- here::here("data", "students_metadata.csv") %>%
read_csv(col_names = TRUE, skip_empty_rows = TRUE) %>%
filter(!is.na(old_variable))

# convert variables to factors as needed
df$age <- as_factor(df$age)
df$eligibility <- as_factor(df$eligibility)

# convert factors to characters as needed
df$university <- as.character(df$university)
df$degree <- as.character(df$degree)
df$major <- as.character(df$major)
df$major <- as.character(df$major)
df$uni_country <- as.character(df$uni_country)
df$nationality <- as.character(df$nationality)

```

Methodological and reporting practices in psychology have changed over the last decade, partly precipitated by the "replication crisis": The discovery that many close replications of published studies did not replicate the original findings [@kleinInvestigatingVariationReplicability2014; @kleinManyLabsInvestigating2018]. Of course, psychology is not alone; the replication crisis extends to other fields, including economics [@camerer_evaluating_2016] and medicine [e.g., @errington_challenges_2021]. These apparent problems with replication have led to various potential solutions to make research practices more reproducible and transparent, including more published replication studies [see @brandtReplicationRecipeWhat2014], more thorough reporting of methods and results [@wigboldusEncouragePlayingData2016], open sharing of data [see @meyerPracticalTipsEthical2018], and preregistration of data collection and analytic plans [@nosekPreregistrationRevolution2018]. These calls for more reproducible and transparent research practices have prompted the discipline to examine researchers' norms, attitudes, and practices [e.g., @baker500ScientistsLift2016; @beaudrySwinburneOpenScience2019; @harrisUseReproducibleResearch2018]. 

These changes present significant pedagogical challenges for the teacher of psychology. Keeping textbooks and other instructional materials up to date is difficult when supposedly well-established findings are contradicted by new replications emerging at a rapid pace. Furthermore, knowledge of emerging methodological practices is crucial for graduate student training. Even for students who do not go on to conduct research, an understanding of contemporary methodological practices---and problematic methodological practices---is essential for becoming informed and critical consumers of psychological knowledge. Studies have explored strategies for educating psychology students about replicability and open science practices [e.g., @chopikHowWhetherTeach2018; @graheHarnessingUndiscoveredResource2012; @jekelHowTeachOpen2020]. These initiatives may help ingrain open science norms and change attitudes about research practices, but we know little about what students know or believe about open science research practices prior to entering the university classroom. This knowledge could be useful for two main reasons.

First, unlike some psychological phenomena in undergraduate courses (e.g., the internal workings of human senses), the replication crisis is frequently discussed in mainstream and social media [e.g., @ogradyPsychologyReplicationCrisis2020; @yongPsychologyReplicationCrisis2016; @yongPsychologyReplicationCrisis2018]. As such, incoming students might have some knowledge about these issues prior to their formal studies. Teaching methods should be informed by understanding students' pre-existing levels of knowledge. 

Second, anecdotal evidence suggests that students being taught about open science practices and reproducibility reforms (e.g., open sharing of data and code) are sometimes surprised that these are not already standard practice. This could imply a need for educators to reinforce and build on students' “naive” impressions rather than radically altering their understanding of research practices. On the other hand, some evidence suggests that undergraduate students may quickly begin to engage in practices that hamper reproducibility. For example, @moranKnowItBad2021 found that 26.5% of the undergraduate students in their  sample admitted to “Conducting multiple statistical analyses on the same dataset in an attempt to find a statistically significant result” (p. 14; i.e., *p*-hacking), while 9.6% reported rounding down *p*-values to meet a significance threshold. Knowing the extent to which incoming psychology students may---or may not---be “naive open scientists” could help guide pedagogical approaches.

To examine this, we conducted a descriptive study, asking incoming students in undergraduate psychology courses about their beliefs regarding reproducibility and open science practices. Our survey encompassed questions concerning norms (how students felt research should be conducted), norms in practice (how students believe psychological research is conducted), and replicability (how replicable students believe psychological research is). Our study was exploratory [see @wagenmakersAgendaPurelyConfirmatory2012] and descriptive; as such, we did not specify or test hypotheses. 

# Methods
We preregistered the exploratory nature of this study at https://tinyurl.com/4p64mvwp [blinded]. Our materials, data, and analysis script are available at https://tinyurl.com/excm4heu [blinded]. Ethical approval was granted by [blinded].

```{r inclusion}

# participant numbers (all & included cases)
df_all <- df
n_all <- nrow(df_all)

# only participants who met the inclusion criterion
df <- df_all %>% 
  filter(exclude %in% "include") 
n <- nrow(df)

ex <- df_all %>% 
  filter(exclude %in% "exclude") %>% # n for those excluded
  nrow()

# note re: exclusion [all coded in `exclude` in preprocessing, 
# but need to summarise exclusion criteria here]

age_ex <- df_all %>%
  filter(age_criteria %in% "exclude") %>% 
  nrow()

el <- df_all %>%
  filter(eligibility_criteria %in% "exclude") %>% 
  dplyr::count(eligibility_lab) # need the breakdown by condition

status <- df_all %>% 
  filter(status_criteria %in% "exclude") %>% 
  dplyr::count(status_lab) # need the breakdown by condition

nmiss <- df_all %>% 
  filter(nmiss_criteria %in% "exclude") %>% 
  nrow()

```


```{r demographics for those included in analyses}

# age
age <- df %>% dplyr::count(age_lab)

# gender 
gender <- df %>% dplyr::count(gender)

# high school status
hs_status <- df %>% dplyr::count(high_school_lab)

# high school psyc 
hs_psyc <- df %>% dplyr::count(psych_hs_lab)

# country of university 
uni_country <- df %>% dplyr::count(uni_country)

# country of nationality
nationality <- df %>% dplyr::count(nationality)

# whether parents' attended uni
first_gen <- df %>% dplyr::count(first_gen_lab)
```

## Procedure

Contacts from tertiary institutions in Australia, New Zealand, the UK, and the USA invited students enrolled in their first-year psychology course to participate in the online survey hosted on Qualtrics. These contacts were identified via several avenues: a session at the annual meeting of the Society for the Improvement of Psychological Science (2019), a Twitter thread, and through contacts of the research team. We did not have inclusion criteria with respect to country, but most contacts who advertised the study were from the United Kingdom, Australia, or New Zealand.

Each contact sent their students invitations to participate via email or a course website 1–2 weeks before their course began. The invitation specified that students needed to be 18 years or older and starting their first course/unit of study in psychology at a university within the following month and included a link to the survey. The measures and items in the survey were presented in the same order for all participants.

## Participants

Of those who started the survey (*n* = `r n_all`), we screened out `r ex` 
participants who were ineligible based on our
preregistered exclusion criteria. Specifically, we screened out people who were younger 
than 18 years (*n* = `r age_ex`). We also screened out individuals who had already 
started a psychology unit at a university, regardless of whether they had 
completed it (*n* = `r el[1,2] + el[3,2] + el[4,2]`); were not enrolled in a 
psychology unit at a university (*n* = `r el[2,2]`); or did not answer this 
question (*n* = `r el[5,2]`). We excluded additional responses that 
Qualtrics flagged as spam (*n* = `r status [1,2]`) or as a survey preview 
(*n* = `r status [2,2]`). Finally, we excluded participants who met all other 
eligibility criteria, but did not respond to any of the main survey items (*n* = `r nmiss`). 

Of the eligible `r n` participants, most reported that they were 18–24 years old (*n* = `r age[1,2]`);
`r age [2,2]` reported that they were 25–34 years old, `r age [3,2]` were 35–44 
years old, `r age [4,2]` were 45–54 years old, and `r age [5,2]` was 55–64 years old. 
Most participants (*n* = `r gender[3,2]`) reported that they were women, 
`r gender[1,2]` reported that they were men, and `r gender[2,2]` participant reported 
non-binary gender.

Most participants attended university in Australia (*n* = `r uni_country[1,2]`), 
the United Kingdom (*n* = `r uni_country[5,2]`), or New Zealand (*n* = 
`r uni_country[4,2]`). Nearly all reported graduating from high school or secondary 
school (*n* = `r hs_status[3,2]`). Of those who attended high school, 
fewer than half completed high school psychology courses (*n* = `r hs_psyc[2,2]`).
Participants reported `r nrow(nationality)` different nationalities; the most 
common were from Australia (*n* = `r nationality[3,2]`), the United Kingdom (*n* = 
`r nationality[24,2]`), and China (*n* = `r nationality[6,2]`). About two-thirds of 
participants reported that at least one of their parents attended university 
(*n* = `r first_gen[2,2]`). 

## Material

### Open Science Norms and Counternorms

To measure norms and counternorms, we focussed on participants' evaluations of ten specific open science practices that reflected diverse open science norms (see Table \@ref(tab:normcounternormitems)). Although we acknowledge the debatable merits of some of these practices [@TQMP16-4-376; @rubinWhenDoesHARKing2017; @szollosiPreregistrationWorthwhile2020], we selected the items to be emblematic of common concerns in the open science movement.

Each practice was elucidated by a pair of items (pro-open science; counter-open science) in the context of a common scenario (see the note accompanying Table \@ref(tab:normcounternormitems)). Participants rated their agreement separately for each norm and counternorm on a 1 (*strongly disagree*) to 5 (*strongly agree*) scale. We used the scenario of a specific researcher rather than referring to psychology researchers in general because the items were more concrete, engaging, and clear. 


```{r normcounternormitems}

# create a table with the norms and counternorm items from the metadata

norm_items <- metadata %>% 
  filter(scale == "norms") %>% 
  mutate(Category = case_when(str_detect(new_variable, "_cnorm") ~ "Counternorm",
         TRUE ~ "Norm"))   %>% 
  select (c(text_label, item_text, Category))


norm_items_wide <- pivot_wider(
  norm_items,
  names_from = Category,
  values_from = item_text) %>% 
  relocate(Norm, .before = Counternorm)


knitr::kable(norm_items_wide,
             caption = "Norm and Counternorm Items",
             col.names = c("Practice", "Norm", "Counternorm"),
             align = c('lll'))

```

*Note*: Participants responded to each item in the context of the scenario, “Imagine that Deborah is a psychology researcher who has designed a study to test a specific hypothesis.” Responses were provided on a Likert-type response scale ranging from 1 (*strongly disagree*) to 5 (*strongly agree*).

\ 

### Beliefs About the Practice of Psychological Science

We measured participants' normative beliefs about how researchers actually conduct psychological science. We asked, "When you consider all of the psychology studies conducted globally each year, what percentage of them would have the following characteristics?" These practices corresponded approximately to our ten norm–counternorm pairs, with two exceptions (see Table \@ref(tab:normsinpracticeitems)). We excluded the critical thinking norm because it did not refer to externally observable behaviour, and we included data availability on request out of interest. Participants provided their estimate on a visual slider from 0% to 100%.


```{r normsinpracticeitems}

# create a table with the norms in practice items from the metadata

practice_items <- metadata %>% 
  filter(scale == "practice") %>% 
  select (c(text_label, item_text))

knitr::kable(practice_items,
             caption = "Norms in Practice Items",
             col.names = c("Practice", "Item"),
             align = c('ll'))

```
\ 

### Beliefs About Replications

Two questions measured participants' beliefs about the replicability of results in psychological science. The first asked participants to: 

> "Imagine that a set of researchers selected 100 published studies about psychology, and repeat each study again, exactly how it was described in the original report but with a new set of participants. We call repeating a study like this a 'replication study.' At a guess, how many of these replication studies do you think would produce the same conclusions as the original studies?”

Estimates were recorded using a visual slider from 0% to 100%. 
A second question asked participants whether they had heard of the replication crisis. Those who had heard of the crisis were asked two additional open-ended questions: (1) describe the replication crisis, and (2) name the scientific disciplines most affected by the replication crisis. 

# Results

As preregistered, we calculated the difference in means
between responses to each norm item and its accompanying counternorm. For this analysis, we removed three cases with missing values. 
Table \@ref(tab:differencebetweennormandcounternorm) displays the means and standard deviations for each norm, counternorm, and the difference values. See Table \@ref(tab:corrbetweennormandcounternorm) in the Supplemental Materials for the correlations between norm and counternorm items.

In general, the students seemed to endorse open science practices: For seven of the ten open science practices, agreement with the norm item was stronger than for the corresponding counternorm item. However, participants' endorsement of norms varied substantially across different practices. Participants most strongly endorsed norms that researchers should provide sufficient information about studies to permit replicability, be critical of published findings, and avoid *p*-hacking. They also tended to agree that researchers should avoid HARKing, share papers as open access, share open materials, and preregister their studies. On the other hand, they tended to disagree with the practice of sharing preprints prior to peer review and, to a lesser extent, with sharing open data.


```{r differencebetweennormandcounternorm}

# select columns that relate to norms & pivot it long
norm_long <- df %>% 
  dplyr::select(c(id, ends_with("_norm"))) %>% 
    pivot_longer (cols = ends_with("norm"),
                names_to = "practice",
                values_to = "norm rating") 

# remove "_norm" from the practice variables
norm_long$practice = gsub(pattern = "_norm", replacement = "", x = norm_long$practice)

# select columns that relate to counternorms
cnorm_long <- df %>% 
  dplyr::select(c(id, ends_with("_cnorm"))) %>% 
  pivot_longer (cols = ends_with("cnorm"),
                names_to = "practice",
                values_to = "cnorm rating") 

# remove "_cnorm" from the practice variables
cnorm_long$practice = gsub(pattern = "_cnorm", replacement = "", x = cnorm_long$practice)

# join the norm and counternorm tibbles
norm_cnorm <- norm_long %>% inner_join(cnorm_long)

# create a `diff` variable that is the counternorm subtracted from the norm
norm_cnorm_long <- norm_cnorm %>% 
  mutate (diff = (`norm rating` - `cnorm rating`)) %>% 
    drop_na() %>% # drop the three NA values
      dplyr::select(-(`id`)) # remove this now that I've confirmed it works

# get the n values for each of the practices
prac_counts <- norm_cnorm_long %>% 
    dplyr::group_by(practice) %>%  
    count() %>% 
  ungroup()

# calculate means
prac_ms_sds <- norm_cnorm_long %>% 
    dplyr::group_by(practice) %>%  
    summarise(norm_mean = mean(`norm rating`), 
              norm_sd = sd(`norm rating`),
              cnorm_mean = mean(`cnorm rating`), 
              cnorm_sd = sd(`cnorm rating`),
              diff_mean = mean(`diff`), 
              diff_sd = sd(`diff`)) %>% 
  ungroup()

# join the n values with the means
prac_means <- prac_counts %>% inner_join(prac_ms_sds)
  
# calculate 95% CIs
error <- qnorm(0.975) * prac_means$diff_sd/sqrt(prac_means$n)
prac_means$lower <- prac_means$diff_mean - error
prac_means$upper <- prac_means$diff_mean + error


# combine means & SDs
prac_means$Norm <- paste0(round_tidy(prac_means$norm_mean,2),
                          " (", 
                          round_tidy(prac_means$norm_sd,2), 
                          ")")
prac_means$Counternorm <- paste0(round_tidy(prac_means$cnorm_mean,2), 
                            " (", 
                            round_tidy(prac_means$cnorm_sd,2), 
                            ")")
prac_means$Difference <- paste0(round_tidy(prac_means$diff_mean,2), 
                            " (", 
                            round_tidy(prac_means$diff_sd,2),
                            ")", 
                            " [",
                            round_tidy(prac_means$lower,2),
                            ", ", 
                            round_tidy(prac_means$upper,2),
                            "]")

# clean it up into a new dataframe with only the columns we need
practice_means <- prac_means %>% 
  rename(Practice = practice) %>% 
  dplyr::select(Practice, Norm, Counternorm, Difference)



practice_means$Practice <- c("Critical thinking", "HARKing", "Information for replicability", "Open access", "Open data", "Open materials", "p-hacking", "Preprints", "Preregistration", "Registered Reports")


knitr::kable(practice_means,
             caption = "Means and Standard Deviations for Norms, Counternorms, and the Difference between Norms and Counternorms, with 95% Confidence Intervals (in Square Brackets)",
             digits = 2,
             align = c('lccc'))
  

```


## Norms in Practice
Participants were also asked to estimate the proportion of psychology studies globally that apply each of ten practices (see Table \@ref(tab:normsinpractice)). For many of these practices, participants’ estimates were near the midpoint of 50%. Nonetheless, participants perceived some practices as being applied relatively infrequently: For example, the mean estimate of the prevalence of HARKing, preprint sharing before publication, and open data sharing were all near 30%. See Figure A in Supplemental Materials to see the distribution for each item. Participants perceived that the most common practice was providing enough information in written reports to permit replication without asking questions of the original researchers.


```{r normsinpractice}

# select columns that relate to norms in practice & pivot it long & drop the na values
in_prac_long <- df %>% 
  dplyr::select(c(id,preregistration:open_access)) %>% 
      pivot_longer (cols = -id,
                names_to = "practice",
                values_to = "prevalence rating") %>% 
  drop_na()

# get the n values for each of the practices
in_prac_counts <- in_prac_long %>% 
  dplyr::group_by(practice) %>% 
  count() %>% 
  ungroup()

# calculate means
in_prac_ms_sds <- in_prac_long %>%
  dplyr::group_by(practice) %>%
  summarise(mean = mean(`prevalence rating`),
            sd = sd(`prevalence rating`)) %>%
  ungroup()

# join the n values with the means
in_prac_means <- in_prac_counts %>% inner_join(in_prac_ms_sds)
  
# calculate 95% CIs
error <- qnorm(0.975) * in_prac_means$sd/sqrt(in_prac_means$n)
in_prac_means$lower <- in_prac_means$mean - error
in_prac_means$upper <- in_prac_means$mean + error


# combine means & SDs
in_prac_means$`Prevalence Ratings` <- paste0(round_tidy(in_prac_means$mean,2),
                          " (", 
                          round_tidy(in_prac_means$sd,2),
                            ")", 
                            " [",
                            round_tidy(in_prac_means$lower,2),
                            ", ", 
                            round_tidy(in_prac_means$upper,2),
                            "]")

# clean it up into a new dataframe with only the columns we need
in_practice_means <- in_prac_means %>% 
  dplyr::select(practice, `Prevalence Ratings`) %>% 
  rename(Practice = practice)

# reorder the tibble to match the order in Table 2

in_practice_means <- in_practice_means %>% 
  arrange(match(Practice, c("preregistration", "registered_report", "phack", "harking", "info_for_rep", "preprint", "open_materials", "open_data", "available_data", "open_access")), desc(Practice))


in_practice_means$Practice <- c("Preregistration", "Registered Report", "p-hacking", "HARKing", "Information for replicability", "Preprints", "Open materials", "Open data", "Data on request", "Open access")
  
knitr::kable(in_practice_means,
             caption = "Means, Standard Deviations, and 95% Confidence Intervals for the Prevalence Ratings for Each Research Practice",
             digits = 2,
             align = c('lc'))
  

```

## The Replication Crisis

```{r coding qual responses}

inter_rater_test <- inter_rater %>% 
  select(c(coding_1, coding_2))

# calculate the percentage of agreement
perc_agree <- agree(inter_rater_test, tolerance = 0)
kappa <- kappa2(inter_rater_test)

```

Interestingly, only `r round(sum(df$crisis==1, na.rm = TRUE)/length(df$crisis)*100, 0)`% of participants indicated they had heard of "the replication crisis". Most (`r round(sum(df$crisis==2, na.rm = TRUE)/length(df$crisis)*100, 0)`%) indicated that they had not heard of the crisis; `r round(sum(is.na(df$crisis))/length(df$crisis)*100, 0)`% did not respond.

Participants who indicated they had heard of the replication crisis were asked, "What do you think 'the replication crisis' is"? Two independent coders marked the responses as accurate or inaccurate; they agreed on `r perc_agree$value`% of the descriptions and resolved disagreements through discussion. Of the `r length(df$crisis_text[is.na(df$crisis_text)==FALSE])` participants who answered this question, most (`r sum(crisis_descriptions$accurate =="y")/length(df$crisis_text[is.na(df$crisis_text)==FALSE])*100`%) provided responses consistent with the conventional interpretation of the term (i.e., describing a realisation that some scientific findings cannot be replicated). For example, "Where previously trusted studies have been replicated the results were not able to be replicated". Only `r sum(crisis_descriptions$accurate == "n")` responses indicated a misunderstanding of the term (e.g., "Plagiaris[m]").

Participants' estimates of the replicability of psychological studies were variable (see Figure \@ref(fig:repperchistogram)). The mean (_M_ = `r mean(df$rep_perc, na.rm = TRUE)`) was near the midpoint of 50%, but with substantial variability around this estimate (_SD_ = `r sd(df$rep_perc, na.rm = TRUE)`, range = `r min(df$rep_perc, na.rm = TRUE)` to `r max(df$rep_perc, na.rm = TRUE)`). 

```{r repperchistogram, fig.cap="Histogram of Participants' Estimates of the Percentage of Psychology Studies that Replicate"}

df %>%
  filter(!is.na(rep_perc)) %>% 
  ggplot(aes(x = rep_perc)) +
  geom_histogram(aes(y=stat(count)), binwidth = 10, colour = "black", fill = "grey") +
    xlab("Percentage") +
    ylab("Frequency") +
    theme_classic () +
    theme(axis.text = element_text(size = 11), axis.title = element_text(size = 11))
  
```


*Note:* Responses to the question: \"Imagine that a set of researchers select 100 published studies about psychology, and repeat each study again, exactly how it was described in the original report but with a new set of participants. We call repeating a study like this a \'replication study\'. At a guess, how many of these replication studies do you think would produce the same conclusions as the original studies?\" Responses provided on a visual slider from 0% to 100%.

<!-- Participants were also asked "Which scientific disciplines do you think are most affected by the replication crisis?" As we discussed, I'm not sure this response will really be useful to look at given that we have "primed" them to think of psychology as being most affected (and indeed that's what the responses largely indicate).-->


# Discussion

Despite efforts to improve the replicability of research and the corresponding media attention, most incoming psychology students in our sample were unaware of the replication crisis. Encouragingly, on the other hand, most tended to endorse open science norms more-so than counternorms. Notably this was not the case for all open science practices (namely sharing of pre-prints and open data). It may well be that scientific training has enshrined peer review as an essential practice that is robust to change.

Participants' estimates of the proportion of published psychological studies that apply open science practices tended to hover around 50%. Although this could imply that participants literally believe these practices are roughly as commonly applied as not, it might also represent participants' uncertainty or indifference about the application of these practices. This said, 50% was the modal response for only a minority of practices (open access, data availability on request). 

That the mean estimated prevalence was no higher than 64% for any practice is inconsistent with the sometime-reported claim that incoming students tend to naively assume that science is conducted openly. Nevertheless, the students appear to have at least somewhat over-estimated the prevalence of some open science practices. For example, on average students estimated that nearly a third of published psychological studies share data openly and that nearly half of studies are preregistered. In contrast, an examination of a random sample of 250 psychology articles published between 2014 and 2017 [@hardwickeEstimatingPrevalenceTransparency2021] found that just 2% shared open data and 3% were preregistered.


## Strengths and Limitations
Despite a moderate sample size, our results provide an imperfect picture of new psychology students’ methodological understandings. Our use of convenience samples coupled with the imprecision of polling students’ impressions at one point in time suggest that we can make only tentative generalisations to a wider population of incoming psychology students. In particular, the fact that our sample mostly comprised students from Australia, NZ, or the United Kingdom limits the extent to which our findings can be generalised to other populations.

Our questions focused primarily on attitudes and beliefs so that we could better document the implicit understanding that guided students’ thoughts about the research process. However, we did not probe students' factual knowledge of most methodological findings or concepts, except the replication crisis. It is quite possible that reform in secondary school instruction has improved students’ understandings of more fundamental statistical (e.g., confidence interval estimation) and design (e.g., direct replications) considerations without necessarily influencing their open science attitudes and beliefs. Yet, research at the tertiary level casts doubt that the methodological reform in psychology has had much impact on the content of our teaching [@friedrichReplicatingNationalSurvey2018].

Although we have interpreted participants' responses to the norm and counternorm items as reflecting  students' personal endorsement of practices, their perceptions of what we might perceive as the "right" answers could have also influenced their responses. Differences between endorsement of norms and counternorms might have also been affected by wording differences between the matched options; for example, the HARKing, information for replicability, and *p*-hacking counternorm items included the phrase "good scientific practice" whereas their matched norm items did not. More broadly, our choices of terminology in constructing items might have affected participants' responses to some degree. For example, the reference to keeping data "protected" in the open data counternorm item may have nudged participants towards endorsing this item because failing to "protect" participants' data might sound irresponsible. Future research could examine the influence of revised language on participants' responses. 

We did not investigate the degree to which results differed according to whether or not participants had studied psychology in high school. Interested readers could conduct this and other comparisons using our open data.

## Conclusion and Teaching Implications
Our findings suggest that these incoming psychology students had a degree of sympathy for open science norms. Nevertheless, despite significant media attention, few had heard of the replication crisis. Similarly, the students seemed to be relatively unfamiliar with responses to the replication crisis (e.g., open science practices). It is therefore important that teachers of psychology not assume pre-existing knowledge among incoming students. 

For teachers interested in systematically surveying their students' existing understanding of open science practices in psychology, our survey questions could be a useful resource (available at https://tinyurl.com/excm4heu [blinded]).


# Supplemental Materials

The supplemental materials contain three sets of results and details of the R packages we used in our analyses. In order, the three results are: (1) the correlations between norm and counternorm items; (2) histograms of participants’ prevalence ratings for each of the norms in practice items; and (3) an additional exploratory analysis examining whether the proportion of students who are unaware of the replication crisis differed between psychology and non-psychology majors. 

Table A contains the correlations between norms and counternorms, with the 95% confidence intervals and associated *p*-values.


```{r corrbetweennormandcounternorm}

# ADD CORRELATIONS (per participant, use df)

# use this tutorial to get the correlations for each practice (using the long data)
# https://cran.r-project.org/web/packages/broom/vignettes/broom_and_dplyr.html

# this will create a tibble for each practice

# calculates the correlation test for each tibble & then tidies it up
corr <- norm_cnorm_long %>% 
  nest(data = -practice) %>% # create a tibble for each practice
  mutate (test = map(data, ~ cor.test(.x$`norm rating`, .x$`cnorm rating`)), 
          tidied = map(test,tidy)) %>% # run the corr & tidy it
  unnest(tidied) # unnest it to look at the results

# remove unnecessary columns, round the values, and move columns
corr <- corr %>% 
  dplyr::select(-c(data, test, statistic, parameter, method, alternative))  %>%
  mutate(p.value = p_tidy(p.value, 3, prefix = "")) %>% 
  mutate(estimate = round_tidy(estimate,2), 
         conf.low = round_tidy(conf.low, 2), 
         conf.high = round_tidy(conf.high, 2)) %>% 
  dplyr::relocate(estimate, .before = conf.low) %>%  
    dplyr::relocate(p.value, .after = last_col())

# combine the confidence intervals into one column
corr$Correlation <- paste0(corr$estimate, 
                            " [",
                            corr$conf.low,
                            ", ", 
                            corr$conf.high,
                            "]")

# clean up the tibble                          
corr <- corr %>% 
  relocate (p.value, .after = last_col()) %>% 
  rename(Practice = practice) %>% 
  dplyr::select(-c(estimate, conf.low, conf.high))

# reorder the tibble to match the order in Table 3

corr <- corr %>% 
  arrange(match(Practice, c("critical", "hark", "info_for_rep", "open_access", "open_data", "open_materials", "phack", "preprint", "prereg", "reg_report")), desc(Practice))


corr$Practice <- c("Critical thinking", "HARKing", "Information for replicability", "Open access", "Open data", "Open materials", "p-hacking", "Preprints", "Preregistration", "Registered reports")


knitr::kable(corr,
             caption = "Correlations between Norms and Counternorms with 95% Confidence Intervals (in Square Brackets) and Associated p-values",
             digits = 2,
             align = c('lccc'))
  

```

\newpage

Figure A displays the histograms for each of the norms in practice items. 

```{r normsinpracticehistogram, fig.cap="Histogram of Participants' Prevalence Ratings (%) for each of the Norms in Practice Items."}

practice_labels <- c(
  available_data = "Data on\nrequest",
  info_for_rep = "Information\nfor replication",
  harking = "HARKing",
  phack = "p-hacking",
  open_access = "Open access",
  registered_reports = "Registered reports",
  open_data = "Open data",
  open_materials = "Open\nmaterials",
  preprint_pre = "Preprints",
  preregistration = "Preregistration",
  registered_report = "Registered\nreport"
)

in_prac_long %>%
  ggplot(aes(x = `prevalence rating`)) +
  geom_histogram(aes(y=stat(count)), binwidth = 10, colour = "black", fill = "grey") +
    xlab("Prevalence Ratings (%)") +
    ylab("Frequency") +
    theme_classic () +
    theme(axis.text = element_text(size = 9), axis.title = element_text(size = 9)) +
    facet_wrap(~practice, nrow = 2, labeller = labeller(practice = practice_labels))


```

*Note:* Responses provided on a visual slider from 0% to 100%.

\newpage

```{r awarenessofrepcrisisbymajor}

psyc_major <- df %>% 
  select(c(major_recoded, crisis)) %>% 
  na.omit() # remove NAs

not_psyc <- psyc_major %>% 
  filter(major_recoded == "Not psychology")

psyc <- psyc_major %>% 
  filter(major_recoded == "Psychology")

# run the chi-square
psyc_major_table <- table(psyc_major)

chi <- chisq.test(psyc_major_table)


```

At a reviewer's request, we ran an additional exploratory analysis to determine whether
the proportion of students who are unaware of the replication crisis differed between psychology 
and non-psychology majors. We were able to code the majors for `r length(psyc_major$major_recoded)` students; of these, `r round(length(psyc$crisis)/length(psyc_major$crisis)*100, 0)`% reported that psychology was their major. 

Of those majoring in psychology, `r round(sum(psyc$crisis==1)/length(psyc$crisis)*100,0)`% indicated that they 
were aware of the replication crisis. Of those majoring in another program other than psychology, 
`r round(sum(not_psyc$crisis==1)/length(not_psyc$crisis)*100,0)`% indicated that they were aware of the replication crisis. A chi-square test of independence indicated that there was no significant difference between psychology
majors and non-majors in terms of their self-reported awareness of the replication crisis, X^2^ (`r chi$parameter`) = `r round(chi$statistic,2)`, *p* = `r round(chi$p.value,2)`. 
This lack of a significant difference should be replicated in a larger population; however, it suggests that psychology instructors should not presume that students choosing to major in psychology are aware of the replication crisis. 


\newpage

# Reproducible Code Statement

We used `r cite_r(file = "r-references.bib", pkgs = c("broom", "finalfit", "gendercoder", "here", "papaja", "tidyverse"), withhold = FALSE)` for our analyses.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>

<!-- We have four references that aren't including the urls. Including here so we can add them to the final document.

Beaudry, J. L., Kaufman, J., Johnstone, T., & Given, L. (2019). Swinburne Open Science Task Force Survey Report. https://osf.io/eczvd/

O’Grady, C. (2020, December 9). Psychology’s replication crisis inspires ecologists to push for more reliable research. Science. https://www.sciencemag.org/news/2020/12/psychology-s-replication-crisis-inspires-ecologists-push-more-reliable-research

Yong, E. (2016, March 4). Psychology’s replication crisis can’t be wished away. The Atlantic. https://www.theatlantic.com/science/archive/2016/03/psychologys-replication-crisis-cant-be-wished-away/472272/


Yong, E. (2018, November 19). Psychology’s replication crisis is running out of excuses. The Atlantic. https://www.theatlantic.com/science/archive/2018/11/psychologys-replication-crisis-real/576223/ -->




\endgroup
