---
title: What do incoming university students believe about open science practices in
  psychology?
author:
- name: Jennifer L. Beaudry
  affiliation: '1'
  corresponding: yes
  address: Postal address
  email: jbeaudry@swin.edu.au
  role: null
- name: Michael C. Philipp
  affiliation: '2'
  role: null
- name: Matt N. Williams
  affiliation: '2'
  role: null
shorttitle: Psychology students' beliefs about open science
output:
  papaja::apa6_pdf: default
  papaja::apa6_word: default
authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.
  Enter author note here.
abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  One or two sentences to put the results into a more **general context**.
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  <!-- https://tinyurl.com/ybremelq -->
keywords: open science, psychology, teaching, reproducibility, replication
wordcount: X
bibliography: Matt_Zotero_Library.bib
floatsintext: yes
figurelist: no
tablelist: no
footnotelist: no
linenumbers: yes
mask: yes
draft: no
documentclass: apa6
classoption: man
affiliation:
- id: '1'
  institution: Swinburne University of Technology
- id: '2'
  institution: Massey University
---
<!-- I've only partially entered authorship deets for the moment and set it to mask authorship info when knitting
It's set to default to knit to .docx, but can also knit to pdf (if you have a tex distro installed)
The pdf looks nicer and might be good for a preprint but .docx probably easiest to use for journal submissions.
To knit to RMarkdown you will probably need the papaja package installed, and to knit to .pdf you will need a tex distro (e.g., TinyTex).
However, it is possible to run all the R code (without knitting to .docx or .pdf) by clicking Run > Run All Chunks Below.
This could be useful in future for readers interested in reproducing results but who don't have or wish to install 
papaja and its many dependencies.
-->

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```
<!--Intro below is very rough, just making sure everything works here in markdown really.-->

The last decade has seen unprecedented change in methodological and reporting practices in psychology. These changes were partly precipitated by what is popularly known as the "replication crisis": The discovery that close replications of published psychological studies are often unable to replicate the original findings[@kleinInvestigatingVariationReplicability2014; @kleinManyLabsInvestigating2018]. For example, in an attempt to replicate 100 published psychological studies, only 38% were adjudged to have replicated the original result, and the average effect size was half that in the original studies [@opensciencecollaborationEstimatingReproducibilityPsychological2015]. 

These apparent problems with replicability led to speculation regarding its causes, and a variety of potential solutions being proferred. Some of these potential solutions include more frequently conducting and publishing replication studies [see @brandtReplicationRecipeWhat2014], more transparent reporting of methods and results [@simmons21WordSolution2012], open sharing of data [see @meyerPracticalTipsEthical2018], and preregistration of data collection and analysis plans [@nosekPreregistrationRevolution2018]. In some cases changes to practices have been drastic and widespread: For example, the practice of accepting *Registered Reports* (wherein peer review and in principle acceptance for publication happens *prior* to data collection) was first proposed in 2012 [see @chambersRegisteredReportsPresent2020 for a history] and is already offered by more than 250 journals (https://osf.io/rr/).

Many of these recent changes have links to the classic *Mertonian* norms of science [@mertonSociologyScienceTheoretical1973]. For example, the practice of sharing open source software directly corresponds to the Mertonian norm of *communism*: Scientists should have common ownership of scientific goods. Similarly, the practices of sharing of preprints for open peer review and open data for checking of reproducibility corresponds to the Mertonian norm of *organised skepticism*: Scientific claims should be subjected to critical scrutiny. Even the practice of preregistration has a connection to the norm of *disinterestedness*, in that a preregistration acts as a social control reducing the potential impact of personal investment in a particular outcome from affecting the choice of results reported in a given study. In this sense, the ongoing reform in psychological research can partly be understood as simultaneously a set of new practices and a re-affirmation of old norms.

Nevertheless, rapid changes in methodological practice and empirical findings present significant pedagogical challenges for the teacher of psychology. Keeping textbooks and other instructional materials "up to date" is obviously difficult when supposedly well-established findings are being contradicted by new replications emerging at a rapid pace. Furthermore, training in emerging methodological practices is crucial for graduate students who may go on to apply psychological research methods themselves, such that the next generation of researchers can produce research which is more replicable than that of the last. Even for students who do not go on to conduct research themselves, an understanding of contemporary methodological practices - and problems with methodological practices in psychology - is essential for these students to become informed and critical consumers of knowledge about psychology. However, the question of how best to provide this understanding is by no means trivial. Indeed, there are several reasons why it might be unwise to assume a simple knowledge deficit, where students lack knowledge about reproducibility and open science practices and the teacher's only role is to communicate this knowledge. 

First, unlike some areas of psychology covered in undergraduate courses (e.g., models of working memory, or the internal workings of human senses), the replication crisis is frequently discussed in mainstream and social media: Many students may plausibly have some knowledge about these issues already. Teaching methods should thus be informed by some understanding of what students' pre-existing levels of knowledge are. 

Second, when psychology students are just beginning their university studies, they are often learning for the first time about how and why it is useful to apply scientific methods to studying human behaviour (rather than only relying on alternative sources of knowledge such as intuition or anecdote or authority). Could an over-emphasis on problems with replicability leave such students unconvinced that scientific methods for studying human behaviour are valuable *at all*, leaving them to favour even less credible alternative sources of knowledge? It is therefore important to determine what levels of trust in psychological research (and its replicability) are prevalent in incoming undergraduate students, rather than simply assuming that the correct degree of calibration is always to prompt students to trust psychology research *less* than they already do. In their examination of the usefulness of a one-hour lecture about the replication crisis, @chopikHowWhetherTeach2018 found that undergraduate students trusted the results of studies by psychologists less after the lecture than they did before it, although the effect size was fairly small (d = -.36), and mean trust levels remained fairly high after the lecture (M = 4.94 on a scale of 1 to 7). 

Third, anecdotal evidence suggests that students being taught about open science practices and reforms to improve reproducibility (such as open sharing of data and analysis code) are often surprised that this is not *already* standard practice. In other words, their naive conception of how science works may in some ways be closer to that embodied in recent reforms rather than traditional practices. It is thus possible that, for some students, teaching about reform may be less a matter of conveying new information and more of reinforcing "naive" assumptions. In fact, although beyond the scope of this article, it may possibly be the case that incoming undergraduate students more strongly endorse open science norms than do academics.

These considerations suggest that it is important that teachers wishing to inform undergraduate psychology students about the replication crisis and open science practices have some understanding of what such students actually know and believe about these topics already. In addition, undergraduate psychology students themselves represent one of the most important audiences for psychological research: The number of undergraduate students enrolled in psychology courses and learning about psychological research in any given year dwarfs the number of academics working in psychology. As such, the *preferences* of these students with respect to reproducibility and open science practices are themselves important as a phenomenon of interest. If, for example, undergraduate psychology students have a strong preference that journal articles are freely available to members of the public, this preference on the part of some of the consumers of the knowledge we produce should have some bearing on our choices in relation to sharing of manuscripts. This is especially the case given that undergraduate students are directly or indirectly responsible for much of the funding which allows universities to operate and research to be conducted. In fact, in serving as participants for course credit in many universities, undergraduate psychology students also provide the data underlying much psychological research.

For these reasons, we aimed to conduct a study describing what incoming undergraduate students of psychology believe about reproducibility and open science practices in psychology. Our survey encompassed norms (how students felt research *should* be conducted), norms in practice (how students believe psychological research *is* conducted) and replicability (how replicable students believe psychological research is). In doing so we hope to provide knowledge which can inform the pedagogy of teaching about replication and open science practices.

# Methods
We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

## Participants


## Material

## Procedure

## Data analysis
We used `r cite_r("r-references.bib")` for all our analyses.


# Results

# Discussion


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
