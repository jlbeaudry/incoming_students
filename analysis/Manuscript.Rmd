---
title: What do incoming university students believe about open science practices in
  psychology?
author:
- name: Jennifer L. Beaudry
  affiliation: '1'
  corresponding: yes
  address: Postal address
  email: jbeaudry@swin.edu.au
  role: null
- name: Michael C. Philipp
  affiliation: '2'
  role: null
- name: Matt N. Williams
  affiliation: '2'
  role: null
shorttitle: Psychology students' beliefs about open science
output:
  papaja::apa6_word: default
  papaja::apa6_pdf: default
authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.
  Enter author note here.
abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  One or two sentences to put the results into a more **general context**.
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  <!-- https://tinyurl.com/ybremelq -->
keywords: open science, psychology, teaching, reproducibility, replication
wordcount: X
bibliography: Matt_Zotero_Library.bib
floatsintext: yes
figurelist: no
tablelist: no
footnotelist: no
linenumbers: yes
mask: yes
draft: no
documentclass: apa6
classoption: man
affiliation:
- id: '1'
  institution: Swinburne University of Technology
- id: '2'
  institution: Massey University
---
<!-- I've only partially entered authorship deets for the moment and set it to mask authorship info when knitting
It's set to default to knit to .docx, but can also knit to pdf (if you have a tex distro installed)
The pdf looks nicer and might be good for a preprint but .docx probably easiest to use for journal submissions.
To knit to RMarkdown you will probably need the papaja package installed, and to knit to .pdf you will need a tex distro (e.g., TinyTex).
However, it is possible to run all the R code (without knitting to .docx or .pdf) by clicking Run > Run All Chunks Below.
This could be useful in future for readers interested in reproducing results but who don't have or wish to install 
papaja and its many dependencies.
-->

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```
<!--Intro below is very rough, just making sure everything works here in markdown really.-->

The last decade has seen unprecedented change in methodological and reporting practices in psychology. These changes were partly precipitated by what is popularly known as the "replication crisis": The discovery that close replications of published psychological studies are often unable to replicate the original findings[@kleinInvestigatingVariationReplicability2014; @kleinManyLabsInvestigating2018]. For example, in an attempt to replicate 100 published psychological studies, only 38% were adjudged to have replicated the original result, and the average effect size was half that in the original studies [@opensciencecollaborationEstimatingReproducibilityPsychological2015]. 

These apparent problems with replicability led to speculation regarding its causes, and a variety of potential solutions being proferred. Some of these potential solutions include more frequently conducting and publishing replication studies [see @brandtReplicationRecipeWhat2014], more transparent reporting of methods and results [@simmons21WordSolution2012], open sharing of data [see @meyerPracticalTipsEthical2018], and preregistration of data collection and analysis plans [@nosekPreregistrationRevolution2018]. In some cases changes to practices have been drastic and widespread: For example, the practice of accepting *Registered Reports* (wherein peer review and in principle acceptance for publication happens *prior* to data collection) was first proposed in 2012 [see @chambersRegisteredReportsPresent2020 for a history] and is already offered by more than 250 journals (https://osf.io/rr/).

These rapid changes present a pedagogical challenge. Traditionally, textbooks and other instructional materials for undergraduate students - especially at the first-year level - emphasise the most fundamental and "well-established" studies and findings in a given discipline. 
Keeping textbooks and other instructional materials "up to date" is currently very difficult in psychology, when supposedly well-established findings are being contradicted by new replications emerging at a rapid pace. Exacerbating this challenge is the presence of dilemmas in approach. 

The first of these open questions is: Which methods are most effective for teaching students about replicability and open science? This question was addressed by @chopikHowWhetherTeach2018, who tested  a one-hour lecture for undergraduate students pertaining to the replication crisis and methods for increasing reproducibility. Using a pre-post survey, they found that students seemed to comprehend the key messages in this lecture.

The second open question relates to the fact that when psychology students are just beginning their university studies, they are often learning for the first time about how and why it is useful to apply scientific methods to studying human behaviour (rather than just relying on intuition or anecdote or authority). Could an over-emphasis on problems with replicability leave such students unconvinced that scientific methods for studying human behaviour are valuable *at all*, leaving them to favour even less credible alternative sources of knowledge? @chopikHowWhetherTeach2018 found that students trusted the results of studies by psychologists less after their lecture on the replication crisis, although the effect size was fairly small (d = -.36), and mean trust levels remained fairly high after the lecture (M = 4.94 on a scale of 1 to 7). 

Many of these recent changes have links to the classic *Mertonian* norms of science. For example, the practice of sharing open source software directly corresponds to the Mertonian norm of *communism*: Scientists should have common ownership of scientific goods. Similarly, the practices of sharing of preprints for open peer review and open data for checking of reproducibility corresponds to the Mertonian norm of *organised skepticism*: Scientific claims should be subjected to critical scrutiny.

<!--In fact I think our items may have been designed to correspond to specific Mertonian norms and counternorms - we should describe how this works when we come to writing Method > Measures.-->

These rapid changes lead to an important challenge: Training psychology students to understand and apply methodological practices which are rapidly evolving. Training in methodological practices is obviously crucial for graduate students who will apply psychological research methods themselves, such that the next generation of researchers can produce research which is more replicable than that of the last. But this training is also important for early undergraduate students. Only by understanding contemporary methodological practices - and problems with methodological practices in psychology - can they become informed and critical consumers of knowledge about psychology. 

Recent research has therefore been dedicated to the question of how undergraduate students might most effectively be taught about the replication crisis (and related open science reforms in psychology) [@chopikHowWhetherTeach2018]. However, there are several reasons why it might be unwise to assume a simple knowledge deficit situation, where students lack knowledge about the topic and the teacher's only role is to find ways to fill in this knowledge. 

First, unlike some areas of psychology covered in undergraduate courses (e.g., models of working memory, or the internal workings of human senses), the replication crisis is frequently discussed in mainstream and social media: Many students may plausibly have some knowledge about these issues already (and perhaps even more knowledge than some of their teachers). Teaching methods should thus be informed by some understanding of what students' pre-existing levels of knowledge are. 

Second, when psychology students are just beginning their university studies, they are often learning for the first time about how and why it is useful to apply scientific methods to studying human behaviour (rather than just relying on intuition or anecdote or authority). Could an over-emphasis on problems with replicability leave such students unconvinced that scientific methods for studying human behaviour are valuable *at all*, leaving them to favour even less credible alternative sources of knowledge? It is therefore important to determine what levels of trust in psychological research (and its replicability) are prevalent in incoming undergraduate students, rather than simply assuming that the correct degree of calibration is always to prompt students to trust psychology research *less* than they already do.

Third, anecdotal evidence suggests that students being taught about open science practices and reforms to improve reproducibility (such as open sharing of data and analysis code) are often surprised that this is not *already* standard practice. In other words, their naive conception of how science works may in some ways be closer to that embodied in recent reforms rather than traditional practices. It is thus possible that, for some students, teaching about reform may be less a matter of conveying new information and more of reinforcing "naive" assumptions. In fact, although beyond the scope of this article, it may possibly be the case that incoming undergraduate students more strongly endorse open science norms than do academics.

These considerations suggest that it is important that teachers wishing to inform undergraduate psychology students about the replication crisis and open science practices have some understanding of what such students actually know and believe about these topics already. In addition, undergraduate psychology students themselves represent one of the most important audiences for psychological research: The number of undergraduate students enrolled in psychology courses and learning about psychological research in any given year dwarfs the number of academics working in psychology. As such, the preferences of these students with respect to reproducibility and open science practices are intrinsically important for psychological researchers to know about. If, for example, undergraduate psychology students universally have a strong preference that journal articles are freely available to members of the public, this preference on the part of some of the consumers of the knowledge we produce would surely have some bearing on our choices in relation to sharing of manuscripts. This is especially the case given that undergraduate students are directly or indirectly responsible for much of the funding which allows universities to operate and research to be conducted. In fact, in serving as participants for course credit in many universities, undergraduate psychology students also provide the data underlying much psychological research. As such, their preferences are intrinsically important.

For these reasons, we aimed to conduct a study describing what incoming undergraduate students of psychology believe about reproducibility and open science practices in psychology. Our survey encompassed norms (how students felt research *should* be conducted), norms in practice (how students believe psychological research *is* conducted) and replicability (how replicable students believe psychological research is). In doing so we hope to provide knowledge which can inform the pedagogy of teaching about replication and open science practices.

## Aims



# Methods
We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

## Participants


## Material

## Procedure

## Data analysis
We used `r cite_r("r-references.bib")` for all our analyses.


# Results

# Discussion


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
