---
title: What do incoming university students believe about open science practices in
  psychology?
author:
- name: Jennifer L. Beaudry
  affiliation: '1'
  corresponding: yes
  address: Postal address
  email: jbeaudry@swin.edu.au
  role: null
- name: Michael C. Philipp
  affiliation: '2'
  role: null
- name: Matt N. Williams
  affiliation: '2'
  role: null
shorttitle: Psychology students' beliefs about open science
output:
  papaja::apa6_word: default
  papaja::apa6_pdf: default
authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.
  Enter author note here.

# Abstract: |

## BACKGROUND: Rapid changes to methodological best practices present significant challenges for educating students new to the field of psychology. Some students may have existing knowledge about recent methodological reforms (e.g., preregistration, replication). Yet, where knowledge is lacking, overemphasizing the problems that precipitate these reforms (e.g., p-hacking, HARKing) may lead students to de-value scientific methods as a credible means for generating knowledge. 
## OBJECTIVE: Understanding students’ naive conceptions about how science works and the norms that guide scientific best practice is important so that teachers can adapt their teaching to the students’ existing understandings. 
## METHOD: This study examined ### incoming undergraduate psychology students’ understandings of how research *should* be conducted (norms) and how they believed research *is* conducted (practice) using an online survey. 
## RESULTS: We found that XXX. . .
## CONCLUSION: 

Keywords: open science, psychology, teaching, reproducibility, replication
wordcount: X

bibliography: ['r-references.bib','Undergrads & Open Science (public).bib']
floatsintext: yes
figurelist: no
tablelist: no
footnotelist: no
linenumbers: yes
mask: yes
draft: no
documentclass: apa6
classoption: man
affiliation:
- id: '1'
  institution: Swinburne University of Technology
- id: '2'
  institution: Massey University
---
<!-- I've only partially entered authorship deets for the moment and set it to mask authorship info when knitting
It's set to default to knit to .docx, but can also knit to pdf (if you have a tex distro installed)
The pdf looks nicer and might be good for a preprint but .docx probably easiest to use for journal submissions.
To knit to RMarkdown you will probably need the papaja package installed, and to knit to .pdf you will need a tex distro (e.g., TinyTex).
However, it is possible to run all the R code (without knitting to .docx or .pdf) by clicking Run > Run All Chunks Below.
This could be useful in future for readers interested in reproducing results but who don't have or wish to install 
papaja and its many dependencies.
-->

<!-- The .bib file is now just from our shared library -->

```{r setup, include = FALSE}
library(papaja)
library(here)
library(tidyverse)
library(finalfit) # to keep trailing zeros & round_tidy
library(psych) #for correlation matrices
library(tidyselect) 
library(psychometric) # for correlation confidence intervals - might not need...
library(broom)

r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed,
  results = 'as.is')
```

```{r functions}

p_tidy <- function (x, digits, prefix = "= ") {
    x.out = paste0(prefix, round_tidy(x, digits))
    all_zeros = paste0(prefix, round_tidy(0, digits))
    less_than = paste0("< ", format(10^-digits, scientific = FALSE))
    x.out[x.out == all_zeros] = less_than
    return(x.out)
}
```


```{r import data}

# import the data 
df <- read.csv (here::here("data", "students_processed.csv"))

# convert variables to factors as needed
df$age <- as_factor(df$age)
df$eligibility <- as_factor(df$eligibility)

# convert factors to characters as needed
df$university <- as.character(df$university)
df$degree <- as.character(df$degree)
df$major <- as.character(df$major)
df$major <- as.character(df$major)
# df$uni_country <- as.character(df$uni_country)
# df$nationality <- as.character(df$nationality) [i think these should be factors]

# # import the metadata [check if I need it for these analyses?]
# metadata <- here::here("survey", "data", "os_metadata_raw_data.csv") %>% 
#   read_csv(col_names = TRUE, skip_empty_rows = TRUE) %>% 
#   filter(!is.na(OldVariable))
```


The last decade has seen unprecedented change in methodological and reporting practices in psychology. These changes were partly precipitated by what is popularly known as the "replication crisis": The discovery that close replications of published psychological studies are often unable to replicate the original findings [@kleinInvestigatingVariationReplicability2014; @kleinManyLabsInvestigating2018]. These apparent problems with replication have lead to a variety of potential solutions to make research practices more reproducible and transparent, including more frequent publication of replication studies [see @brandtReplicationRecipeWhat2014], more thorough reporting of methods and results [@simmons21WordSolution2012], open sharing of data [see @meyerPracticalTipsEthical2018], and preregistration of data collection and analysis plans [@nosekPreregistrationRevolution2018]. These calls for more reproducible and transparent research practices prompted the discipline to reflect on the norms and beliefs that underpin its practice of research. Many studies have polled researchers' understandings of and adherence to open science norms, beliefs, and practices [e.g., @baker500ScientistsLift2016; @openscienceworkinggroup2017SurveyOpen2018; @harrisUseReproducibleResearch2018].

The most comprehensive set of principles for how science *ought* to be practiced are Merton's norms of science [@mertonNoteScienceDemocracy1942]. Scientists [@andersonExtendingMertonianNorms2010] and graduate students [@andersonGraduateStudentExperience1994] alike have historically endorsed the normative value of Merton's principles. Furthermore, many of the recent practices designed to make science more transparent and open reflect Mertonian norms [see @vazireWeWantBe; vazireWhereAreSelfcorrecting2020]. For example, the practice of sharing open source software corresponds to the Mertonian norm of *communism*: Scientists should have common ownership of scientific goods. Similarly, the practices of sharing of preprints for open peer review and open data for checking of reproducibility corresponds to the norm of communality, and also to that of *organised skepticism*: Scientific claims should be subjected to critical scrutiny. Preregistration can be connected to the norm of *disinterestedness*: By making (and preregistering) decisions about how to analyse data before results are produced, a researcher can limit the degree to which the substantive results produced by different analytic strategies affect their decisions regarding which analyses to report. In this sense, the ongoing reform in psychological research can partly be understood as simultaneously a set of new practices and a re-affirmation of established norms.

Rapid changes in methodological practice and empirical findings present significant pedagogical challenges for the teacher of psychology. Keeping textbooks and other instructional materials up to date is difficult when supposedly well-established findings are contradicted by new replications emerging at a rapid pace. Furthermore, training in emerging methodological practices is crucial for graduate students who may go on to apply psychological research methods themselves. Even for students who do not go on to conduct research themselves, an understanding of contemporary methodological practices - and problems with methodological practices in psychology - is essential for becoming informed and critical consumers of psychological knowledge. Studies have therefore begun to explore strategies for educating psychology students about replicability and open science practices [e.g., @chopikHowWhetherTeach2018; @graheHarnessingUndiscoveredResource2012; @jekelHowTeachOpen2020; @schonbrodtTrainingStudentsOpen2019]. These initiatives may help ingrain open science norms and change attitudes about research practices, but we know little about what these students know or believe about open science research practices prior to entering the university classroom. There are several reasons why this knowledge could be useful.

First, unlike some psychological phenomena in undergraduate courses (e.g., models of working memory, or the internal workings of human senses), the replication crisis is frequently discussed in mainstream and social media [e.g., @ogradyPsychologyReplicationCrisis2020; @yongPsychologyReplicationCrisis2016; @yongPsychologyReplicationCrisis2018]. Students may plausibly have some knowledge about these issues obtained prior to (or independently of) their formal studies. Teaching methods should thus be informed by some understanding of what students' pre-existing levels of knowledge are. 

Second, when psychology students are beginning their university studies, they are often learning for the first time about how and why it is useful to apply scientific methods to studying human behaviour (rather than only relying on alternative sources of knowledge such as intuition or anecdote or authority). An emphasis on problems with replicability could at least in principle leave these students unconvinced that scientific methods for studying human behaviour are valuable *at all*, leaving them to favour even less credible alternative sources of knowledge. For instance, @chopikHowWhetherTeach2018 found that undergraduate students tended to trust the results of psychological studies *less* after a one-hour educational lecture about the replication crisis. This said, the effect of the lecture on trust of psychological studies was relatively small (d = -.36), and mean trust levels remained fairly high after the lecture (M = 4.94 on a scale of 1 to 7). Knowledge about incoming students’ “baseline” perceptions of the replicability of psychological science could further inform teachers about the salience of this risk.

Third, anecdotal evidence suggests that some students being taught about open science practices and reforms to improve reproducibility (such as open sharing of data and analysis code) are often surprised that these are not *already* standard practice [e.g. see tweets on this topic: @buraktuncaUygunTuncRmounce2021; @duyguuygun-tuncLastNightWas2021]. This could imply a need for educators to reinforce and build on students “naive” impressions rather than radically altering their understanding of how science should be done. On the other hand, other evidence suggests that undergraduate students may quickly begin to engage in practices that hamper reproducibility. For example, @moranKnowItBad2021 found that 26.5% of the undergraduate students in their Canadian sample admitted “Conducting multiple statistical analyses on the same dataset in an attempt to find a statistically significant result” (i.e., p-hacking), while 9.6% reported rounding down p-values. The extent to which incoming psychology students may - or may not - typically be “naive open scientists” is useful knowledge for guiding pedagogical approaches.

For these reasons, we aimed to conduct a study describing what incoming undergraduate students of psychology believe about reproducibility and open science practices in psychology. Our survey encompassed questions norms (how students felt research *should* be conducted), norms in practice (how students believe psychological research *is* conducted) and replicability (how replicable students believe psychological research is). In doing so we hope to provide knowledge which can inform the pedagogy of teaching about replication and open science practices. This study is exploratory [see @wagenmakersAgendaPurelyConfirmatory2012] and descriptive, and does not involve the specification or testing of hypotheses.

# Methods
We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

```{r inclusion}

# participant numbers (all & included cases)
df_all <- df
n_all <- nrow(df_all)

# only participants who met the inclusion criterion
df <- df_all %>% 
  filter(exclude %in% "include") 
n <- nrow(df)

ex <- df_all %>% 
  filter(exclude %in% "exclude") %>% # n for those excluded
  nrow()

# note re: exclusion [all coded in `exclude` in preprocessing, 
# but need to summarise exclusion criteria here]

age_ex <- df_all %>%
  filter(age_criteria %in% "exclude") %>% 
  nrow()

el <- df_all %>%
  filter(eligibility_criteria %in% "exclude") %>% 
  dplyr::count(eligibility_lab) # need the breakdown by condition

status <- df_all %>% 
  filter(status_criteria %in% "exclude") %>% 
  dplyr::count(status_lab) # need the breakdown by condition

nmiss <- df_all %>% 
  filter(nmiss_criteria %in% "exclude") %>% 
  nrow()

```


```{r demographics for those included in analyses}

# age
age <- df %>% dplyr::count(age_lab)

# gender --> need to summarise once we've recoded it in preprocessing script
  # [breadcrumb]
gender <- df %>% dplyr::count(gender)

# high school status
hs_status <- df %>% dplyr::count(high_school_lab)

# high school psyc 
hs_psyc <- df %>% dplyr::count(psych_hs_lab)

# country of university 
uni_country <- df %>% dplyr::count(uni_country)

# country of nationality
nationality <- df %>% dplyr::count(nationality)

# whether parents' attended uni
first_gen <- df %>% dplyr::count(first_gen_lab)
```


## Participants

Of those who started the survey (*n* = `r n_all`), we screened out `r ex` 
participants who were not eligible to participate based on the combination of our
preregistered exclusion criteria. Specifically, we screened out people who were younger than 
18 years (*n* = `r age_ex`). We also screened out individuals who had already 
started a psychology unit at a university, regardless of whether they had 
completed it (*n* = `r el[1,2] + el[3,2] + el[4,2]`); were not enrolled in a 
psychology unit at a university (*n* = `r el[2,2]`); or did not answer this 
question (*n* = `r el[5,2]`). We then excluded additional responses that 
Qualtrics flagged as spam (*n* = `r status [1,2]`) or as a survey preview 
(*n* = `r status [2,2]`). Finally, we excluded participants who met all other 
eligibility criteria, but did not respond to any of the main items
in the study (*n* = `r nmiss`). 

The remaining `r n` participants were eligible based on starting their first unit 
of study in psychology at a university within the next month. For those individuals 
who were screened out, our Qualtrics program automatically directed them to the 
debriefing form.  

Of the remaining `r n` participants, most reported being
18-24 years old (*n* = `r age[1,2]`), `r age [2,2]` 
reported that they were 25-34 years old, `r age [3,2]` were 35-44 years old,
`r age [4,2]` were 45-54 years old, and `r age [5,2]` was 55-64 years old. 

The majority of participants (*n* = `r gender[1,2]`) reported that they were female, 
`r gender[2,2]` reported that they were male, `r gender[3,2]` participants reported 
their gender identity as "queer man", and `r gender[3,2]` participants reported 
non-binary gender.^[While the response "queer man" could potentially be coded as
"male", we are aware that some individuals who report gender in this way would
not feel that such reclassification was appropriate. We have retained the original
response in order to provide a faithful representation of the participant's 
self-reported gender.]

Most participants reported attending university in Australia (*n* = `r uni_country[1,2]`), 
the United Kingdom (*n* = `r uni_country[5,2]`), or New Zealand (*n* = `r uni_country[4,2]`). Nearly all reported graduating from high
school or secondary school (*n* = `r hs_status[3,2]`). Of those participants who 
attended high school, fewer than half completed psychology courses in 
high school (*n* = `r hs_psyc[2,2]`). Most reported their nationality as being from Australia (*n* = `r nationality[3,2]`), the United Kingdom (*n* = `r nationality[24,2]`), or China
(*n* = `r nationality[6,2]`). About 2/3rds of participants reported that at least one of their parents attended university (*n* = `r first_gen[2,2]`). 


```{r qual responses}

# [breadcrumbs] 
 # once qual responses are coded, I will need to return here to 
 # include details about which univrsity the participant is enrolled in
 # name of qualification
 # major [how many are majoring in psyc; NAs; will decide later?]

```


## Material

### Open Science Norms and Counternorms

Our approach to measuring norms and counternorms focussed on participants' evaluation of ten specific open science practices that reflected diverse open science norms rather than trying to measure endorsement of the more abstract Mertonian norms (cf. Anderson et al., 2007). <!-- Mention here our discussions workshopping of common open science norms at SIPs? ! -->
The practices assessed in our survey included: critical thinking, preregistration, registered reports, p-hacking, hypothesising after the results are known (HARKing), providing sufficient information of replicability, preprints, open materials, open data, and open access publishing. Each practice was elucidated by a pair of items (pro-open science; counter-open science) in the context of a common scenario, "Imagine that Deborah is a psychology researcher who has designed a study to test a specific hypothesis." Although we acknowledge the existence of important debate about the merits of some of these practices [e.g., @TQMP16-4-376, @rubinWhenDoesHARKing2017, @szollosiPreregistrationWorthwhile2020], we selected the items to be emblematic of common concerns in the open science movement.


There were 20 scenarios in total (see Table \@ref(tab:norm_counternorm_items)). Participants rated their agreement with each prescriptive statement on a 1 (strongly disagree) to 5 (strongly agree) scale.

<!-- Add Table X of norms and counternorms statements that map to each practice -->

```{r norm_counternorm_items}
norm_items = data.frame(
  "Practice" = c("critical", "critical", "HARKing", "HARKing", "Info for replicability", "Info for replicability",
                 "Open access", "open access", "Open data", "Open data", "Open materials", "Open materials",
                 "p-hacking", "p-hacking", "Preprints", "Preprints", "Pregistration", "Preregistration",
                 "Registered reports", "Registered reports"),
  "Category" = rep(c("Norm", "Counternorm"), times = 10),
  "Item" = c("Deborah used previously published research to inform her research study. Deborah should be critical of the findings published in journals because published research can be wrong.",
             "Deborah used previously published research to inform her research study. Deborah should accept the findings published in journals because journals would not publish research with errors.",
             "Deborah should only describe her study as a test of a hypothesis if she decided on her hypothesis and how she would test it before she started collecting data.",
             "Once Deborah has analysed her results, it would be good scientific practice for her to write her manuscript as if she predicted those results from the beginning.",
             "When reporting the findings of her study, Deborah should describe how she completed the study in enough detail that another researcher could repeat her entire study without having to check any details with her - even if this means including lots of “boring” practical details in her report, or in an appendix.",
             "When reporting the findings of her study, it would be good scientific practice for Deborah to gloss over some of the practical details so she can tell a good story.",
             "Once her study is complete, Deborah should publish her findings in a journal that is free for others to access.",
             "Once her study is complete, Deborah should publish the findings in the most prestigious journal she can, even if that journal charges others a fee to access the report.",
             "When Deborah publishes her study, she should post the anonymous responses from participants online so that anyone can access and use the responses in their own research.",
             "Deborah should keep the participants’ responses from her study protected, so that only she and her research team can access them.",
             "Deborah should share the written materials and measures for her study openly online so that other researchers and members of the public can access and use them.",
             "Deborah should keep the written materials and measures for her study protected, so that only she and her research team can access them.",
             "Deborah should report the findings of all analyses of her data that she conducts.",
             "It would be good scientific practice for Deborah to run many different analyses of her data, and report only those that produce interesting findings.",
             "Deborah should post a manuscript describing the findings of her study openly online as soon as it is complete, even if the manuscript has not yet been checked by experts (peer reviewed) and accepted for publication in a journal.",
             "Deborah should not post a manuscript describing the findings of her study online until after it has been checked by experts (peer reviewed) and accepted for publication in a journal.",
             "Before collecting her data, Deborah should write down what her hypotheses are and how she plans to collect and analyse data. She should then save her plan in an online registry so others can tell what methods and analyses she will use to test her hypotheses after collecting data.",
             "Deborah should decide which data analyses are suitable to test her hypotheses only after looking at her data.",
             "Deborah should submit a plan for her study to a journal to be checked by experts (peer reviewed) before she collects and analyses data.",
             "Deborah should submit her study to a journal to be checked by experts (peer reviewed) only after she has finished collecting and analysing her data."))
  
knitr::kable(norm_items,
             caption = "Norm and counternorm items",
             align = c('lll'))
```


### Beliefs About the Practice of Psychological Science

We also sought to measure participants' normative beliefs about how psychological science was actually conducted by researchers around the world. To that end we asked participants, "When you consider all of the psychology studies conducted globally each year, what percentage of them would have the following characteristics?" These practices corresponded approximately to the ten norm-counternorm pairs described above, albeit with one norm excluded on the basis of not referring to externally observable behaviour (critically evaluating published studies vs. accepting them), and one practice added out of interest (data sharing on request).  (see Table XX). Participants were given a slider bar ranging from 0% to 100% to record their estimate. 

<!-- Add Table XX of 10 belief statements -->

```{r norms_in_practice_items}
practice_items = data.frame(
  "Practice" = c("Preregistration", "Registered reports", "p-hacking", "HARKing", "Info for replicability",
                 "Preprint", "open materials", "Open data", "Data on request", "Open access"),
  "Item" = c("The plan for collecting and analysing data for the study is posted in an online registry before data collection starts",
             "A plan for the study is submitted to a journal to be checked by experts (peer reviewed) before the data is collected and analysed",
             "The researchers run many different analyses of the data, and report only those that produce interesting findings",
             "The researchers describe their results as if they predicted those results from the beginning, even if that isn’t actually true",
             "The researchers provide enough practical detail in their written report that another researcher could fully repeat (replicate) their study without needing to ask any questions of the original researchers",
             "A manuscript describing the findings of the study is posted openly online as soon as it is complete, even if the manuscript has not yet been checked by experts (peer reviewed) and accepted for publication in a journal",
             "The written materials and measures for the study are posted openly online so that other researchers and members of the public can access and use them",
             "Once the study is published, the anonymous responses from participants are posted openly online so that anyone can access and use the responses in their own research",
             "Once the study is published, the anonymous responses from participants are available to other researchers on request",
             "The research report describing the study and its findings is published in a journal that is free for anyone to access")
)

  
knitr::kable(practice_items,
             caption = "Norms in practice items",
             align = c('ll'))

#Note to selves: The norm in practice items map fairly closely to norms and counternorms, but not exactly
#I.e., there is no norm in practice item about critically evaluating published literature, but there is one about sharing
#data on request (which isn't in the norms/counternorms)
#I can't remember why we did this, but so it goes.
#The ordering of this table also doesn't match the one above, which I'm leaving for the moment since we probably want to set
#an order of items/norms and stick to it consistently throughout

```


### Beliefs About Replications

Participants' beliefs about the replicability of results in psychological science were indexed by two questions. The first asked participants to, 
> "Imagine that a set of researchers selected 100 published studies about psychology, and repeat each study again, exactly how it was described in the original report but with a new set of participants. We call repeating a study like this a 'replication study.' At a guess, how many of these replication studies do you think would produce the same conclusions as the original studies?”

Estimates were recorded using a slider bar ranging from 0% to 100%. 
A second question asked participants whether they had heard of the replication crisis. Those who had heard of the replication crisis were asked two additional open-ended questions. First, they were asked to describe what the replication crisis is. Second, they were asked which scientific disciplines were most affected by the replication crisis. 


## Procedure

Collaborators/Contributors/Colleagues from ## tertiary institutions in Australia (n=), New Zealand (n=), the UK (n=), and the USA (n=) invited students enrolled in their first-year psychology course to participate in the online survey hosted on Qualtrics (CITE?). An invitation was sent to participants via email or a course website 1-2 weeks before their course began. After completing demographic information, participants completed measures of Open Science Norms and Counternorms, Norms in Practice, and Beliefs about Replications in that order. At the conclusion of the study participants were invited to share any comments about the survey before they were thanked for their participation and provided an informative debriefing about the aims of the study and links to further reading about open science practices in psychology. The survey took most participants approximately #-# minutes to complete.

## Data analysis

# Results


In line with our preregistered analysis plan, we calculated the difference in means
between responses to each norm item and its accompanying counternorm, along with 
95% confidence intervals. For this analysis, we removed three cases with missing values. 
Table XX displays the means and standard deviations for each norm, counternorm, 
and the difference values. 

\@ref(tab:chunk-name)

In general, the students seemed to endorse open science practices: For seven of the ten open science practices, agreement with the norm item was stronger than for the corresponding counternorm. However, the extent to which participants endorsed open science norms varied very substantially across different practices. Participants most strongly endorsed norms that researchers should provide sufficient information about studies to permit replicability, be critical of published findings, and avoid p-hacking (means > 4; 4 representing 'agree'). They also tended to agree that researchers should avoid HARKing, share papers as open access, share open materials, and preregister their studies; the means for these items were all between 3.5 and 4. On the other hand, they tended to disagree with the practice of sharing preprints prior to peer review and, to a lesser extent, with sharing open data. The practice of registered reports was the one about which participants were most equivocal, with very similar levels of agreement (near the midpoint of 3) with both norm and counternorm for this item. This may be relatively unsurprising, given that the registered reports are perhaps the most recent innovation in the list of practices.

Unsurprisingly, there were substantial negative correlations between most norm and counternorm pairs, with the exception of the norm and counternorm items relating to preregistrations and HARKing.


```{r difference between norm and counternorm}

# select columns that relate to norms & pivot it long
norm_long <- df %>% 
  dplyr::select(c(id, ends_with("_norm"))) %>% 
    pivot_longer (cols = ends_with("norm"),
                names_to = "practice",
                values_to = "norm rating") 

# remove "_norm" from the practice variables
norm_long$practice = gsub(pattern = "_norm", replacement = "", x = norm_long$practice)

# select columns that relate to counternorms
cnorm_long <- df %>% 
  dplyr::select(c(id, ends_with("_cnorm"))) %>% 
  pivot_longer (cols = ends_with("cnorm"),
                names_to = "practice",
                values_to = "cnorm rating") 

# remove "_cnorm" from the practice variables
cnorm_long$practice = gsub(pattern = "_cnorm", replacement = "", x = cnorm_long$practice)

# join the norm and counternorm tibbles
norm_cnorm <- norm_long %>% inner_join(cnorm_long)

# create a `diff` variable that is the counternorm subtracted from the norm
norm_cnorm_long <- norm_cnorm %>% 
  mutate (diff = (`norm rating` - `cnorm rating`)) %>% 
    drop_na() %>% # drop the three NA values
      dplyr::select(-(`id`)) # remove this now that I've confirmed it works

# get the n values for each of the practices
prac_counts <- norm_cnorm_long %>% 
    dplyr::group_by(practice) %>%  
    count() %>% 
  ungroup()

# calculate means
prac_ms_sds <- norm_cnorm_long %>% 
    dplyr::group_by(practice) %>%  
    summarise(norm_mean = mean(`norm rating`), 
              norm_sd = sd(`norm rating`),
              cnorm_mean = mean(`cnorm rating`), 
              cnorm_sd = sd(`cnorm rating`),
              diff_mean = mean(`diff`), 
              diff_sd = sd(`diff`)) %>% 
  ungroup()

# join the n values with the means
prac_means <- prac_counts %>% inner_join(prac_ms_sds)
  
# calculate 95% CIs
error <- qnorm(0.975) * prac_means$diff_sd/sqrt(prac_means$n)
prac_means$lower <- prac_means$diff_mean - error
prac_means$upper <- prac_means$diff_mean + error


# combine means & SDs
prac_means$Norm <- paste0(round_tidy(prac_means$norm_mean,2),
                          " (", 
                          round_tidy(prac_means$norm_sd,2), 
                          ")")
prac_means$Counternorm <- paste0(round_tidy(prac_means$cnorm_mean,2), 
                            " (", 
                            round_tidy(prac_means$cnorm_sd,2), 
                            ")")
prac_means$Difference <- paste0(round_tidy(prac_means$diff_mean,2), 
                            " (", 
                            round_tidy(prac_means$diff_sd,2),
                            ")", 
                            " [",
                            round_tidy(prac_means$lower,2),
                            ", ", 
                            round_tidy(prac_means$upper,2),
                            "]")

# clean it up into a new dataframe with only the columns we need
practice_means <- prac_means %>% 
  dplyr::select(practice, Norm, Counternorm, Difference)

# ADD CORRELATIONS (per participant, use df)

# notes for the correlations....
# just the correlation with CI
# critical_corr <- round(cor(df$critical_norm, df$critical_cnorm),2)
# critical_CI <- round(CIr(r = critical_corr, n = nrow(df), level = 0.95),2)

#correlation test with CI and p-values in a clean tibble
# critical_corr_test <- cor.test(df$critical_norm, df$critical_cnorm)
# tidy(critical_corr_test)

# use this tutorial to try to get the correlations for each practice (using the long data)
# https://cran.r-project.org/web/packages/broom/vignettes/broom_and_dplyr.html

# this will create a tibble for each practice

# calculates the correlation test for each tibble & then tidies it up
corr <- norm_cnorm_long %>% 
  nest(data = -practice) %>% # create a tibble for each practice
  mutate (test = map(data, ~ cor.test(.x$`norm rating`, .x$`cnorm rating`)), 
          tidied = map(test,tidy)) %>% # run the corr & tidy it
  unnest(tidied) # unnest it to look at the results

# remove unnecessary columns, round the values, and move columns
corr <- corr %>% 
  dplyr::select(-c(data, test, statistic, parameter, method, alternative))  %>% 
  mutate(p.value = p_tidy(p.value, 3, prefix = "")) %>% 
  mutate(estimate = round_tidy(estimate,2), 
         conf.low = round_tidy(conf.low, 2), 
         conf.high = round_tidy(conf.high, 2)) %>% 
  dplyr::relocate(estimate, .before = conf.low) %>%  
    dplyr::relocate(p.value, .after = last_col())

# join the corr tibble with the means tibble 
practice_means_corr <- practice_means %>% inner_join(corr, by = "practice")

# combine the confidence intervals into one column
practice_means_corr$Correlation <- paste0(practice_means_corr$estimate, 
                            " [",
                            practice_means_corr$conf.low,
                            ", ", 
                            practice_means_corr$conf.high,
                            "]")

# clean up the tibble                          
practice_means_corr <- practice_means_corr %>% 
  mutate(Practice = practice) %>% 
  dplyr::select(-c(practice,estimate, conf.low, conf.high)) %>% 
  relocate (Practice, .before = Norm) %>% 
  relocate (p.value, .after = last_col())


# breadcrumbs: keep the values that I need from this tibble
# use p_tidy? to clean up the pvalues {done!}
# round the corr & CI {done!}
# join with the practice_means tibble & turn into a table {done!}
# need to figure out whether I can rotate a page in papaja

# just keeping this here for now, but will be able to delete
# critical_corr <- round(cor(df$critical_norm, df$critical_cnorm),2)
# critical_CI <- round(CIr(r = critical_corr, n = nrow(df), level = 0.95),2)
# 
# hark_corr <- round(cor(df$hark_norm, df$hark_cnorm),2)
# hark_CI <- round(CIr(r = hark_corr, n = nrow(df), level = 0.95),2)
# 
# info_for_rep_corr <- round(cor(df$info_for_rep_norm, df$info_for_rep_cnorm),2)
# info_for_rep_CI <- round(CIr(r = info_for_rep_corr, n = nrow(df), level = 0.95),2)
# 
# open_access_corr <- round(cor(df$open_access_norm, df$open_access_cnorm),2)
# open_access_CI <- round(CIr(r = open_access_corr, n = nrow(df), level = 0.95),2)
# 
# open_data_corr <- round(cor(df$open_data_norm, df$open_data_cnorm),2)
# open_data_CI <- round(CIr(r = open_data_corr, n = nrow(df), level = 0.95),2)
# 
# open_materials_corr <- round(cor(df$open_materials_norm, df$open_materials_cnorm),2)
# open_materials_CI <- round(CIr(r = open_materials_corr, n = nrow(df), level = 0.95),2)
# 
# phack_corr <- round(cor(df$phack_norm, df$phack_cnorm),2)
# phack_CI <- round(CIr(r = phack_corr, n = nrow(df), level = 0.95),2)
# 
# preprint_corr <- round(cor(df$preprint_norm, df$preprint_cnorm),2)
# preprint_CI <- round(CIr(r = preprint_corr, n = nrow(df), level = 0.95),2)
# 
# prereg_corr <- round(cor(df$prereg_norm, df$prereg_cnorm),2)
# prereg_CI <- round(CIr(r = prereg_corr, n = nrow(df), level = 0.95),2)


# breadcrumbs: 
# I need to use apa_tables.... (in line with papaja)
# pull means & sd into the same column with (sd) in parentheses [done!]
# headings should then be norm, counternorm, difference [done!]
# add correlations with 95% CIs 
# update practice descriptions so they make sense to readers

  
knitr::kable(practice_means_corr,
             caption = "Means (sd) for norms, counternorms, the difference between norms and counternorms, and correlations between norms and counternorms",
             digits = 2,
             align = c('lccc'))
  

```

<!-- I imagine we will remove the p.value column, but we might want this for now.  -->

```{r correlation_matrix}
#pairs.panel etc. won't really work because there are too many variables
#And we only need corrs between matched pairs
#So doing this a bit more manually
layout(matrix(1:8, ncol = 4))

plot(df$critical_norm ~ jitter(df$critical_cnorm), xlab = "critical counternorm", ylab = "critical norm")
abline(lm(df$critical_norm ~ df$critical_cnorm), col = "blue")
legend(x='bottomright', legend=paste('Cor =', round(cor(df$critical_norm, df$critical_cnorm),2)))

plot(df$prereg_norm ~ jitter(df$prereg_cnorm), ylab = "Prereg norm", xlab = "Prereg counternorm")
abline(lm(df$prereg_norm ~ df$prereg_cnorm), col = "blue")
legend(x='bottomright', legend=paste('Cor =', round(cor(df$prereg_norm, df$prereg_cnorm),2)))

plot(df$reg_report_norm ~ jitter(df$reg_report_cnorm), ylab = "Reg Reports norm", xlab = "Reg Reports counternorm")
abline(lm(df$reg_report_norm ~ df$reg_report_cnorm), col = "blue")
legend(x='bottomleft', legend=paste('Cor =', round(cor(df$reg_report_norm, df$reg_report_cnorm),2)))

plot(df$phack_norm ~ jitter(df$phack_cnorm), ylab = "p-hacking rnorm", xlab = "p-hacking counternorm")
#potential confusion here about direction of phrasing (pro p-hacking = counternorm?)
abline(lm(df$phack_norm ~ df$phack_cnorm), col = "blue")
legend(x='bottomleft', legend=paste('Cor =', round(cor(df$phack_cnorm, df$phack_norm),2)))

plot(df$hark_norm ~ jitter(df$hark_cnorm), ylab = "HARKing norm", xlab = "HARKing counternorm")
abline(lm(df$hark_norm ~ df$hark_cnorm), col = "blue")
legend(x='bottomright', legend=paste('Cor =', round(cor(df$hark_cnorm, df$hark_norm),2)))

plot(df$info_for_rep_norm ~ jitter(df$info_for_rep_cnorm), ylab = "Replicability norm", xlab = "Replicability counternorm")
abline(lm(df$info_for_rep_norm ~ df$info_for_rep_cnorm), col = "blue")
legend(x='right', legend=paste('Cor =', round(cor(df$info_for_rep_norm, df$info_for_rep_cnorm),2)))

plot(df$preprint_norm ~ jitter(df$preprint_cnorm), ylab = "Preprints norm", xlab = "Preprints counternorm")
abline(lm(df$preprint_norm ~ df$preprint_cnorm), col = "blue")
legend(x='bottomleft', legend=paste('Cor =', round(cor(df$preprint_norm, df$preprint_cnorm,
                                                       use = "pairwise.complete.obs"),2)))

plot(df$open_materials_norm ~ jitter(df$open_materials_cnorm), ylab = "Open materials norm", xlab = "Open materials counternorm")
abline(lm(df$open_materials_norm ~ df$open_materials_cnorm), col = "blue")
legend(x='bottomleft', legend=paste('Cor =', round(cor(df$open_materials_norm, df$open_materials_cnorm, use = "pairwise.complete.obs"),2)))

plot(df$open_data_norm ~ jitter(df$open_data_cnorm), ylab = "Open data norm", xlab = "Open data counternorm")
abline(lm(df$open_data_norm ~ df$open_data_cnorm), col = "blue")
legend(x='bottomleft', legend=paste('Cor =', round(cor(df$open_data_norm, df$open_data_cnorm),2)))

plot(df$open_access_norm ~ jitter(df$open_access_cnorm), ylab = "Open access norm", xlab = "Open access counternorm")
abline(lm(df$open_access_norm ~ df$open_access_cnorm), col = "blue")
legend(x='bottomright', legend=paste('Cor =', round(cor(df$open_access_norm, df$open_access_cnorm),2)))


```


## Norms in practice
Participants were also asked to estimate the proportion of psychology studies globally that apply each of ten practices. For many of these practices, participants’ estimates were near the midpoint of 50%. This said, participants perceived some practices as being applied relatively infrequently: For example, the mean estimate of the prevalence of HARKing, preprint sharing before publication, and open data sharing were all near 30%. Participants perceived the most common practice was providing enough information in written reports to permit replication without asking any questions of the original researchers.

<!-- There's a key point for the discussion here, I think, in that the fact that many participants guessed near 50% for these practices suggests that students may have little idea of the prevalence of these practices in reality, and certainly education is needed on that point. 

<!-- Add another paragraph here to elaborate on the shape of these distributions to help interpret the above point of discussion.-->

--> 

## The replication crisis
Interestingly, the vast majority of participants had not heard of "the replication crisis" <!-- add numeric estimate-->.

<!-- For people who answered "yes" (they had heard the term) there were a couple of open-ended follow-up items: What do you think the replication is? Which scientific disciplines do you think are most affected by the replication crisis? We could analyse these and try to give some indication of whether the students who *had* heard of it actually knew what the term refers to. -->

Participants' estimates of the replicability of psychological studies was near the midpoint of the possible range <!-- about 56% - need to code this in--> . 

<!--Point for discussion: Interestingly, this estimate is fairly consistent (albeit perhaps slightly more optimistic) than empirical estimates from large-scale studies. However, it could plausibly simply reflect a great deal of uncertainty on the part of participants.
-->

<!-- ☐ On the above point, Comment here on the shape of the distribution for this question. -->

# Discussion

Despite efforts to popularize the replicability of research the past decade, and despite popular media attention, most incoming psychology students in our sample were unaware of the replication crisis. Encouragingly, on the other hand, most tended to endorse open science norms more-so than counternorms, although this wasn't the case for all open science practices (namely sharing of pre-prints and avoiding HARKing). IIt may be interesting to consider whether student norms regarding the sharing of pre-prints would be expected to have shifted since this data was collected given the rapid increase in the dissemination of pre-prints related to COVID-19.

Participants' estimates of the proportion of psychological studies published which apply each of a set of ten open science practices tended to hover around a mean of 50%. Although this could be read as implying literally that participants believe these practices are roughly as commonly applied as they are not, it could also be read as implying a great deal of uncertainty on the part of participants. Indeed, some practices, “open access publication”, “data availability”, and “incomplete results” had a modal response of 50%, indicating that many students either literally perceived that these practices are as commonly applied as not, or selected a respon as near 50% as an indicator of indifference or uncertainty. In contrast, other practices with a mean response near 50% (e.g. registered reports) were not normally distributed around the scale midpoint. Instead, participants demonstrated substantial lack of agreement about the frequency of this practice. While the modal values were 60% (n=13) and 80% (n=13), there was also frequent endorsement of 40% (n=12), 20% (n=11) and 70% (n=11). As a result, it appears that there is substantial lack of consensus with regard to the practice of open access publication.  





Overall these findings suggest that while incoming psychology students may have a degree of sympathy for open science norms, they know relatively little about replicability and open science. 

## Limitations, strengths, and future directions

Despite a relatively large sample size, our results provide an imperfect picture of new psychology students’ methodological understandings. Our use of convenience samples coupled with the imprecision of polling students’ impressions at one point in time suggest that we can make only very tentative generalisations to a wider population of incoming psychology students. However, collectively these results demonstrate some clear topics that might be targeted in our teaching: for example, emphasising the value of pre-prints and open-data to research practice.  

Our questions focused primarily on attitudes and beliefs so that we could better document the implicit understanding that guided students’ thoughts about the research process. However, we did not probe students' factual knowledge of methodological findings or concepts. It’s quite possible that reform in secondary school instruction has improved students’ understandings of more fundamental statistical (e.g., confidence interval estimation) and design (e.g., direct replications) open science considerations without necessarily impacting their attitudes and beliefs. Yet, research at the tertiary level casts doubt that the disciplinary reform from the past 2 decades has had much impact on the content of our teaching (Friedrich et al., 2018).

<!--
More to say here.
-->


## Directions for future research


\newpage
# Reproducible Code Statement

We used `r cite_r("r-references.bib")` for all our analyses.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>

\endgroup


